<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Dev News</title><link>https://konrad.website/liveboat-github-runner/</link><description></description><item><title>Linus Torvalds Vents Over &quot;Completely Crazy Rust Format Checking&quot;</title><link>https://www.phoronix.com/news/Linus-Torvalds-Rust-Formatting</link><author>/u/unixbhaskar</author><category>dev</category><category>reddit</category><pubDate>Fri, 3 Oct 2025 03:43:06 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[
After Linus Torvalds yesterday shot down RISC-V big endian prospects for the Linux kernel, today he has used his authority to wage a war on "crazy" Rust code formatting as well as to critique poor text formatting.
In response to the recently submitted Direct Rendering Manager (DRM) subsystem pull request, Torvalds began by voicing his displeasure over the text formatting of it:
"Notice how there are multiple sub-areas: Alloc, DMA/Scatterlist, DRM and Rust.
But it's all just a random jumble, because you have apparently pasted it into your editor or MUA or whatever and dropped the indentation in the process.
What kind of *broken* editor are you using? I'm not trying to start an emacs or vi war here, but you seem to be using something truly broken.
...
Look, again, no logic and you've completely corrupted any multi-level indentation that presumably existed at some point judging by the organization.
...
Please make the explanations *readable*, not just a random jumble of words."Edlin is the line/text editor found on early versions of PC DOS and MS-DOS...
But then on a more technical note, Linus Torvalds went on to seek better clarity of Rust code formatting moving forward. When looking at the Rust DRM code added for Linux 6.18, Torvalds commented after looking at some of the new code:
"And on a more technical side: I absolutely detest the mindless and completely crazy Rust format checking.
I noticed that people added multiple
next to each other, so I turned them into
     xyz,
  };
instead to make it easy to just add another crate without messing crap up. The use statements around it had that format too, so it all seemed sensible and visually consistent.
But then I run rustfmtcheck, and that thing is all bass-ackwards garbage. Instead of making it clean and clear to add new rules, it suggests
but I have no idea what the heuristics for when to use multiple lines and when to use that compressed format are.
This is just ANNOYING. It's automated tooling that is literally making bad decisions for the maintainability. This is the kind of thing that makes future conflicts harder for me to deal with.
Miguel, I know you asked me to run rustfmtcheck, but that thing is just WRONG. It may be right "in the moment", but it is
 (a) really annoying when merging and not knowing what the heck the rules are
 (b) it's bad long term when you don't have clean lists of "add one line for a new use"
Is there some *sane* solution to this? Because I left my resolution alone and ignored the horrible rustfmtcheck results.
I tried to google the rust format rules, and apparently it's this:
    https://doc.rust-lang.org/style-guide/index.html#small-items
can we please fix up whatever random heuristics? That small items thing may make sense when we're talking things that really are one common data structure, but the "use" directive is literally about *independent* things that get used, and smushing them all together seems entirely wrong.
I realize that a number of users seem to just leave the repeated
   use kernel::abc;
as separate lines, possibly *becasue* of this horrendous rustfmt random heuristic behavior."The post in full can be read on the LKML.]]></content:encoded></item><item><title>I spent the day teaching seniors how to use an iPhone</title><link>https://forums.macrumors.com/threads/i-spent-the-day-trying-to-teach-seniors-how-to-use-an-iphone-and-it-was-a-nightmare.2468117/</link><author>dabinat</author><category>dev</category><category>hn</category><pubDate>Fri, 3 Oct 2025 01:20:32 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
			Honestly, I think Apple really needs to simplify the iPhone for the elderly. I know there are accessibility modes, but you don’t want to have to go through all that and spend hours trying to customize the phone. Also, the whole phone setup process needs to be delayed; having to go through it for an hour puts them off from even wanting to bother. I first set the phones up to make accounts, but it turns out none of them could understand how to unlock the phone. Entering a passcode was a nightmare because they kept forgetting it, even though it was a birthday they knew, lol.
So, I tried Touch ID and Face ID, and that was even more complicated and kept erroring out. Then, the Siri thing kept popping up on the phones with Touch ID, despite turning it off, and the whole swiping from the button kept making the screen go down to the bottom half. :/ There were too many apps; all they wanted was the phone app, but it doesn’t default to the keypad, which was too much for them to find.
The phones are too fiddly now, and pressing random things as they try to hold the phone meant the phone got lost in a sea of opening stuff up. So, I tried the assistive access, but why isn’t this an option from the get-go? It asks you the age of setup; why not have a 65+ or something for a senior mode?
They don’t need passcodes, accounts, and a sea of information. It’s insane, and it’s insane how fiddly these phones are. I never noticed because I’m used to it, but for these people with hands that barely move, the fake Touch ID button and the swiping from the bottom on Face ID phones seem to be the worst! I think having a proper physical button, like iPhones used to have, would have been superior. The one complaint about the fake button was that it didn’t feel like a real button, so they couldn’t gauge it.
I left there achieving nothing because they couldn’t figure out their old Nokia phones. The unlock thing on the keypad was too difficult, and if I turned that off, they kept dialing 999 in their pockets for some reason. That’s why I was there: they were calling emergency services 100 times a day, lol.
I think what I’ve realized is that I need to go back with flip phones that answer and hang up when you open and close them. However, the two I tried before didn’t act like that, and they had too many features. I really thought I could make the iPhone simple, but NOPE!
Apple should work on their phones to make them more accessible and less fiddly, without having to go through a sea of menus.
		]]></content:encoded></item><item><title>Security update (4 hours ago): Incident related to Red Hat Consulting GitLab instance</title><link>https://www.reddit.com/r/linux/comments/1nwk3uy/security_update_4_hours_ago_incident_related_to/</link><author>/u/fenix0000000</author><category>dev</category><category>reddit</category><pubDate>Thu, 2 Oct 2025 23:56:13 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Intro: "We are writing to provide an update regarding a security incident related to a specific GitLab environment used by our Red Hat Consulting team. Red Hat takes the security and integrity of our systems and the data entrusted to us extremely seriously, and we are addressing this issue with the highest priority".]]></content:encoded></item><item><title>Ladybird browser update (September 2025)</title><link>https://www.youtube.com/watch?v=6vsjIIiODhY</link><author>/u/InsectAlert1984</author><category>dev</category><category>reddit</category><pubDate>Thu, 2 Oct 2025 23:48:18 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The strangest letter of the alphabet: The rise and fall of yogh</title><link>https://www.deadlanguagesociety.com/p/history-of-letter-yogh</link><author>penetralium</author><category>dev</category><category>hn</category><pubDate>Thu, 2 Oct 2025 21:34:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[English spelling has a reputation. And it’s not a good one. English spelling is so complex that we’ve made mastering it into a competitive sport: what would be the point of a spelling bee in a language with a predictable spelling system? Where’s the fun unless you have to sweat a little as you struggle to recall whether this particular word is one where “‘i’ before ‘e’ except after ‘c’” doesn’t apply?In short, English has a complicated writing system.the Frenchstingy printerslate medieval yuppiesBut like an Icelandic family saga, we begin not with the story of yogh, but with the story of its parent. So allow me to introduce you to the letter ‘g,’ which, as you’ll soon see, is a complicated letter in its own right, dating back to Old English.It starts with the shape of the letter. When modern editors print Old English today, they print nice, modern-looking ‘g’s — that is, the ones we use today, with an open or closed loop on the bottom, depending on the typeface.‘How we have heard of the glory of the kings of the spear-Danes in days of old’ But in the manuscript, they’re written like this:London, British Library, Cotton MS Vitellius A XV, f. 132r.Now, there’s clearly lots of other weird stuff going on in the manuscript, but focus on how the ‘g’ is represented. While the majuscule (capital) ‘G’ in ‘gardena’ is spelled more like a modern ‘g,’ all the others are insular ‘ᵹ.’So for a period, both ‘g’ and ‘ᵹ’ were used in England, but generally speaking, each was used for writing the ‘g’ sound in a different language. Simple enough, but the stage was set for things to get a lot more complicated. But for that, we need the help of the Normans.The Dead Language Societyupgrade to support my mission of bringing historical linguistics out of the ivory towertwo extra Saturday deep-divesBeowulf Book Clubpart 1part 2Norman scribesThese Norman scribes inherited the writing traditions that the Carolingian renaissance had given birth to. This meant the latest, greatest, 11th-century French versions of the Carolingian minuscule. These weren’t so different from the way Anglo-Saxon scribes had written Latin. But they were very different from the way they had written Old English, especially in the ‘g’ department.But that wasn’t so much of an issue, since these Norman-trained scribes, and those of the generations that came after them, didn’t write much English at all. In fact, writing in English of any kind was very scarce up until the end of the 12th century.Over the course of that tumultuous — and, for English, silent — century, the language had changed a great deal. All the scribes trained in the old, Anglo-Saxon traditions were long dead, so when a new generation of scribes turned their attention once again to English, they had to devise some new strategies for writing it.And this, after a surprisingly long delay, is where we first meet the star of today’s issue: ‘ȝ,’ also known as yogh. entirely different letterOr is it actually so bizarre?Modern English spelling is, of course, chaotic. So perhaps it shouldn’t surprise us that we too have a very yogh-like situation with two of our letters: ‘c’ and — wait for it — ‘g.’A process like this happened in the ancestor of French too, just slightly differently. Instead of the ‘g’ being pulled forward into a ‘y’ sound before ‘i’ and ‘e,’ it got pulled forward into a ‘j’ sound. Back then, there was just one single language, which would later split into English, Dutch, German, Swedish, and all the other Germanic languages.And that’s why yogh has two sounds, each of which corresponds to a pronunciation of the Old English letter ‘ᵹ’ that the French scribal tradition couldn’t accept writing with ‘g.’When you’re reading Middle English, it can get a bit confusing: Which kind of yogh is which?Sir Gawain and the Green KnightSir Gawain and the Green Knightmodern readers of Middle Englishother letters unique to EnglishAnd Scottish printers were more eager to keep it than English printers were. So they took advantage of the visual similarity between ‘ȝ’ and ‘z’ — most forms of cursive writing in English still write ‘z’ like ‘ȝ’ — to write their yoghs with ‘z’s.And that’s how one single letter of the Middle English alphabet ended up being pronounced like ‘y,’ ‘gh,’ or even eventually like ‘z.’ I warned you it would be complicated. But the journey through the history of yogh has allowed us to peer down some interesting side alleys of the history of writing, from Carolingian scribal practices to the compromises of Scottish printers.I don’t lament the loss of yogh myself, not nearly as much as I lament the fate of other lost letters. But if the cause of yogh is one ȝou fancy taking up ȝourself, there’s nothing standing in ȝour waȝ (it’s included in many modern fonts), althouȝ I can imagine hiȝer causes to aspire to.]]></content:encoded></item><item><title>Where It&apos;s at://</title><link>https://overreacted.io/where-its-at/</link><author>/u/steveklabnik1</author><category>dev</category><category>reddit</category><pubDate>Thu, 2 Oct 2025 20:36:48 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[You might have heard about the AT protocol (if not, read this!)Together, all servers speaking the AT protocol comprise —a web of hyperlinked JSON. Each piece of JSON on the atmosphere has its own  URI:But where do they point, exactly?Given an  URI, how do you locate the corresponding JSON?In this post, I’ll show you the exact process of resolving an  URI step by step. Turns out, this is also a great way to learn the details of how  works.Let’s start with the structure of a URI itself.As you might know, a URI often contains a scheme (for example, ), an  (like ), a path (like ), and maybe a query.In most protocols, including , the authority part points at whoever’s  the data. Whoever  this data is either not present, or is in the path:The  protocol flips that around.In  URIs, whoever  the data is the authority, in the most literal sense:The user is the authority for their own data. Whoever’s  the data could change over time, and is  directly included in an  URI. To find out the actual physical server hosting that JSON, you’re gonna need to take a few steps.Let’s try to resolve this  URI to the piece of JSON it represents:An easy way to resolve an  URI is to use an SDK or a client app. Let’s try an online client, for example, pdsls or Taproot or atproto-browser. They’ll figure out the physical server where its JSON is currently hosted, and show that JSON for you.The above  URI points at this JSON, wherever it is currently being hosted:You can guess by the  field being  that this is some kind of a post (which might explain why it has fields like  and ).However, note that this piece of JSON represents a certain social media post , not a web page or a piece of some app. It’s pure data as a piece of JSON, not a piece of UI. You may think of the  stating the data ; the  prefix tells us that the  application might know something about what to do with it. Other applications may also consume and produce data in this format.A careful reader might notice that the  in the JSON block is  an  URI but it’s slightly different from the original  URI we requested:In particular, the short  authority has expanded into a longer  authority. Maybe that’s the physical host?Actually, no, that’s not the physical host either—it’s something called an . Turns out, resolving an  URI is done in three distinct steps:Resolve the handle to an identity Resolve that identity to a hosting Request the JSON from that hosting Let’s go through each of these steps and see how they work.The  URIs you’ve seen earlier are fragile because they use handles.Here, , , and  are handles:The user may choose to change their  handle later, and it is important for that not to break any links between pieces of JSON already on the network.This is why, before you  an  URI, you should turn it into a canonical form by resolving the handle to something that never changes—an . An identity is like an account ID, but global and meant for the entire web. There are two mechanisms to resolve a handle to an identity (also known as a “DID”):Query the DNS TXT record at  looking for Make an HTTPS GET to https://<handle>/.well-known/atproto-didThe thing you’re looking for, the DID, is going to have a shape like . (We’ll revisit what that means later.)For example, let’s try to resolve  via the DNS mechanism:The  handle  to be owned by , whoever that may be. That’s all that we wanted to know at this point:Note this doesn’t  their association yet. We’ll need to verify that whoever controls the  identity “agrees” with  being their handle. The mapping is bidirectional. But we’ll confirm that in a later step.Now let’s try to resolve  using the DNS route:That also worked! The  handle claims to be owned by the did:plc:fpruhuo22xkm5o7ttr2ktxdo identity, whoever that may be:This DID looks a bit different than what you saw earlier but it’s also a valid DID. Again, it’s important to emphasize we’ve not confirmed the association yet.Subdomains like  can also be handles.The DNS mechanism didn’t work, so let’s try with HTTPS:That worked! This means that  handle claims to be owned by the did:plc:5c6cw3veuqruljoy5ahzerfx identity, whoever that is:So you get the idea. When you see a handle, you can probe it with DNS and HTTPS to see if it claims to be owned by some identity (a DID). If you found a DID, you’ll then be able to (1) verify it actually owns that handle, and (2) locate the server that hosts the data for that DID. And that will be the server you’ll ask for the JSON.In practice, if you’re building with AT, you’ll likely want to either deploy your own handle/did resolution cache or hit an existing one. (Here’s one implementation.)Now you know how handles resolve to identities, also known as DIDs. Unlike handles, which change over time, DIDs never change—they’re immutable.These  links, which use handles, are human-readable but fragile:They will break if one of us changes a handle again.In contrast, the  links below, which use DIDs, will not break until we either delete our accounts, delete these records, or permanently take down our hosting:So, really, this is the “true form” of an  URI:Think of  links with DIDs as “permalinks”. Any application  URIs should store them in this canonical form so that logical links between our pieces of JSON don’t break when we change our handles or change our hosting.Now that you know how to resolve a handle to a DID, you want to do two things:Verify that whoever owns this DID actually goes by that handle.Find the server that hosts all the data for this DID.You can do both of these things by fetching a piece of JSON called the . You can think of it as sort of a “passport” for a given DID.How you do that depends on what kind of DID it is.Currently, there are two kinds of DIDs, known as , supported by the AT protocol:  (a W3C draft) and  (specified by Bluesky).The  handle claims to be owned by :To check this claim, let’s find the DID Document for . The  method is a specification that specifies an algorithm for that.In short, you cut off the  from the DID, append  to the end, and run an HTTPS GET request:This DID Document looks sleep-inducing but it tells us three important things: The  field confirms that whoever controls  indeed wants to use  as a handle. ✅How to verify the integrity of their data. The  field tells us the public key with which all changes to their data are signed.Where their data is stored. The  field tells us the actual server with their data. Rudy’s data is currently hosted at .A DID Document really  like an internet passport for an identity: here’s their handle, here’s their signature, and here’s their location. It connects a handle to a hosting while letting the identity owner change  the handle  the hosting.Users who interact with  on different apps in the atmosphere don’t need to know or care about his DID  about his current hosting (and whether it moves). From their perspective, his current handle is the only relevant identifier. As for developers, they’ll refer to him by DID, which conveniently never changes.All of this sounds great, but there is one big downside to the  identity. If  ever loses control of the  domain, he will lose control over his DID Document, and thus over his entire identity.Let’s have a look at an alternative to  that avoids this problem.We already know the  handle claims to be owned by the did:plc:fpruhuo22xkm5o7ttr2ktxdo identity (actually, that’s me!)To check this claim, let’s find the DID Document for did:plc:fpruhuo22xkm5o7ttr2ktxdo.The DID Document itself works exactly the same way. It specifies: The  field confirms that whoever controls did:plc:fpruhuo22xkm5o7ttr2ktxdo uses  as a handle. ✅How to verify the integrity of my data. The  field tells us the public key with which all changes to my data are signed. The  field tells us the actual server with my data. It’s currently at https://morel.us-east.host.bsky.network.Although my handle is , the actual server storing my data is currently https://morel.us-east.host.bsky.network. I’m happy to keep hosting it there but I’m thinking of moving it to a host I control in the future. I can change both my handle and my hosting without disruption to my social apps.Unlike Rudy, who has a  identity, I stuck with  (which is the default one when you create an account on Bluesky) so that I’m not irrecovably tying myself to any web domain. “PLC” officially stands for a “Public Ledger of Credentials”—essentially, it is like an npm registry but for DID Documents. (Fun fact: originally PLC meant “placeholder” but they’ve decided it’s a good tradeoff.)The upside of a  identity is that I can’t lose my identity if I forget to renew a domain, or if something bad happens at the top level to my TLD.The downside of a  identity is that whoever operates the PLC registry has some degree of control over my identity. They can’t outright  it because every version is recursively signed with the hash of the previous version, every past version is queryable, and the hash of the initial version  the DID itself.So far, you’ve learned how to:Resolve a handle to a DID.Grab the DID Document for that DID.That actually tells you enough to get the JSON by its  URI!Each DID Document includes the  which is the actual hosting.  the service you can hit by HTTPS to grab any JSON record it stores.For example, the  handle resolves to , and its DID Document has a  pointing at .The  handle resolves to did:plc:fpruhuo22xkm5o7ttr2ktxdo.The DID Document for did:plc:fpruhuo22xkm5o7ttr2ktxdo points at https://morel.us-east.host.bsky.network as the current hosting.And that’s how you resolve an  URI.Exercise: In the record above, the  is a link to another record. Figure out the handle of its owner and the contents of that record. Use pdsls to check your answer.To resolve an arbitrary  URI, you need to follow three steps:Resolve the handle to an identity (using DNS and/or HTTPS).Resolve that identity to a hosting (using the DID Document).Request the JSON from that hosting (by hitting it with ).If you’re building a client app or a small project, an SDK will handle all of this for you. However, for good performance, you’ll want to hit a resolution cache instead of doing DNS/HTTPS lookups on every request. QuickDID is one such cache. You can also check out the pdsls source to see how exactly it handles resolution.In practice, a lot of apps don’t end up needing to resolve  URIs or load JSON records because they  data from the network via a websocket and aggregate it in a local database. If that’s your approach, you’ll still use the  URIs as unique identifiers for user-created data, but the data itself will get pushed to you rather than pulled by you. Still, it’s useful to know that you  fetch it on demand.The AT protocol is fundamentally an abstraction over HTTP, DNS, and JSON. But by standardizing how these pieces fit together—putting the user in the authority position, separating identity from hosting, and making data portable—it turns the web into a place where your content belongs to you, not to the apps that display it.There’s more to explore in the atmosphere, but now you know where it’s .]]></content:encoded></item><item><title>Why most product planning is bad and what to do about it</title><link>https://blog.railway.com/p/product-planning-improvement</link><author>ndneighbor</author><category>dev</category><category>hn</category><pubDate>Thu, 2 Oct 2025 19:34:08 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ENHANCE - a golang terminal UI for GitHub Actions</title><link>https://www.reddit.com/r/golang/comments/1nwdarl/enhance_a_golang_terminal_ui_for_github_actions/</link><author>/u/e-lys1um</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 2 Oct 2025 19:24:38 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I'm very excited to share what I've been working on!Introducing , a terminal UI for GitHub Actions that lets you easily see and interact with your PRs checks.It's available under a sponsorware model, more info on the site:This is an attempt to make my OSS development something sustainable. Happy to hear feedback about the model as well as the tool!]]></content:encoded></item><item><title>OpenAI&apos;s H1 2025: $4.3B in income, $13.5B in loss</title><link>https://www.techinasia.com/news/openais-revenue-rises-16-to-4-3b-in-h1-2025</link><author>breadsniffer</author><category>dev</category><category>hn</category><pubDate>Thu, 2 Oct 2025 18:37:28 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Developing a BASIC language interpreter in 2025</title><link>https://nanochess.org/ecs_basic.html</link><author>/u/ketralnis</author><category>dev</category><category>reddit</category><pubDate>Thu, 2 Oct 2025 18:15:39 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[
Recently, I had the chance to get an Intellivision II console along the ECS (Entertainment Computer System) and keyboard. I found and typed a game Bomb Run 1, using its integrated BASIC language, and I was pretty surprised to see how incredibly slow is it.

Apparently, Mattel Electronics only developed the ECS to avoid paying a daily penalty of $10,000 dollars to the USA government because they were advertising a keyboard component that wasn't yet available. They developed the ECS in secret, while putting all their money in the ill-fated Keyboard Component which was a full 6502 computer around the Intellivision with an officially licensed MS-Basic.

After getting the ECS on sale, Mattel forgot completely about it. It is a shame, because the keyboard is reasonable enough, it looks nice, and it could have been a nice starter BASIC platform.
    Anyway, the turtle speed intrigued me as I was pretty convinced that the Intellivision processor could do faster floating-point, so why not write my own extended BASIC interpreter?

    For the remaining of this article, the original Mattel ECS BASIC will be simply called ECS BASIC.

The implementation of my extended BASIC interpreter is for a General Instruments CP1610 processor. This is a 16-bit processor introduced in 1975, with a resemblance to the PDP-11. However, General Instruments didn't allowed second sources, only wanted big orders, and ignored small requests (basically shooting itself in the foot), and this processor was used widely only in the Mattel Intellivision, and it ceased production in 1985 at the same time of the Intellivision demise.

A full documentation of the instruction set is available along the jzintv emulator that can be downloaded from http://spatula-city.org/~im14u2c/intv/.I'll be using the basic 8K words between $5000-$6fff for the BASIC language, and I'll ignore altogether the EXEC (the Intellivision BIOS) and the ECS ROMs (I didn't even bother disabling it).
    I never coded a full BASIC language before, because it was not needed. In the eighties, I had access to a Z80 BASIC that was already ported to the homebrew computer I was using, and in the nineties, I managed to put floating-point in the Li-Chen Wang's Tiny BASIC, do some statement extensions for it, and also tried to do a tokenized BASIC, but I never went too far.

I started in Sep/17/2025 by coding a floating-point addition subroutine. I didn't had a format in mind, it only had to be 32-bit because the CP1610 registers are 16-bit, so two registers fit nicely for keeping a floating-point number, and another two for the second operand.

The format was decided in the basis of how so easy was to extract the sign, exponent, and the mantissa with 16-bit operations, as 8-bit operations are difficult to do. This automatically discarded an IEEE-754 compatible format, and I settled for a format based on a 24-bit mantissa in the higher bits, followed by the sign bit, and the exponent in the lower 7 bits. The exponent is a bit smaller than the classic IEEE-754, but we get an extra precision bit.

    The code for extracting the mantissa is pretty short:

    ANDI #$FF00,R1  ; Remove the sign and exponent.
    SETC
    RRC R0,1        ; Insert the top bit of the mantissa (fixed one)
    RRC R1,1        ; Now we have a 25-bit mantissa,
                    ; aligned at the higher bit.

    I didn't define denormalized numbers, nor infinity and NaN (Not a Number), because this wasn't supported in the BASIC interpreters of the eighties.

    It was easy to get the subtraction routine once I got the addition subroutine working, because you only need to flip the sign bit in the second operand.

    I did later the multiplication routine, but I stumbled over a problem where sometimes the mantissa overflowed. I simply added a very complicated code to move the mantissa by one bit. It was several days later when I studied it, and I discovered you can only have a result of x+y-1 bits or x+y bits (where x is the number of significant bits in the first operand, and y is the number of significant bits in the second operand), and I could optimize it by simply inserting an extra zero bit at the left to account for the carry.

    Of course, I made a small test program to check for the validity of the arithmetic operations with several cases. It took me four days to code the fully functional floating-point library.

I couldn't start a BASIC interpreter without the keyboard reading code, and terminal-style output. Fortunately, Joe Zbiciak (intvnut) already had developed routines for reading the ECS keyboard, and I integrated these with a ROM header, adding along terminal handling for displaying letters, scrolling the screen, and moving the cursor. 

    With all this integrated I had a dumb terminal working, you type anything on the keyboard, and you get the same keys displayed on the screen. This was Sep/19/2025.

    The CP1610 processor cannot address directly the internal memory in byte terms, instead everything is handled by full words. I had to take this in account for my tokenized BASIC representation. A standard Intellivision doesn't have enough memory for a BASIC interpreter, so the ECS BASIC included 2K of 8-bit RAM.

    However, a few years ago, the JLP-Flash cartridge was manufactured and it provides 8K of 16-bit RAM over $8000-$9fff, so for my extended BASIC this was excellent.

    When I talk about tokenization, I mean that all the language's reserved words are represented with a token. This speeds up the execution of the language, as it doesn't have to run a word match each time.

    My first version of the internal representation for BASIC lines was the line number as a word, followed by a pointer to the next line, followed by the tokenized BASIC code for the line, ended with a zero word.

    As I coded the line insertion routines in Sep/22/2025, I discovered the pointer to the next line wasn't a good idea, because it needed to move  pointer after a line insertion. Instead, I converted the pointer into a length (the number of words used by the tokenized line). This allowed for a very compact code to jump over lines:

    INCR R4     : Jump over the line number.
    ADD@ R4,R4  ; Add the tokenized length to the current pointer
                ; Et voila! It jumped over the line.

    With the line insertion routines completed, I went to implement the BASIC tokenization subroutine. I decided against handling tokenization byte-per-byte, and instead made each token a word. Of course, it is wasted space if you are using strings, but it is faster on execution. Token numbers start at $0100. It only remained to interface the input with the new routines.

    I decided to read the text directly from the screen, very unplanned, and probably buggy, but it has worked for the current time. And maybe later I'll extend it for a full-screen editor.

keywords:
	DECLE ":",0	; $0100
	DECLE "LIST",0
	DECLE "NEW",0
	DECLE "CLS",0
	DECLE "RUN",0	; $0104
	DECLE "STOP",0
	DECLE "PRINT",0
	DECLE "INPUT",0
	DECLE "GOTO",0	; $0108
	DECLE "IF",0
	DECLE "THEN",0
	DECLE "ELSE",0
	DECLE "FOR",0	; $010C
	DECLE "TO",0
	DECLE "STEP",0
	DECLE "NEXT",0
	DECLE "GOSUB",0	; $0110
Excerpt of the tokenization table.
    Now I was able to edit, correct, and delete BASIC code lines. The next logical step was the execution of the program. I implemented  by reading each program line sequentially, and each token found choose directly the command to execute.

    My first program was simply  and I was happy when I typed  and the screen was cleared.

    This was followed shortly by  and . Where  was only capable of putting a string on the screen, and  changed the execution flow. I added a check for the Esc key to exit an infinite loop.

    I was also pretty impatient to see if my extended BASIC language was speedier than the ECS BASIC, so I decided to implement , and a small expression parser supporting the relational operators, and the basic arithmetic operators (, ,  and ), along numbers and variables.

    The numbers were simply read as integers and converted to floating-point format, while the variables used 26 double-word memory spaces covering the A to Z variables.

    In order to create a loop, it was required to implement variable assignment.

    10 A=1
    20 PRINT "Hello"
    30 A=A+1
    40 IF 6>A THEN 20

The tokenization of the BASIC program.

    For some reason, I couldn't type the less-than operator with the emulated ECS keyboard. Later, I discovered that intvnut missed the character in the Shift table, and it was a matter of a simple fix.
It was past midnight when I finally could try the benchmark. As I didn't had yet a  statement, I had to replicate it using increment and comparison.
I ran it, and I was amazed when I discovered it took only 15 seconds. In the ECS BASIC it takes 210 seconds! There are screenshots of the programs in the git.
More floating-point curiosities
    This wasn't the first time I programmed a floating-point package. My first one was for a Z80-based computer, I don't remember if it was complete, if it had bugs, or if it was actually used. What I can remember is that I was never able to make a proper subroutine for displaying floating-point numbers. I got stuck with a simple conversion to integer, and printing the integer. 

    The display of a number followed by fraction and exponent, for me was closer to black magic than anything. I believed that a single routine did everything, but I was wrong. And I came to illumination by reading a Commodore 64 BASIC manual, it says something like numbers in this range are displayed complete, while in other cases the number will be displayed in exponent format.

    This triggered a pattern in my mind: If the whole integer fits in the mantissa, display it alongside a small fraction, and if the number doesn't fit, make it bigger or smaller so it fits in an integer, and this one can be displayed in exponent format.

    The algorithm is as follows:

        The 25-bit mantisa allows integers up from 0 to 33554431. We limit it to the biggest integer all nines, or 9999999.
    
        If the floating-point number is less than 10,000,000 then the integer part is displayed, and then it gets 2 fraction digits.
    
        If the floating-point number is less than 0.01 then it is multiplied until it reachs the range 1,000,000 - 9,999,999. The first digit will be the integer part, the following digits will be the fractional part, and the exponent will be displayed along.
    
        If the floating-point number is greater than 9,999,999 then it is divided by 10 until it fits the same range. And again display like in step 3.
    
    And this way, thirty years later, I discovered printing floating-point numbers isn't so obscure as I believed, but indeed it has a lot of magic.

    The statements  allow to create small subroutines, and these have their own stack to keep track of where to return (a pointer plus the line number)

The  loop is one of the most known statements of the core BASIC language. Implementing this required a redesign of my execution loop, because I was doing it line-by-line, but the  changed the line, but on the next statement it would lost track and get back to the line following the .

The loops also require their own stack, but including the counter variable address, a pointer to the  expression, and a pointer to the  expression (5 words in total)

The  statement was replaced with a code that runs sequentially over the tokens, and jumps over the line headers. This way is easier to change the execution flow to a new token.

I also added the negation operator (required for ) and some functions like , , , and . The  function in particular allows to create little games for guessing numbers, and so.

Checking against the ECS BASIC, I was only missing , , and . So I bite the bullet to implement these. Adding along .
 for creating arrays was pretty easy, and I adjusted all the variable access paths of the interpreter in a way that any indexed access is the same as accessing a normal variable.

At this point, my extended BASIC language was already orders of magnitude faster than the ECS BASIC, and it could be used to write little text games (well, using only numbers)

However, it didn't handled yet the controllers, sound, graphics, and sprites. The ECS BASIC had some statements for it, but the sprites cannot be defined, and instead these had to be "grabbed" from a game cartridge. Of course, the user was limited to these game sprites. Also positioning sprites was done with multiple variable assignments in an array-like style of access.

For my extended BASIC I decided for a kind of advanced statements patterned after the ones from my compiled IntyBASIC language but not exactly the same:
 for setting the color stack mode. for setting the foreground/background mode. for defining GRAM cards. for setting the paint color used in . for displaying a sprite on the screen. for waiting the next video frame. for accessing the sound chip. for reading the 16 disc directions. for reading the side-buttons. for reading the keypad. for accesing the screen.
Once these were implemented, I started coding a minimal game to test the interpreter, and I called it UFO Invasion. Of course, I found a few bugs in my interpreter and fixed them.

The game was working, and at a reasonable speed. What about testing in real hardware? I loaded the interpreter into a LTO-Flash cartridge and connected my ECS system.

My first attempt crashed continuously. I lost half an hour looking for errors, until I noticed  crashed the interpreter. I had forgot to enable the extra RAM of the JLP cartridge. And finally it worked!

Typing the program was difficult, as the keyboard bounced a lot. This happens when you read too fast the keyboard, so fast you can see that effectively the key contact isn't perfect. I had to add a small wait before reading the keyboard, and it solved most of the problems.

    At the end, my extended BASIC interpreter was coded in six days! I think it is way faster when you are enjoying programming it.
10 CLS:REM UFO INVASION. NANOCHESS 2025
20 DEFINE 0,"183C00FF007E3C000018183C3C7E7E000000183C3C3C3C7EFF2400"
50 x=96:w=0:v=0:u=0:t=159
60 SPRITE 0,776+x,344,2061
70 SPRITE 1,776+v,256+w,2066
80 SPRITE 2,1796+t,256+u,6149
90 WAIT:c=STICK(0)
100 IF c>=3 AND c<=7 THEN IF x<152 THEN x=x+4
110 IF c>=11 AND c<=15 THEN IF x>0 THEN x=x-4
120 IF w=0 THEN SOUND 2,,0:IF STRIG(0) THEN v=x:w=88
130 t=t+5:IF t>=160 THEN t=0:u=INT(RND*32)+8
140 IF w THEN SOUND 2,w+20,12:w=w-4:IF ABS(w-u)<8 AND ABS(v-t)<8 THEN
    t=164:w=0:SOUND 3,8000,9:SOUND 1,2048,48
150 GOTO 60

UFO Invasion running on the Mattel Intellivision ECS, and a partial listing of the game.

A big difference against "standard" BASIC is the lack of proper strings. In the ECS BASIC, you could read a string from the keyboard using GET, and put it again on the screen using PUT, but that was all.

Adding the support for standard strings would mean it could run some text-processing programs like Eliza in BASIC, and some other small games could be easily translated.

This was one of the portability things that the BASIC language had at the time, and it was used by many books in a way that the programs were written with that "core" BASIC language in mind, and these could be typed into almost any computer with a decent interpreter.

The source code is released at https://github.com/nanochess/ecsbasic. I tried to release it so early as possible, so you can get a glance of how it was growing in the commits.

Enjoy it! After publishing this article in Sep/28/2025, several people pointed to me that I didn't explained why the ECS BASIC was so slow. Truth to be told, I was so happy with my working extended BASIC that I didn't even bother to look more on the ECS BASIC.First and all, there is a thread in Atariage about the ECS BASIC Color Patent, and the eighth post also by intvnut explains in great detail how he disassembled the code and found a terrible way of doing a shift of the floating-point accumulator.However, there are a few other details that make it slow. For example, the extra RAM is 2K of , and all the Intellivision memory accesses are for 16 bits (one word), so every single access to variables requires the SDBD instruction. This instruction tells the CP1610 processor to read the word in two steps.I did my own disassembly, and after giving a look around the same zone disassembled by intvnut, I found this code that extracts the exponent of a floating-point number:
 $E1DD: PSHR R5                                     
 $E1DE: MVI@ R1,R2                                  
 $E1DF: ANDI #$007F,R2                              
 $E1E1: MOVR R2,R5                                  
 $E1E2: ANDI #$0040,R5                              
 $E1E4: BNEQ $E1E8                                  
 $E1E6: NEGR R2                                     
 $E1E7: PULR R7                                     
 $E1E8: XORI #$0040,R2                              
 $E1EA: PULR R7                                     
It extracts the seven bits of the exponent. In the range $00-$3f makes it negative, and the range $40-$7f is converted to $00-$3f. So simply reading the exponent takes 7 instructions. Whoever developed this code didn't take in account that you could save the exponent in two's complement format offset by $40, and it is used nine times.For comparison, my code for extracting the exponent is simply .It gets worst when I found the code calling $E1DD, and it is for extracting two exponents and doing a comparison between both:
 $E147: PSHR R5                                     
 $E148: MOVR R3,R1                                  
 $E149: JSR  R5,$E1DD                               
 $E14C: MOVR R2,R0                                  
 $E14D: MOVR R4,R1                                  
 $E14E: JSR  R5,$E1DD                               
 $E151: CMPR R0,R2                                  
 $E152: BEQ  $E159                                  
 $E154: BMI  $E15B                                  
 $E156: MVII #$0001,R0                              
 $E158: PULR R7                                     
 $E159: CLRR R0                                     
 $E15A: PULR R7                                     
 $E15B: CLRR R0                                     
 $E15C: DECR R0                                     
 $E15D: PULR R7                                     
My code for exponent comparison is composed of only three instructions (two AND and one CMPR) This big code would be kind of reasonable if it wasn't for the fact that it is only called , and it is by the floating-point addition subroutine starting at $E059:
 $E067: CLRR R0                                     
 $E068: SDBD                                        
 $E069: MVII #$47D4,R3                              
 $E06C: MOVR R3,R4                                  
 $E06D: SUBI #$0007,R4                              
 $E06F: JSR  R5,$E147                               
 $E072: TSTR R0                                     
 $E073: BEQ  $E081                                  
 $E075: MOVR R4,R1                                  
 $E076: TSTR R0                                     
 $E077: BMI  $E07B                                  
 $E079: ADDI #$0007,R1                              
 $E07B: CLRR R0                                     
 $E07C: JSR  R5,$E15E                               
 $E07F: B    $E067                                  
This routine calls the exponent comparison at $E06F, and if both are equal it jumps out to $E081, else it adjusts the exponent of one number, and  the comparison (notice the B $E067 instruction) The mantissa shifting routine at $E15E operates shifting in steps of 4 bits.At $E081 (not shown), it checks the sign of the second operand, and if it is negative, it calls $E194 to do a negation of the number.At $E091 it does the addition of the two numbers calling $E1C0, and calls $E183 to check if both signs are equal.The code at $E0A4 reinserts the exponent in a very slow way, and it ends by calling $E21A to normalize the floating-point number, again shifting the mantissa in steps of 4 bits.It has been just too much code yet, but let's look at the shifting routine:
 $E238: MVI@ R1,R2                                  
 $E239: MOVR R2,R5                                  
 $E23A: ANDI #$000F,R2                              
 $E23C: SLL  R2,2                                   
 $E23D: SLL  R2,2                                   
 $E23E: XORR R4,R2                                  
 $E23F: MVO@ R2,R1                                  
 $E240: MOVR R5,R2                                  
 $E241: ANDI #$00F0,R2                              
 $E243: SLR  R2,2                                   
 $E244: SLR  R2,2                                   
 $E245: MOVR R2,R4                                  
 $E246: DECR R1                                     
 $E247: SDBD                                        
 $E248: CMPI #$47CD,R1                              
 $E24B: BNEQ $E238                                  
It takes a byte, shifts it left 4 bits, inserts the carry, and copies the extra 4 bits as the new carry. I couldn't resist showing how it could be made a lot smaller and faster this way:
LE238:
    MVI@ R1,R2  ; R2 = 0x00ff
    SLL R2,2
    SLL R2,2    ; R2 = 0x0ff0
    XORR R4,R2  ; R2 = 0x0ff0 + carry
    MVO@ R2,R1  ; This saves the low byte.
    SWAP R2     ; R2 = 0xf00f
    ANDI #$000F,R2
    MOVR R2,R4  ; R4 = 0x000f
    DECR R1
    SDBD
    CMPI #$47CD,R1
    BNEQ LE238
This saves four instructions in the loop, and it is faster.The ECS BASIC has a mantissa of six bytes, so it has more precision than my extended BASIC (three bytes), but it is done in a very slow way!For sure the ECS BASIC could be optimized to run at least two times faster.Ok, but I haven't yet answered a simple question. How many cycles takes adding 3.0 and 7.0?I ran the jzintv emulator with this command line (remember we need a game cartridge so the ECS BASIC works):./jzintv -d -s1 -z3 Basketball.binI entered the R command to make it run the ECS BASIC. When I reached the ECS BASIC, I typed PRIN 3+7 and before pressing Enter, I went to the debugger window and pressed Ctrl+C. Then I put breakpoints at selected places (start of the floating-point addition, the place where it calls the addition, and the return instruction):Again, I typed the R command, using along M47C0 to watch the memory addresses where floating-point addition happened until I saw the numbers 3 and 7.I took note of the cycle number at the right. The operation 3+7 takes exactly 1558 cycles. By the way, the interpreter executes further three floating point additions when processing the numbers, and another two for displaying the number.
jzintv debug window with the ECS BASIC. The cycle count at the right shows how much time takes a floating-point addition.
Now, let's do the same with my extended BASIC interpreter.After assembling with as1600, I generate a .lst file where I searched for the  label (start) and  label (return) The addresses are $61d1 and $625b.I ran again the jzintv emulator with the debugging option, and I typed PRINT 3+7, and before pressing Enter, I stopped the debugger using Ctrl+C, and I setup these breakpoints:Then I entered the command R, and pressed Enter on the BASIC screen. It took me four R commands to see the values 3 and 7 in the registers, and the result 10.And the total cycles used were 479. This means that only in the floating-point addition the Mattel ECS BASIC is five times slower.
jzintv debug window with my extended ECS BASIC. The cycle count at the right shows how much time takes a floating-point addition.
Last modified: Sep/29/2025]]></content:encoded></item><item><title>In C++ modules globally unique module names seem to be unavoidable</title><link>https://nibblestew.blogspot.com/2025/09/in-c-modules-globally-unique-module.html</link><author>/u/ketralnis</author><category>dev</category><category>reddit</category><pubDate>Thu, 2 Oct 2025 18:12:06 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Writing out C++ module files and importing them is awfully complicated. The main cause for this complexity is that the C++ standard can not give requirements like "do not engage in Vogon-level stupidity, as that is not supported". As a result implementations have to support anything and everything under the sun. For module integration there are multiple different approaches ranging from custom on-the-fly generated JSON files (which neither Ninja nor Make can read so you need to spawn an extra process per file just to do the data conversion, but I digress) to custom on-the-fly spawned socket server daemons that do something. It's not really clear to me what.Instead of diving to that hole, let's instead approach the problem from first principles from the opposite side.A single project consists of a single source tree. It consists of a single executable E and a bunch of libraries L1 to L99, say. Some of those are internal to the project and some are external dependencies. For simplicity we assume that they are embedded as source within the parent project. All libraries are static and are all linked to the executable E.With a non-module setup each library can have its own header/source pair with file names like  and . All of those can be built and linked in the same executable and, assuming their symbol names won't clash, work just fine. This is not only supported, but in fact quite common.The dream, then, is to convert everything to modules and have things work just as they used to.If all libraries were internal, it could be possible to enforce that the different util libraries get different module names. If they are external, you clearly can't. The name is whatever upstream chooses it to be. There are now two modules called  in the build and it is the responsibility of someone (typically the build system, because no-one else seems to want to touch this) to ensure that the two module files are exposed to the correct compilation commands in the correct order.This is complex and difficult, but once you get it done, things should just work again. Right?That is what I thought too, but that is actually not the case. This very common setup does not work, and to work. You don't have to take my word for it, here is a quote from the GCC bug tracker:This is already IFNDR, and can cause standard ODR-like issues as the name of the module is used as the discriminator for module-linkage entities and the module initialiser function.  Of course that only applies if both these modules get linked into the same executable;IFNDR (ill-formed, no diagnostic required) is a technical term for "if this happens to you, sucks to be you". The code is broken and the compiler is allowed to whatever it wants with it (including s.What does it mean in practice?According to my interpretation of thiscomment (which, granted, might be incorrect as I am not a compiler implementer) if you have an executable and you link into it any code that has multiple modules with the same name, the end result is broken. It does not matter how the same module names get in, the end result is broken. No matter how much you personally do not like this and think that it should not happen, it will happen and the end result is broken.At a higher level this means that this property forms a namespace. Not a C++ namespace, but a sort of a virtual name space. This contains all "generally available" code, which in practice means all open source library code. As that public code can be combined in arbitrary ways it means that if you want things to work, module names must be globally unique in that set of code (and also in every final executable). Any duplicates will break things in ways that can only be fixed by renaming all but one of the clashing modules.Globally unique modules names is thus not a "recommendation", "nice to have" or "best practice". It is a  that comes directly from the compiler and standard definition.If we accept this requirement and build things on top of it, things suddenly get a lot simpler. The build setup for modules reduces to the following for projects that build all of their own modules:At the top of the build dir is a single directory for modules (GCC already does this, its directory is called )All generated module files are written in that directory, as they all have unique names they can not clashAll module imports are done from that directoryModule mappers and all related complexity can be dropped to the floor and ignoredImporting modules from the system might take some more work (maybe copy Fortran and have a  flag for module search paths). However at the time of writing GCC and Clang module files are not stable and do not work between different compiler versions or even when compiler flags differ between export and import. Thus prebuilt libraries can not be imported as modules from the system until that is fixed. AFAIK there is no timeline for when that will be implemented.So now you have two choices:Accept reality and implement a system that is simple, reliable and working.Reject reality and implement a system that is complicated, unreliable and broken.[Edit, fixed quote misattribution.]]]></content:encoded></item><item><title>ZLUDA update Q3 2025 – ZLUDA 5 is here</title><link>https://vosen.github.io/ZLUDA/blog/zluda-update-q3-2025/</link><author>/u/vosen_l</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Thu, 2 Oct 2025 18:08:43 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Babel is why I keep blogging with Emacs</title><link>https://entropicthoughts.com/why-stick-to-emacs-blog</link><author>ibobev</author><category>dev</category><category>hn</category><pubDate>Thu, 2 Oct 2025 18:06:41 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[
Every time I look at someone’s simple static site generation setup for their
blog, I feel a pang of envy. I’m sure I could make a decent blogging engine in
2,000 lines of code, and it would be something I’d understand, be proud over,
able to extend, and willing to share with others.

Instead, I write these articles in Org mode, and use mostly the standard Org
publishing functions to export them to . This is sometimes brittle, but
most annoyingly, I don’t understand it. I have been asked for details on how my
publishing flow works, but the truth is I have no idea what happens when I run
the  command.

I could find out by tracing the evaluation of the Lisp code that runs on export,
but I won’t, because just the  exporting code () is 5,000
lines of complexity. The general exporting framework ( and
) is 8,000 lines. The framework depends on Org parsing code
() which is at least another 9,000 lines. This is over 20,000
lines of complexity I’d need to contend with.

It might seem like a no-brainer to just write that 2,000 line custom static
generator and use that instead.

Any lightweight markup format (like Markdown or ReStructuredText or whatever)
allows for embedding code blocks, but Org, through Babel, can  that code on
export, and then display the output in the published document, even when the
output is a table or an image. It supports sessions that lets code reuse
definitions from earlier code blocks. It allows for injecting variables from the
markup into the code, and vice versa. As a bonus, Org doesn’t require a
JavaScript syntax highlighter, because it generates inline styles in the source
code.

It does this for a large number of languages, although I mainly use it with R
for drawing plots. Being able to do this is incredibly convenient, because it
makes it trivial to draft data, illustrations, and text at the same time,
adjusting both until the article coheres. Having tried it, I cannot see myself
living without it.

A simple 2,000 line blogging engine would be a fun weekend project. Mirroring
the features of Babel I use would turn it into a multi-month endeavour for
someone with limited time such as myself. Not going to happen, and I will
continue to beat myself up for overcomplicating my publishing workflow.
]]></content:encoded></item><item><title>It&apos;s easy to take image rendering in a terminal as granted, let alone video rendering. It&apos;s so cool when you think about it.</title><link>https://www.reddit.com/r/linux/comments/1nwaxlf/its_easy_to_take_image_rendering_in_a_terminal_as/</link><author>/u/keremdev</author><category>dev</category><category>reddit</category><pubDate>Thu, 2 Oct 2025 17:56:16 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Props to kitty/sixel devs for this, ofc it's terminal IO bound but it's still really really cool.   submitted by    /u/keremdev ]]></content:encoded></item><item><title>Signal Messenger&apos;s SPQR for post-quantum ratchets, written in formally-verified Rust</title><link>https://signal.org/blog/spqr/</link><author>/u/kibwen</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Thu, 2 Oct 2025 17:54:22 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[We are excited to announce a significant advancement in the security of the Signal Protocol: the introduction of the Sparse Post Quantum Ratchet (SPQR). This new ratchet enhances the Signal Protocol’s resilience against future quantum computing threats while maintaining our existing security guarantees of forward secrecy and post-compromise security.The Signal Protocol is a set of cryptographic specifications that provides end-to-end encryption for private communications exchanged daily by billions of people around the world. After its publication in 2013, the open source Signal Protocol was adopted not only by the Signal application but also by other major messaging products. Technical information on the Signal Protocol can be found in the specifications section of our docs site.In a previous blog post, we announced the first step towards advancing quantum resistance for the Signal Protocol: an upgrade called PQXDH that incorporates quantum-resistent cryptographic secrets when chat sessions are established in order to protect against harvest-now-decrypt-later attacks that could allow current chat sessions to become compromised if a sufficiently powerful quantum computer is developed in the future. However, the Signal Protocol isn’t just about protecting cryptographic material and keys at the beginning of a new chat or phone call; it’s also designed to minimize damage and heal from compromise as that conversation continues.We refer to these security goals as Forward Secrecy (FS) and Post-Compromise Security (PCS). FS and PCS can be considered mirrors of each other: FS protects past messages against future compromise, while PCS protects future messages from past compromise. Today, we are happy to announce the next step in advancing quantum resistance for the Signal Protocol: an additional regularly advancing post-quantum ratchet called the Sparse Post Quantum Ratchet, or SPQR. On its own, SPQR provides secure messaging that provably achieves these FS and PCS guarantees in a quantum safe manner. We mix the output of this new ratcheting protocol with Signal’s existing Double Ratchet, in a combination we refer to as the Triple Ratchet.What does this mean for you as a Signal user? First, when it comes to your experience using the app, nothing changes. Second, because of how we’re rolling this out and mixing it in with our existing encryption, eventually all of your conversations will move to this new protocol without you needing to take any action. Third, and most importantly, this protects your communications both now and in the event that cryptographically relevant quantum computers eventually become a reality, and it allows us to maintain our existing security guarantees of forward secrecy and post-compromise security as we proactively prepare for that new world.The Current State of the Signal ProtocolThe original Signal ratchet uses hash functions for FS and a set of elliptic-curve Diffie Hellman (ECDH) secret exchanges for PCS. The hash functions are quantum safe, but elliptic-curve cryptography is not. An example is in order: our favorite users, Alice and Bob, establish a long-term connection and chat over it regularly. During that session’s lifetime, Alice and Bob regularly agree on new ECDH secrets and use them to “ratchet” their session. Mean ol’ Mallory records the entire (encrypted) communication, and really wants to know what Alice and Bob are talking about.The concept of a “ratchet” is crucial to our current non-quantum FS/PCS protection. In the physical world, a ratchet is a mechanism that allows a gear to rotate forward, but disallows rotation backwards. In the Signal Protocol, it takes on a similar role. When Alice and Bob “ratchet” their session, they replace the set of keys they were using prior with a new set based on both the older secrets and a new one they agree upon. Given access to those new secrets, though, there’s no (non-quantum) way to compute the older secrets. By being “one-way”, this ratcheting mechanism provides FS.The ECDH mechanism allows Alice and Bob to generate new, small (32 bytes) data blobs and attach them to every message. Whenever each party receives a message from the other, they can locally (and relatively cheaply) use this data blob to agree on a new shared secret, then use that secret to ratchet their side of the protocol. Crucially, ECDH also allows Alice and Bob to both agree on the new secret without sending that secret itself over their session, and in fact without sending anything over the session that Mallory could use to determine it. This description of Diffie-Hellman key exchange provides more details on the concepts of such a key exchange, and this description of ECDH provides specific details on the variant used by the current Signal protocol.Sometime midway through the lifetime of Alice and Bob’s session, Mallory successfully breaches the defences of both Alice and Bob, gaining access to all of the (current) secrets used for their session at the time of request. Alice and Bob should have the benefits of Forward Secrecy - they’ve ratcheted sometime recently before the compromise, so no messages earlier than their last ratchet are accessible to Mallory, since ratcheting isn’t reversible. They also retain the benefits of Post-Compromise Security. Their ratcheting after Mallory’s secret access agrees upon new keys that can’t be gleaned just from the captured data they pass between each other, re-securing the session.Should Mallory have access to a quantum computer, though, things aren’t so simple. Because elliptic curve cryptography is not quantum resistant, it’s possible that Mallory could glean access to the secret that Alice and Bob agreed upon, just by looking at the communication between them. Given this, Alice and Bob’s session will never “heal”; Mallory’s access to their network traffic from this point forward will allow her to decrypt all future communications.Mixing In Quantum SecurityIn order to make our security guarantees stand up to quantum attacks, we need to mix in secrets generated from quantum secure algorithms. In PQXDH, we did this by performing an additional round of key agreement during the session-initiating handshake, then mixing the resulting shared secret into the initial secret material used to create Signal sessions. To handle FS and PCS, we need to do continuous key agreement, where over the lifetime of a session we keep generating new shared secrets and mixing those keys into our encryption keys.Luckily there is a tool designed exactly for this purpose: the quantum-secure Key-Encapsulation Mechanism (KEM). KEMs share similar behavior to the Diffie-Hellman mechanisms we described earlier, where two clients provide each other with information, eventually deciding on a shared secret, without anyone who intercepts their communications being able to access that secret. However, there is one important distinction for KEMs - they require ordered, asymmetric messages to be passed between their clients. In ECDH, both clients send the other some public parameters, and both combine these parameters with their locally held secrets and come up with an identical shared secret. In the standardized ML-KEM key-encapsulation mechanism, though, the initiating client generates a pair of encapsulation key (EK) and decapsulation key (DK) (analogous to a public and private key respectively) and sends the EK. The receiving client receives it, generates a secret, and wraps it into a ciphertext (CT) with that key. The initiating client receives that CT and decapsulates with its previously generated DK. In the end, both clients have access to the new, shared secret, just through slightly different means.Wanting to integrate this quantum-secure key sharing into Signal, we could take a simple, naive approach for each session. When Alice initiates a session with Bob,Alice, with every message she sends, sends an EKBob, with every message he receives, generates a secret and a CT, and sends the CT backAlice, on receiving a CT, extracts the secret with her DK and mixes it inThis initially simple-looking approach, though, quickly runs into a number of issues we’ll need to address to make our protocol actually robust. First, encapsulation keys and CTs are large - over 1000 bytes each for ML-KEM 768, compared to the 32 bytes required for ECDH. Second, while this protocol works well when both clients are online, what happens when a client is offline? Or a message is dropped or reordered? Or Alice wants to send 10 messages before Bob wants to send one?Some of these problems have well-understood solutions, but others have trade-offs that may shine in certain circumstances but fall short in others. Let’s dive in and come to some conclusions.How does Alice decide what to send based on what Bob needs next, and vice versa? If Bob hasn’t received an EK yet, she shouldn’t send the next one. What does Bob send when he hasn’t yet received an EK from Alice, or when he has, but he’s already responded to it? This is a common problem when remote parties send messages to communicate, so there’s a good, well-understood solution: a state machine. Alice and Bob both keep track of “what state am I in”, and base their decisions on that. When sending or receiving a message, they might also change their state. For example:Alice wants to send a message, but she’s in a StartingA state, so she doesn’t have an EK. So, she generates an EK/DK pair, stores them locally, and transitions to the SendEK stateAlice wants to send a message and is in the SendEK state. She sends the EK along with the messageAlice wants to send another message, but she’s still in the SendEK state. So, she sends the EK with the new message as wellBob receives the message with the EK. He generates a secret and uses the EK to create a CT. He transitions to the SendingCT state.Bob wants to send a message and he’s in the SendingCT state. He sends the CT along with the messageBob wants to send a message and he’s in the SendingCT state. He sends the CT along with the messageBy crafting a set of states and transitions, both sides can coordinate what’s sent. Note, though, that even in this simple case, we see problems. For example, we’re sending our (large) EK and (large) CT multiple times.We’ve already mentioned that the size of the data we’re sending has increased pretty drastically, from 32 bytes to over 1000 per message. But bandwidth is expensive, especially on consumer devices like client phones, that may be anywhere in the world and have extremely varied costs for sending bytes over the wire. So let’s discuss strategies for conserving that bandwidth.First, the simplest approach - don’t send a new key with every message. Just, for example, send with every 50 messages, or once a week, or every 50 messages unless you haven’t sent a key in a week, or any other combination of options. All of these approaches tend to work pretty well in online cases, where both sides of a session are communicating in real-time with no message loss. But in cases where one side is offline or loss can occur, they can be problematic. Consider the case of “send a key if you haven’t sent one in a week”. If Bob has been offline for 2 weeks, what does Alice do when she wants to send a message? What happens if we can lose messages, and we lose the one in fifty that contains a new key? Or, what happens if there’s an attacker in the middle that wants to stop us from generating new secrets, and can look for messages that are 1000 bytes larger than the others and drop them, only allowing keyless messages through?Another method is to chunk up a message. Want to send 1000 bytes? Send 10 chunks of 100 bytes each. Already sent 10 chunks? Resend the first chunk, then the second, etc. This smooths out the total number of bytes sent, keeping individual message sizes small and uniform. And often, loss of messages is handled. If chunk 1 was lost, just wait for it to be resent. But it runs into an issue with message loss - if chunk 99 was lost, the receiver has to wait for all of chunks 1-98 to be resent before it receives the chunk it missed. More importantly, if a malicious middleman wants to stop keys from being decided upon, they could always drop chunk 3, never allowing the full key to pass between the two parties.We can get around all of these issues using a concept called erasure codes. Erasure codes work by breaking up a larger message into smaller chunks, then sending those along. Let’s consider our 1000 byte message being sent as 100 byte chunks again. After chunk #10 has been sent, the entirety of the original 1000 byte message has been sent along in cleartext. But rather than just send the first chunk over again, erasure codes build up a new chunk #11, and #12, etc. And they build them in such a way that, once the recipient receives any 10 chunks in any order, they’ll be able to reconstruct the original 1000 byte message.When we put this concept of erasure code chunks together with our previous state machine, it gives us a way to send large blocks of data in small chunks, while handling messages that are dropped. Crucially, this includes messages dropped by a malicious middleman: since any N chunks can be used to recreate the original message, a bad actor would need to drop all messages after #N-1 to disallow the data to go through, forcing them into a complete (and highly noticeable) denial of service. Now, if Alice wants to send an EK to Bob, Alice will:Transition from the StartingA state to the SendingEK state, by generating a new EK and chunking itWhile in the SendingEK state, send a new chunk of the EK along with any messages she sendsWhen she receives confirmation that the recipient has received the EK (when she receives a chunk of CT), transition to the ReceivingCT stateTransition from the StartingB state to the ReceivingEK state when he receives its first EK chunkKeep receiving EK chunks until he has enough to reconstruct the EKAt that point, reconstruct the EK, generate the CT, chunk the CT, and transition to the SendingCT stateFrom this point on, he will send a chunk of the CT with every messageOne interesting way of looking at this protocol so far is to consider the messages flowing from Alice to Bob as potential capacity for sending data associated with post-quantum ratcheting: each message that we send, we could also send additional data like a chunk of EK or of the CT. If we look at Bob’s side, above, we notice that sometimes he’s using that capacity (IE: in step 4 when he’s sending CT chunks) and sometimes he’s not (if he sends a message to Alice during step 2, he has no additional data to send). This capacity is pretty limited, so using more of it gives us the potential to speed up our protocol and agree on new secrets more frequently.A Meditation On How Faster Isn’t Always BetterWe want to generate shared secrets, then use them to secure messages. So, does that mean that we want to generate shared secrets as fast as possible? Let’s introduce a new term: an epoch. Alice and Bob start their sessions in epoch 0, sending the EKs for epoch 1 (EK#1) and associated ciphertext (CT#1) to each other. Once that process completes, they have a new shared secret they use to enter epoch 1, after which all newly sent messages are protected by the new secret. Each time they generate a new shared secret, they use it to enter a new epoch. Surely, every time we enter a new epoch with a new shared secret, we protect messages before that secret (FS) and after that secret (PCS), so faster generation is better? It seems simple, but there’s an interesting complexity here that deserves attention.First, let’s discuss how to do things faster. Right now, there’s a lot of capacity we’re not using: Bob sends nothing while Alice sends an EK, and Alice sends nothing while Bob sends a CT. Speeding this up isn’t actually that hard. Let’s change things so that Alice sends EK#1, and once Bob acknowledges its receipt, Alice immediately generates and sends EK#2. And once she notices Bob has received that, she generates and sends EK#3, etc. Whenever Alice sends a new message, she always has data to send along with it (new EK chunks), so she’s using its full capacity. Bob doesn’t always have a new CT to send, but he is receiving EKs as fast as Alice can send them, so he often has a new CT to send along.But now let’s consider what happens when an attacker gains access to Alice. Let’s say that Alice has sent EK#1 and EK#2 to Bob, and she’s in the process of sending EK#3. Bob has acknowledged receipt of EK#1 and EK#2, but he’s still in the process of sending CT#1, since in this case Bob sends fewer messages to Alice than vice versa. Because Alice has already generated 3 EKs she hasn’t used, Alice needs to keep the associated DK#1, DK#2, and DK#3 around. So, if at this point someone maliciously gains control of Alice’s device, they gain access to both the secrets associated with the current epoch (here, epoch 0) and to the DKs necessary to reconstruct the secrets to other epochs (here, epochs 1, 2, and 3) using only the over-the-wire CT that Bob has yet to send. This is a big problem: by generating secrets early, we’ve actually made the in-progress epochs and any messages that will be sent within them less secure against this single-point-in-time breach.To test this out, we at Signal built a number of different state machines, each sending different sets of data either in parallel or serially. We then ran these state machines in numerous simulations, varying things like the ratio of messages sent by Alice vs Bob, the amount of data loss or reordering, etc. And while running these simulations, we tracked what epochs’ secrets were exposed at any point in time, assuming an attacker were to breach either Alice’s or Bob’s secret store. The results showed that, in general, while simulations that handled multiple epochs’ secrets in parallel (IE: by sending EK#2 before receipt of CT#1) did generate new epochs more quickly, they actually made more messages vulnerable to a single breach.But Let’s Still Be EfficientThis still leaves us with a problem, though: the capacity present in messages we send in either direction is still a precious resource, and we want to use it as efficiently as possible. And our simple approach of Alice’s “send EK, receive CT, repeat” and Bob’s “receive EK, send CT, repeat” leaves lots of time where Alice and Bob have nothing to send, should that capacity be available.To improve our use of our sending capacity, we decided to take a harder look into the ML-KEM algorithm we’re using to share secrets, to see if there was room to improve. Let’s break things down more and share some actual specifics on the ML-KEM algorithm.Alice generates an EK of 1184 bytes to send to Bob, and an associated DKBob samples a new shared secret (32 bytes), which he encrypts with EK into a CT of 1088 bytes to send to AliceAlice receives the CT, uses the DK to decrypt it, and now also has access to the 32 byte shared secretDiving in further, we can break out step #3 into some sub-stepsAlice generates an EK of 1184 bytes to send to Bob, and an associated DKBob samples a new shared secret (32 bytes), which he encrypts with EK into a CT of 1088 bytes to send to Alice Bob creates a new shared secret S and sampled randomness R by sampling entropy and combining it with a hash of EKBob hashes the EK into a HashBob pulls 32 bytes of the EK, a SeedBob uses the Seed and R to generate the majority of the CTBob then uses S and EK to generate the last portion of the CTAlice receives the CT, uses the DK to decrypt it, and now also has access to the 32 byte shared secretStep 3.d, which generates 960 bytes of the 1088-byte CT, only needs 64 bytes of input: a Seed that’s 32 of EK’s bytes, and the hash of EK, which is an additional 32. If we combine these values and send them first, then most of EK and most of the CT can be sent in parallel from Alice to Bob and Bob to Alice respectively. Our more complicated but more efficient secret sharing now looks like this:Alice generates EK and DK. Alice extracts the 32-byte Seed from EKAlice sends 64 bytes EK (Seed + Hash(EK)) to Bob. Bob sends nothing during this time.Bob receives the Seed and Hash, and generates the first, largest part of the CT from them (CT)After this point, Alice sends EK (the rest of the EK minus the Seed), while Bob sends CTBob eventually receives EK, and uses it to generate the final portion of the CT (CT)Once Alice tells Bob that she has received all of CT, Bob sends Alice CT. Alice sends nothing during this time.With both sides having all of the pieces of EK and the CT that they need, they extract their shared secret and increment their epochThere are still places in this algorithm (specifically steps 2 and 6) where one side has nothing to send. But during those times, the other side has only a very small amount of information to send, so the duration of those steps is minimal compared to the rest of the process. Specifically, while the full EK is 37 chunks and the full CT is 34, the two pieces of the new protocol which must be sent without data being received (EK and CT) are 2 and 4 chunks respectively, while the pieces that can be sent while also receiving (EK and CT) are the bulk of the data, at 36 and 30 chunks respectively. Far more of our sending capacity is actually used with this approach.Remember that all of this is just to perform a quantum-safe key exchange that gives us a secret we can mix into the bigger protocol. To help us organize our code, our security proofs, and our understanding better we treat this process as a standalone protocol that we call the ML-KEM Braid.This work was greatly aided by the authors of the libcrux-ml-kem Rust library, who graciously exposed the APIs necessary to work with this incremental version of ML-KEM 768. With this approach completed, we’ve been able to really efficiently use the sending capacity of messages sent between two parties to share secrets as quickly as possible without exposing secrets from multiple epochs to potential attackers.Mixing Things Up - The Triple RatchetThere are plenty of details to add to make sure that we reached every corner - check those out in our online protocol documentation - but this basic idea lets us build secure messaging that has post-quantum FS and PCS without using up anyone’s data. We’re not done, though! Remember, at the beginning of this post we said we wanted post-quantum security without taking away our existing guarantees.While today’s Double Ratchet may not be quantum safe, it provides a high level of security today and we believe it will continue to be strong well into the future. We aren’t going to take that away from our users. So what can we do?Our answer ends up being really simple: we run both the Double Ratchet and the Sparse Post Quantum Ratchet alongside each other and mix their keys together, into what we’re calling the Triple Ratchet protocol. When you want to send a message you ask both the Double Ratchet and SPQR “What encryption key should I use for the next message?” and they will both give you a key (along with some other data you need to put in a message header). Instead of either key being used directly, both are passed into a Key Derivation Function - a special function that takes random-enough inputs and produces a secure cryptographic key that’s as long as you need. This gives you a new “mixed” key that has hybrid security. An attacker has to break both our elliptic curve and ML-KEM to even be able to distinguish this key from random bits. We use that mixed key to encrypt our message.Receiving messages is just as easy. We take the message header - remember it has some extra data in it - and send it to the Double Ratchet and SPQR and ask them “What key should I use to decrypt a message with this header?” They both return their keys and you feed them both into that Key Derivation Function to get your decryption key. After that, everything proceeds just like it always has.So we’ve got this new, snazzy protocol, and we want to roll it out to all of our users across all of their devices… but none of the devices currently support that protocol. We roll it out to Alice, and Alice tries to talk to Bob, but Alice speaks SPQR and Bob doesn’t. Or we roll it out to Bob, but Alice wants to talk to Bob and Alice doesn’t know the new protocol Bob wants to use. How do we make this work?Let’s talk about the simplest option: allowing downgrades. Alice tries to establish a session with Bob using SPQR and sends a message over it. Bob fails to read the message and establish the session, because Bob hasn’t been upgraded yet. Bob sends Alice an error, so Alice has to try again. This sounds fine, but in practice it’s not tenable. Consider what happens if Alice and Bob aren’t online at the same time… Alice sends a message at 1am, then shuts down. Bob starts up at 3am, sends an error, then shuts down. Alice gets that error when she restarts at 5am, then resends. Bob starts up at 7am and finally gets the message he should have received at 3am, 4 hours behind schedule.To handle this, we designed the SPQR protocol to allow itself to downgrade to not being used. When Alice sends her first message, she attaches the SPQR data she would need to start up negotiation of the protocol. Noticing that downgrades are allowed for this session, Alice doesn’t mix any SPQR key material into the message yet. Bob ignores that data, because it’s in a location he glosses over, but since there’s no mixed in keys yet, he can still decrypt the message. He sends a response that lacks SPQR data (since he doesn’t yet know how to fill it in), which Alice receives. Alice sees a message without SPQR data, and understands that Bob doesn’t speak SPQR yet. So, she downgrades to not using it for that session, and they happily talk without SPQR protection.There’s some scary potential problems here… let’s work through them. First off, can a malicious middleman force a downgrade and disallow Alice and Bob from using SPQR, even if both of them are able to? We protect against that by having the SPQR data attached to the message be MAC’d by the message-wide authentication code - a middleman can’t remove it without altering the whole message in such a way that the other party sees it, even if that other party doesn’t speak SPQR. Second, could some error cause messages to accidentally downgrade sometime later in their lifecycle, due either to bugs in the code or malicious activity? Crucially, SPQR only allows a downgrade when it first receives a message from a remote party. So, Bob can only downgrade if he receives his first message from Alice and notices that she doesn’t support SPQR, and Alice will only downgrade if she receives her first reply from Bob and notices that he doesn’t. After that first back-and-forth, SPQR is locked in and used for the remainder of the session.Finally, those familiar with Signal’s internal workings might note that Signal sessions last a really long time, potentially years. Can we ever say “every session is protected by SPQR”, given that SPQR is only added to new sessions as they’re being initiated? To accomplish this, Signal will eventually (once all clients support the new protocol) roll out a code change that enforces SPQR for all sessions, and that archives all sessions which don’t yet have that protection. After the full rollout of that future update, we’ll be able to confidently assert complete coverage of SPQR.One nice benefit to setting up this “maybe downgrade if the other side doesn’t support things” approach is that it also sets us up for the future: the same mechanisms that allow us to choose between SPQR or no-SPQR are designed to also allow us to upgrade from SPQR to some far-future (as yet unimagined) SPQRv2.Making Sure We Get It RightComplex protocols require extraordinary care. We have to ensure that the new protocol doesn’t lose any of the security guarantees the Double Ratchet gives us. We have to ensure that we actually get the post-quantum protection we’re aiming for. And even then, after we have full confidence in the protocol, we have to make sure that our implementation is correct and robust and stays that way as we maintain it. This is a tall order.To make sure we got this right, we started by building the protocol on a firm foundation of fundamental research. We built on the years of research the academic community has put into secure messaging and we collaborated with researchers from PQShield, AIST, and NYU to explore what was possible with post-quantum secure messaging. In a paper at Eurocrypt 25 we introduced erasure code based chunking and proposed the high-level Triple Ratchet protocol, proving that it gave us the post-quantum security we wanted without taking away any of the security of the classic Double Ratchet. In a follow up paper at USENIX 25, we observed that there are many different ways to design a post-quantum ratchet protocol and we need to pick the one that protects user messages the best. We introduced and analyzed six different protocols and two stood out: one is essentially SPQR, the other is a protocol using a new KEM, called Katana, that we designed just for ratcheting. That second one is exciting, but we want to stick to standards to start!As we kept finding better protocol candidates - and we implemented around a dozen of them - we modeled them in ProVerif to prove that they had the security properties we needed. Rather than wrapping up a protocol design and performing formal verification as a last step we made it a core part of the design process. Now that the design is settled, this gives us machine verified proof that our protocol has the security properties we demand from it. We wrote our Rust code to closely match the ProVerif models, so it is easy to check that we’re modeling what we implement. In particular, ProVerif is very good at reasoning about state machines, which we’re already using, making the mapping from code to model much simpler.We are taking formal verification further than that, though. We are using hax to translate our Rust implementation into F* on every CI run. Once the F* models are extracted, we prove that core parts of our highly optimized implementation are correct, that function pre-conditions and post-conditions cannot be violated, and that the entire crate is panic free. That last one is a big deal. It is great for usability, of course, because nobody wants their app to crash. But it also matters for correctness. We aggressively add assertions about things we believe must be true when the protocol is running correctly - and we crash the app if they are false. With hax and F*, we prove that those assertions will never fail.Often when people think about formally verified protocol implementations, they imagine a one-time huge investment in verification that leaves you with a codebase frozen in time. This is not the case here. We re-run formal verification in our CI pipeline every time a developer pushes a change to GitHub. If the proofs fail then the build fails, and the developer needs to fix it. In our experience so far, this is usually as simple as adding a pre- or postcondition or returning an error when a value is out of bounds. For us, formal verification is a dynamic part of the development process and ensures that the quality is high on every merge.Signal is rolling out a new version of the Signal Protocol with the Triple Ratchet. It adds the Sparse Post-Quantum Ratchet, or SPQR, to the existing Double Ratchet to create a new Triple Ratchet which gives our users quantum-safe messaging without taking away any of our existing security promises. It’s being added in such a way that it can be rolled out without disruption. It’s relatively lightweight, not using much additional bandwidth for each message, to keep network costs low for our users. It’s resistant to meddling by malicious middlemen - to disrupt it, all messages after a certain time must be dropped, causing a noticeable denial of service for users. We’re rolling it out slowly and carefully now, but in such a way that we’ll eventually be able to say with confidence “every message sent by Signal is protected by this.” Its code has been formally verified, and will continue to be so even as future updates affect the protocol. It’s the combined effort of Signal employees and external researchers and contributors, and it’s only possible due to the continued work and diligence of the larger crypto community. And as a user of Signal, our biggest hope is that you never even notice or care. Except one day, when headlines scream “OMG, quantum computers are here”, you can look back on this blog post and say “oh, I guess I don’t have to care about that, because it’s already been handled”, as you sip your Nutri-Algae while your self-driving flying car wends its way through the floating tenements of Megapolis Prime.]]></content:encoded></item><item><title>Who&apos;s Hiring - October 2025</title><link>https://www.reddit.com/r/golang/comments/1nwab02/whos_hiring_october_2025/</link><author>/u/jerf</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 2 Oct 2025 17:33:45 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[This post will be stickied at the top of until the last week of October (more or less).: It seems like Reddit is getting more and more cranky about marking external links as spam. A good job post obviously has external links in it. If your job post does not seem to show up please send modmail.  because Reddit sees that as a huge spam signal. Or wait a bit and we'll probably catch it out of the removed message list.Please adhere to the following rules when posting:Don't create top-level comments; those are for employers.Feel free to reply to top-level comments with on-topic questions.Meta-discussion should be reserved for the distinguished mod comment.To make a top-level comment you must be hiring directly, or a focused third party recruiter with specific jobs with named companies in hand. No recruiter fishing for contacts please.The job must be currently open. It is permitted to post in multiple months if the position is still open, especially if you posted towards the end of the previous month.The job must involve working with Go on a regular basis, even if not 100% of the time.One top-level comment per employer. If you have multiple job openings, please consolidate their descriptions or mention them in replies to your own top-level comment.Please base your comment on the following template:[Company name; ideally link to your company's website or careers page.][Full time, part time, internship, contract, etc.][What does your team/company do, and what are you using Go for? How much experience are you seeking and what seniority levels are you hiring for? The more details the better.][Where are your office or offices located? If your workplace language isn't English-speaking, please specify it.][Please attempt to provide at least a rough expectation of wages/salary.If you can't state a number for compensation, omit this field. Do not just say "competitive". Everyone says their compensation is "competitive".If you are listing several positions in the "Description" field above, then feel free to include this information inline above, and put "See above" in this field.If compensation is expected to be offset by other benefits, then please include that information here as well.][Do you offer the option of working remotely? If so, do you require employees to live in certain areas or time zones?][Does your company sponsor visas?][How can someone get in touch with you?]]]></content:encoded></item><item><title>Update and Experience Report: Building a tiling window manager for macOS in Rust</title><link>https://www.reddit.com/r/rust/comments/1nwa2jg/update_and_experience_report_building_a_tiling/</link><author>/u/toxait</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Thu, 2 Oct 2025 17:25:00 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[There are a bunch of different crates offering bindings to Apple frameworks in Rust, but ultimately I chose to go with the objc2 crates, a decision that I am very happy with.When looking at usage examples of these crates on GitHub, I found a lot of code which was written using earlier versions of the various objc2 crates. In fact, I would say that the majority of the code on GitHub I found was using earlier versions.There are enough breaking changes between those earlier versions and the current versions that I would strongly suggest to anyone looking to use these crates to just bite the bullet and use the latest versions, even at the expense of having less readily-available reference code.There are a whole bunch of API calls in Apple frameworks which can only be made on the main thread (seems like a lot of NSWhatever structs are like this) - it took me an embarrassingly long time to figure out that the objc2 crate workspace also contains the  crate to help with dispatching tasks to Grand Central Dispatch to run on the main thread either synchronously or asynchronously.While on the whole the experience felt quite "Rustic", there are some notable exceptions where I had to use macros like  to deal with Apple frameworks which rely on "delegates" and , the latter of which fills me with absolute dread.Once I was able to implement the equivalents of platform-specific functionality in komorebi for Windows, I was able to re-use the vast majority of the code in the Windows codebase.I'm still quite amazed at  work this required and the insanely high level of confidence I had in lifting and shifting huge features from the Windows implementation. I have been working in Rust for about 5 years now, and I didn't expect to be this surprised/amazed after so long in the ecosystem. I still can't quite believe what I have been able to accomplish in such a short period of time thanks to the fearlessness that Rust allows me to work with.As a bonus, I was also able to get the status bar, written in egui, working on macOS in less than 2 hours.In my experience, thanks to the maturity of both the  and  crates, Rust in 2025 is a solid choice for anyone interested in building cross-platform software which interacts heavily with system frameworks targeting both Windows and macOS.]]></content:encoded></item><item><title>Trivy Operator Dashboard – Visualize Trivy Reports in Kubernetes (v1.7 released)</title><link>https://www.reddit.com/r/kubernetes/comments/1nw9zo0/trivy_operator_dashboard_visualize_trivy_reports/</link><author>/u/raoulx24</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Thu, 2 Oct 2025 17:22:07 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Hi everyone! I’d like to share a tool I’ve been building:  - a web app that helps Kubernetes users visualize and manage Trivy scan results more effectively.Trivy is a fantastic scanner, but its raw output can be overwhelming. This dashboard fills that gap by turning scan data into interactive, searchable views. It’s built on top of the powerful AquaSec Trivy Operator and designed to make security insights actually usable.Displays Vulnerability, SBOM, Config Audit, RBAC, and Exposed Secrets reports (and their Clustered counterparts)Exportable tables, server-side filtering, and detailed inspection modesCompare reports side-by-side across versions and namespacesOpenTelemetry integrationFrontend: Angular 20 + PrimeNG 20 One year ago, a friend and I were discussing the pain of manually parsing vulnerabilities. None of the open-source dashboards met our needs, so we built one. It’s been a great learning experience and we’re excited to share it with the community.Would love your feedback—feature ideas, bug reports, or just thoughts on whether this helps your workflow.Thanks for reading this and checking it out!]]></content:encoded></item><item><title>Why I chose Lua for this blog</title><link>https://andregarzia.com/2025/03/why-i-choose-lua-for-this-blog.html</link><author>nairadithya</author><category>dev</category><category>hn</category><pubDate>Thu, 2 Oct 2025 16:58:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[This blog used to run using with a stack based on Racket using Pollen and lots of hacks on top of it. At some point I realised that my setup was working against me. The moving parts and workflow I created added too much friction to keep my blog active. That happened mostly because it was a static generator trying to behave as if it was dynamic website with an editing interface. That can be done really well — cue Grav CMS — but that was not the case for me.Once I decided to rewrite this blog as a simpler system, I faced the dilema of what stack to choose. The obvious choice for me would be Javascript, it is the language I use more often and one that I am quite confortable with. Still, I don't think it is a wise choice for the kind of blog I want to maintain.Talking to some friends recently, I noticed that many people I know that have implemented their own blogging systems face many challenges keeping them running over many years. Not because it is hard to keep software running, but because their stack of choice is moving faster than their codebase.This problem is specially prevalent in the Javascript world. It is almost a crime that JS as understood by the browser is this beautiful language with extreme retrocompatibility, while JS as understood and used by the current tooling and workflows is this mess moving at lightspeed. Let me unpack that for a bit.You can open a web page from 1995 on your browser of choice and it will just work because browser vendors try really hard to make sure they don't break the web.Developers who built the whole ecosystem of NodeJS, NPM, and all those libraries and frameworks don't share the same ethos. They all make a big case of semantic versioning and thus being able to handle breaking changes, but they have breaking changes all the time. You'd be hardpressed to actually run some JS code from ten years ago based on NodeJS and NPM. There is a big chance that dependencies might be gone, broken, or it might be incompatible with the current NodeJS.I know this sounds like FUD, and that for many many projects, maybe even most projects, that will not be the case. But I heard from many people that keeping their blogging systems up to date requires a lot more work than they would like to do and if they don't, then they're screwed.That is also true about other languages even though many of them move at a slower speed. A friend recently complained about a blogging system he implemented that requires Ruby 2.0 and that keeping that running sucks.I want a simpler blogging system; one that requires minimal changes over time.One characteristic that I love about it, is that is evolves very slowly. Lua 5.1 was introduced in 2006, Lua 5.4 which is the current version initial release was in 2020. Yes, there are point released in between, but you can see how much slower it moves when compared to JS.The differences between Lua 5.1 and Lua 5.4 are minimal when compared with how much other languages changed in the same time period.Lua only requires a C89 compiler to bootstrap itself. It is very easy to make Lua work and even easier to make it interface with something.JS is a lot larger than Lua, there is more to understand and more to remember. My blog needs are very simple and Lua can handle them with ease.This is an old-school blog. I uses cgi-bin — aka Comon Gateway Interface — scripts to run it. It is a dynamic website with a SQLite database holding its data. When you open a page, it fetches the data from a database and assembles a HTML to send to the browser using Mustache templates.One process per request. Like the old days.You might argue that if I went with NodeJS, I'd be able to serve more requests using fewer resources. That is true. I don't need to serve that many requests though. My peak access was a couple years ago with 50k visitors on a week, even my old Racket blog could handle that fine. The Lua one should handle it too; and if it breaks it breaks. I'm a flawed human being, my code can be flawed too, we're in this together, holding hands.Your blog is your place to experiment and program how you want it. You can drop the JS fatigue, you can drop your fancy Haskell types, you can just do whatever you find fun and keep going (and that includes JS and Haskell if that's your thing. You do you).Cause I'm using Lua, I don't have as many libraries and frameworks available to me as JS people have, but I still have quite a large collection via Luarocks. I try not to add many dependencies to my blog. At the moment there are about ten and that is mostly because Lua is a batteries-not-included language so you start from a minimal core and build things up to suit your needs.For a lot of things I went with the questionable choice of implementing things myself. I got my own little CGI library. It is 200 lines long and does the bare minimum to make this blog work. I got my own little libraries for many things. Micropub and IndieAuth were all implemented by hand.At the moment I'm  having a lot of fun implementing WebMentions. Doing the Microformats2 extraction on my own is teaching me a lot of things.What I want to say is that by choosing a small language that moves very slowly and very few dependencies, I can keep all of my blogging system in my head. I can make sure it will run without too much change for the next ten or twenty years.Lua is a lego set, a toolkit, it adapts to you and your needs. I don't need to keep chasing the new shiny or the latest framework du jour. I can focus on making the features I want and actually understanding how they work.Instead of installing a single dependency in another language and it pulling a hundred of other small dependencies all of which were transpiled into something the engine understands to the point that understanding how all the pieces work and fit together takes more time than to learn a new language, I decided to keep things simple.I got 29 Luarocks installed here and that is for all my Lua projects in this machine. That is my blog, my game development, my own work scripts for my day job. Not even half of those are for my blog.I often see wisdom in websites such as Hacker News and Lobsters around the idea of "choosing boring" because it is proven, safe, easier to maintain. I think that boring is not necessarily applicable to my case. I don't find Lua boring at all, but all that those blog posts talk about that kind of mindset are all applicable to my own choices here.Next time you're building your own blogging software, consider for a bit for how long do you want to maintain it. I first started blogging on macOS 8 in 2001. I choose badly many times and in the end couldn't keep my content moving forward in time with me as softwares I used or created became impossible to run. The last two changes: from JS to Racket and from Racket to Lua have been a lot safer and I managed to carry all my content forward into increasingly simpler setups and workflows.My blogging system is not becoming more complex over the years, it is becoming smaller, because with each change I select a stack that is more nimble and smaller than the one I had before. I don't think I can go smaller than Lua though.A language I can fully understand and keep on my head.A language that I know how to build the engine and can do it if needed.An engine that requires very few resources and is easy to interface with third-party libraries in native code.I chose Lua because of all that, and I'm happy with it and hope this engine will see me through the next ten or so years.]]></content:encoded></item><item><title>Distracting software engineers is way more harmful than most managers think</title><link>https://workweave.dev/blog/distracting-software-engineers-is-more-harmful-than-managers-think-even-in-the-ai-times</link><author>/u/zaidesanton</author><category>dev</category><category>reddit</category><pubDate>Thu, 2 Oct 2025 16:24:10 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Our work culture changed mainly for the better after COVID-19, but there were also some negative changes - like an increase of 13.5% in the amount of meetings per employee.[1]The problem is that there's a huge gap between how managers think about meetings versus how engineers think about them.In the famous “Maker’s Schedule, Manager’s Schedule” [2], Paul Graham wrote:“When you're operating on the maker's schedule, meetings are a disaster. A single meeting can blow a whole afternoon, by breaking it into two pieces each too small to do anything hard in.”This problem hasn't gone away with AI coding tools - it's getting worse, as managers assume engineers can now be productive in smaller time chunks.Over the past 2 years, we've studied hundreds of engineering teams to understand what the best ones do differently. In this article, we'll share their strategies and actionable steps you can use today. But first, let’s talk about deep work: The term was coined by Cal Newport, in Deep Work: Rules for Focused Success in a Distracted Worldbook [3].Deep Work is the kind of work that requires a big part of your brain power and usually gives some unique value. It can’t be done while distracted! If you can do a task during a Zoom call, it means it’s NOT deep work. is the opposite - things you can do without engaging 100% of your brain. Things like answering Slack messages/emails, reviewing a document, and so on.Software engineering used to be a haven for people who enjoy deep work. There is a reason why some people still think that software engineers work alone with their headphones in the basement.Today it’s becoming harder and harder to get those ‘Deep Work’ times - and I believe they are critical while using AI coding tools. Why Deep Work is so criticalWorking deeply is the only way to achieve the famous ‘flow’ state - AKA being in the zone.More deep work helps engineers to:Support their  - they’ll get more work done in less time, so more free time will be left. - in those focused times the toughest challenges are solved, and the biggest improvements to skills happen. A big mistake managers make is assuming that with AI coding tools, it’s no longer critical. That engineers need to get used to constantly task switching - as you anyway have to wait for a couple of minutes between prompts, so who cares about distractions. What happens then is that the quality of our thoughts and prompts go down. We fall into endless loops of asking the AI to fix the problems, while giving mediocre context.If you stay in the flow state, deeper in the problem, you’ll need many less iterations to achieve the same result. No data for this one, only my experience and gut feeling. So what’s the problem with deep work?Remote work should have been an answer to this - no commute, fewer distractions. In reality, only the first part is true.Since 2020, in addition to the 13.5% increase in total meetings, there has also been a rise of 60% in remote meetings[5]. The unsurprising part is that 92% of people say they multitask during those meetings[6] - and I would guess the number is just about 100% for software engineers.There are 2 main problems with that:1. Work that should be Deep, becomes ShallowRemember the simple test we discussed above? If you can do it in a Zoom meeting => it’s shallow work.A great example is reviewing PRs. If you have a busy day filled with meetings, when would be the best time to review them?During the meetings of course… reviewing a pull request should be deep work! Same for bug fixes, or writing design documents.Doing those tasks during a meeting starts a horrible cycle:The quality of the work gets poorer so more problems arise → more meetings are scheduled to address them.People are being distracted during meetings so no good decisions are taken → more meetings are scheduled…And it’s not the fault of your multitasking engineers - it’s the fault of your meeting culture.2. Engineers are not reaching the flow stateIt takes 15 minutes just to get going and only by the 45-minute mark (!) you will hit your peak, when you are fully immersed in the problem.[7]Every time you’re distracted, that clock resets thanks to context switching. In a study done by Meta [8], they’ve shown how severe the problem is - engineers get just 2 1-hour sessions a week! (And I find it crazy that a 3 minute session is considered ‘focus time’ at all). It was not just the time in meetings that is lost. Every distraction sets you back 15-45 minutes, leaving you by the end of the day with just 1-2 hours of productive time in the BEST case scenario.I love this analogy, borrowed from this Reddit comment[9]:Imagine developers are like miners. digging is our job. Every time you have a meeting, we need to pack our things, and get to the entrance of the mine on time. After the meeting is finished, we need to walk alllll the way back to where we were working, assuming we even remember the right path...So if you want us to get you some diamonds, let us work in peace.Software engineers need at least 4-5 hours of uninterrupted time a day, and it’s our job as managers to provide it.How can we create deep work timeImprove the meeting culture at your companyIt’s not that hard to fix - you ‘just’ need to get all the managers to commit to basic rules:Meetings should be  - obvious right? But still rare… Have a clear agenda, and clear outcomes.Have a fixed daily time for meetings - and leave big chunks of ‘no-meetings’ time. You can either do no-meeting days, or no-meeting hours, it depends on your organization. Invite only people who are really needed - this is something that remote work is bad at - it has become too easy to just invite everyone. People think to themselves: “Worst case, they’ll multitask, and be available in case they are needed”.This is an awful approach! Remember - you cannot achieve a flow state while multitasking.But the best way? Get rid of as many meetings as you can! In the past 2 years, we’ve studied how hundreds of engineering teams operate. The engineering team at Pylon really impressed us. When we asked to see a senior engineer's calendar for the week, it was completely empty. No standups, no sprint planning, no recurring meetings of any kind.There's still plenty of collaboration, but it happens instead of recurring meetings. This is probably possible because they're in person 5 days a week, but even in remote settings you for sure can get rid of some meetings. And if you can’t get rid of all meetings, we recommend at least having some company-wide blocks of focus time. After analyzing data 600,000+ PRs at Weave, the data shows that most engineers are the most productive at 09:00-11:00 and 14:00-16:00: *Note: Those are just commit times, and we know that probably people work in the hours before that (and that there is a lunch break). Still, our point is that there are 'clusters' of productive times, and you should not fragment your people's workday!Rethink your PRs workflowBetter yet, do you actually need all those PRs?The other thing that surprised us about that team at Pylon is that they have almost no code reviews. Engineers merge their own code and only request reviews if they need input, think they have a risky change, or are still onboarding.Code reviews are a big distraction on both sides - when it comes your way you want to do it as fast as possible to not block others, and when the answer comes you are tempted to look at it and address it immediately. In both cases, it can ruin the flow state.This goes against conventional wisdom about code quality, but their thought process is simple: if you hire skilled engineers and trust them, there's no reason to bottleneck every change with mandatory reviews.Once you get the meetings under control, you can deal with the other interruptions.The best way to create a good culture is to cherish your own deep work time, and make sure your team respects it. I have to admit that I’m still struggling with this one - I schedule focus time in my calendar, and put on my headphones, but I still sometimes check Slack and answer when interrupted.I know this sets a bad example for my team - deep work time should be sacred, and people should know it’s completely acceptable to not be available in Slack for a couple of hours.Focus time in 2025 becomes critical I believe that the more we start to depend on AI, the more important focus time will become. The models will become much faster with time, and I believe that companies and engineers who learn to enter the flow state  AI will hugely outpace everyone else.[1] https://www.notta.ai/en/blog/meeting-statistics[2] https://paulgraham.com/makersschedule.html[3] https://www.goodreads.com/book/show/25744928-deep-work[4] https://hbr.org/2014/05/create-a-work-environment-that-fosters-flow[5] https://www.cfo.com/news/remote-meetings-up-60-since-2020-weekly-stat/654749/[6] https://www.flowtrace.co/collaboration-blog/50-meeting-statistics[7] https://www.locationrebel.com/flow-state/#:~:text=The%20science%20shows%20us%20that,disturb%20you%20until%20after%20lunch.[8] https://users.encs.concordia.ca/~pcr/paper/YChen2022ICSE-industry-preprint.pdf?utm_source=chatgpt.com[9] https://www.reddit.com/r/programming/comments/1bbcoec/comment/ku9i16h/]]></content:encoded></item><item><title>Rust in Production Podcast: Amazon Prime Video rewrote their streaming app in Rust (30ms input latency)</title><link>https://corrode.dev/podcast/s05e01-prime-video/</link><author>/u/mre__</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Thu, 2 Oct 2025 16:23:18 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Are you one of over 240 million subscribers of Amazon’s Prime Video service? If so, you might be surprised to learn that much of the infrastructure behind Prime Video is built using Rust. They use a single codebase for media players, game consoles, and tablets. In this episode, we sit down with Alexandru Ene, a Principal Engineer at Amazon, to discuss how Rust is used at Prime Video, the challenges they face in building a global streaming service, and the benefits of using Rust for their systems.
    CodeCrafters helps you become proficient in Rust by building real-world,
    production-grade projects. Learn hands-on by creating your own shell, HTTP
    server, Redis, Kafka, Git, SQLite, or DNS service from scratch.
  
    Start for free today and enjoy 40% off any paid plan by using
    this link.
  Prime Video is a streaming service offered by Amazon that provides a wide range of movies, TV shows, and original content to its subscribers. With over 240 million subscribers worldwide, Prime Video is one of the largest streaming platforms in the world. In addition to its vast content library, Prime Video also offers features such as offline viewing, 4K streaming, and support for multiple devices. On the backend, Prime Video relies on a variety of technologies to deliver its content, including Rust, which is used for building high-performance and reliable systems that can handle the demands of a global audience.Alexandru worked on the transition of Prime Video’s user interface from JavaScript to Rust. He has been with Amazon for over 8 years and previously worked at companies like Ubisoft and EA. He has a background in computer science and is an active open source maintainer. Alexandru lives in London.]]></content:encoded></item><item><title>Comprehensive Kubernetes Autoscaling Monitoring with Prometheus and Grafana</title><link>https://www.reddit.com/r/kubernetes/comments/1nw8bf9/comprehensive_kubernetes_autoscaling_monitoring/</link><author>/u/SevereSpace</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Thu, 2 Oct 2025 16:20:08 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I built a project monitoring-mixin for Kubernetes autoscaling a while back and recently added KEDA dashboards and alerts too it. Thought of sharing it here and getting some feedback.It covers KEDA, Karpenter, Cluster Autoscaler, VPAs, HPAs and PDBs.Here is a Karpenter dashboard screenshot (could only add a single image, there's more images on my blog).Thanks for taking a look! ]]></content:encoded></item><item><title>Playball – Watch MLB games from a terminal</title><link>https://github.com/paaatrick/playball</link><author>ohjeez</author><category>dev</category><category>hn</category><pubDate>Thu, 2 Oct 2025 16:09:15 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Signal Protocol and Post-Quantum Ratchets</title><link>https://signal.org/blog/spqr/</link><author>pluto_modadic</author><category>dev</category><category>hn</category><pubDate>Thu, 2 Oct 2025 16:06:10 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[We are excited to announce a significant advancement in the security of the Signal Protocol: the introduction of the Sparse Post Quantum Ratchet (SPQR). This new ratchet enhances the Signal Protocol’s resilience against future quantum computing threats while maintaining our existing security guarantees of forward secrecy and post-compromise security.The Signal Protocol is a set of cryptographic specifications that provides end-to-end encryption for private communications exchanged daily by billions of people around the world. After its publication in 2013, the open source Signal Protocol was adopted not only by the Signal application but also by other major messaging products. Technical information on the Signal Protocol can be found in the specifications section of our docs site.In a previous blog post, we announced the first step towards advancing quantum resistance for the Signal Protocol: an upgrade called PQXDH that incorporates quantum-resistent cryptographic secrets when chat sessions are established in order to protect against harvest-now-decrypt-later attacks that could allow current chat sessions to become compromised if a sufficiently powerful quantum computer is developed in the future. However, the Signal Protocol isn’t just about protecting cryptographic material and keys at the beginning of a new chat or phone call; it’s also designed to minimize damage and heal from compromise as that conversation continues.We refer to these security goals as Forward Secrecy (FS) and Post-Compromise Security (PCS). FS and PCS can be considered mirrors of each other: FS protects past messages against future compromise, while PCS protects future messages from past compromise. Today, we are happy to announce the next step in advancing quantum resistance for the Signal Protocol: an additional regularly advancing post-quantum ratchet called the Sparse Post Quantum Ratchet, or SPQR. On its own, SPQR provides secure messaging that provably achieves these FS and PCS guarantees in a quantum safe manner. We mix the output of this new ratcheting protocol with Signal’s existing Double Ratchet, in a combination we refer to as the Triple Ratchet.What does this mean for you as a Signal user? First, when it comes to your experience using the app, nothing changes. Second, because of how we’re rolling this out and mixing it in with our existing encryption, eventually all of your conversations will move to this new protocol without you needing to take any action. Third, and most importantly, this protects your communications both now and in the event that cryptographically relevant quantum computers eventually become a reality, and it allows us to maintain our existing security guarantees of forward secrecy and post-compromise security as we proactively prepare for that new world.The Current State of the Signal ProtocolThe original Signal ratchet uses hash functions for FS and a set of elliptic-curve Diffie Hellman (ECDH) secret exchanges for PCS. The hash functions are quantum safe, but elliptic-curve cryptography is not. An example is in order: our favorite users, Alice and Bob, establish a long-term connection and chat over it regularly. During that session’s lifetime, Alice and Bob regularly agree on new ECDH secrets and use them to “ratchet” their session. Mean ol’ Mallory records the entire (encrypted) communication, and really wants to know what Alice and Bob are talking about.The concept of a “ratchet” is crucial to our current non-quantum FS/PCS protection. In the physical world, a ratchet is a mechanism that allows a gear to rotate forward, but disallows rotation backwards. In the Signal Protocol, it takes on a similar role. When Alice and Bob “ratchet” their session, they replace the set of keys they were using prior with a new set based on both the older secrets and a new one they agree upon. Given access to those new secrets, though, there’s no (non-quantum) way to compute the older secrets. By being “one-way”, this ratcheting mechanism provides FS.The ECDH mechanism allows Alice and Bob to generate new, small (32 bytes) data blobs and attach them to every message. Whenever each party receives a message from the other, they can locally (and relatively cheaply) use this data blob to agree on a new shared secret, then use that secret to ratchet their side of the protocol. Crucially, ECDH also allows Alice and Bob to both agree on the new secret without sending that secret itself over their session, and in fact without sending anything over the session that Mallory could use to determine it. This description of Diffie-Hellman key exchange provides more details on the concepts of such a key exchange, and this description of ECDH provides specific details on the variant used by the current Signal protocol.Sometime midway through the lifetime of Alice and Bob’s session, Mallory successfully breaches the defences of both Alice and Bob, gaining access to all of the (current) secrets used for their session at the time of request. Alice and Bob should have the benefits of Forward Secrecy - they’ve ratcheted sometime recently before the compromise, so no messages earlier than their last ratchet are accessible to Mallory, since ratcheting isn’t reversible. They also retain the benefits of Post-Compromise Security. Their ratcheting after Mallory’s secret access agrees upon new keys that can’t be gleaned just from the captured data they pass between each other, re-securing the session.Should Mallory have access to a quantum computer, though, things aren’t so simple. Because elliptic curve cryptography is not quantum resistant, it’s possible that Mallory could glean access to the secret that Alice and Bob agreed upon, just by looking at the communication between them. Given this, Alice and Bob’s session will never “heal”; Mallory’s access to their network traffic from this point forward will allow her to decrypt all future communications.Mixing In Quantum SecurityIn order to make our security guarantees stand up to quantum attacks, we need to mix in secrets generated from quantum secure algorithms. In PQXDH, we did this by performing an additional round of key agreement during the session-initiating handshake, then mixing the resulting shared secret into the initial secret material used to create Signal sessions. To handle FS and PCS, we need to do continuous key agreement, where over the lifetime of a session we keep generating new shared secrets and mixing those keys into our encryption keys.Luckily there is a tool designed exactly for this purpose: the quantum-secure Key-Encapsulation Mechanism (KEM). KEMs share similar behavior to the Diffie-Hellman mechanisms we described earlier, where two clients provide each other with information, eventually deciding on a shared secret, without anyone who intercepts their communications being able to access that secret. However, there is one important distinction for KEMs - they require ordered, asymmetric messages to be passed between their clients. In ECDH, both clients send the other some public parameters, and both combine these parameters with their locally held secrets and come up with an identical shared secret. In the standardized ML-KEM key-encapsulation mechanism, though, the initiating client generates a pair of encapsulation key (EK) and decapsulation key (DK) (analogous to a public and private key respectively) and sends the EK. The receiving client receives it, generates a secret, and wraps it into a ciphertext (CT) with that key. The initiating client receives that CT and decapsulates with its previously generated DK. In the end, both clients have access to the new, shared secret, just through slightly different means.Wanting to integrate this quantum-secure key sharing into Signal, we could take a simple, naive approach for each session. When Alice initiates a session with Bob,Alice, with every message she sends, sends an EKBob, with every message he receives, generates a secret and a CT, and sends the CT backAlice, on receiving a CT, extracts the secret with her DK and mixes it inThis initially simple-looking approach, though, quickly runs into a number of issues we’ll need to address to make our protocol actually robust. First, encapsulation keys and CTs are large - over 1000 bytes each for ML-KEM 768, compared to the 32 bytes required for ECDH. Second, while this protocol works well when both clients are online, what happens when a client is offline? Or a message is dropped or reordered? Or Alice wants to send 10 messages before Bob wants to send one?Some of these problems have well-understood solutions, but others have trade-offs that may shine in certain circumstances but fall short in others. Let’s dive in and come to some conclusions.How does Alice decide what to send based on what Bob needs next, and vice versa? If Bob hasn’t received an EK yet, she shouldn’t send the next one. What does Bob send when he hasn’t yet received an EK from Alice, or when he has, but he’s already responded to it? This is a common problem when remote parties send messages to communicate, so there’s a good, well-understood solution: a state machine. Alice and Bob both keep track of “what state am I in”, and base their decisions on that. When sending or receiving a message, they might also change their state. For example:Alice wants to send a message, but she’s in a StartingA state, so she doesn’t have an EK. So, she generates an EK/DK pair, stores them locally, and transitions to the SendEK stateAlice wants to send a message and is in the SendEK state. She sends the EK along with the messageAlice wants to send another message, but she’s still in the SendEK state. So, she sends the EK with the new message as wellBob receives the message with the EK. He generates a secret and uses the EK to create a CT. He transitions to the SendingCT state.Bob wants to send a message and he’s in the SendingCT state. He sends the CT along with the messageBob wants to send a message and he’s in the SendingCT state. He sends the CT along with the messageBy crafting a set of states and transitions, both sides can coordinate what’s sent. Note, though, that even in this simple case, we see problems. For example, we’re sending our (large) EK and (large) CT multiple times.We’ve already mentioned that the size of the data we’re sending has increased pretty drastically, from 32 bytes to over 1000 per message. But bandwidth is expensive, especially on consumer devices like client phones, that may be anywhere in the world and have extremely varied costs for sending bytes over the wire. So let’s discuss strategies for conserving that bandwidth.First, the simplest approach - don’t send a new key with every message. Just, for example, send with every 50 messages, or once a week, or every 50 messages unless you haven’t sent a key in a week, or any other combination of options. All of these approaches tend to work pretty well in online cases, where both sides of a session are communicating in real-time with no message loss. But in cases where one side is offline or loss can occur, they can be problematic. Consider the case of “send a key if you haven’t sent one in a week”. If Bob has been offline for 2 weeks, what does Alice do when she wants to send a message? What happens if we can lose messages, and we lose the one in fifty that contains a new key? Or, what happens if there’s an attacker in the middle that wants to stop us from generating new secrets, and can look for messages that are 1000 bytes larger than the others and drop them, only allowing keyless messages through?Another method is to chunk up a message. Want to send 1000 bytes? Send 10 chunks of 100 bytes each. Already sent 10 chunks? Resend the first chunk, then the second, etc. This smooths out the total number of bytes sent, keeping individual message sizes small and uniform. And often, loss of messages is handled. If chunk 1 was lost, just wait for it to be resent. But it runs into an issue with message loss - if chunk 99 was lost, the receiver has to wait for all of chunks 1-98 to be resent before it receives the chunk it missed. More importantly, if a malicious middleman wants to stop keys from being decided upon, they could always drop chunk 3, never allowing the full key to pass between the two parties.We can get around all of these issues using a concept called erasure codes. Erasure codes work by breaking up a larger message into smaller chunks, then sending those along. Let’s consider our 1000 byte message being sent as 100 byte chunks again. After chunk #10 has been sent, the entirety of the original 1000 byte message has been sent along in cleartext. But rather than just send the first chunk over again, erasure codes build up a new chunk #11, and #12, etc. And they build them in such a way that, once the recipient receives any 10 chunks in any order, they’ll be able to reconstruct the original 1000 byte message.When we put this concept of erasure code chunks together with our previous state machine, it gives us a way to send large blocks of data in small chunks, while handling messages that are dropped. Crucially, this includes messages dropped by a malicious middleman: since any N chunks can be used to recreate the original message, a bad actor would need to drop all messages after #N-1 to disallow the data to go through, forcing them into a complete (and highly noticeable) denial of service. Now, if Alice wants to send an EK to Bob, Alice will:Transition from the StartingA state to the SendingEK state, by generating a new EK and chunking itWhile in the SendingEK state, send a new chunk of the EK along with any messages she sendsWhen she receives confirmation that the recipient has received the EK (when she receives a chunk of CT), transition to the ReceivingCT stateTransition from the StartingB state to the ReceivingEK state when he receives its first EK chunkKeep receiving EK chunks until he has enough to reconstruct the EKAt that point, reconstruct the EK, generate the CT, chunk the CT, and transition to the SendingCT stateFrom this point on, he will send a chunk of the CT with every messageOne interesting way of looking at this protocol so far is to consider the messages flowing from Alice to Bob as potential capacity for sending data associated with post-quantum ratcheting: each message that we send, we could also send additional data like a chunk of EK or of the CT. If we look at Bob’s side, above, we notice that sometimes he’s using that capacity (IE: in step 4 when he’s sending CT chunks) and sometimes he’s not (if he sends a message to Alice during step 2, he has no additional data to send). This capacity is pretty limited, so using more of it gives us the potential to speed up our protocol and agree on new secrets more frequently.A Meditation On How Faster Isn’t Always BetterWe want to generate shared secrets, then use them to secure messages. So, does that mean that we want to generate shared secrets as fast as possible? Let’s introduce a new term: an epoch. Alice and Bob start their sessions in epoch 0, sending the EKs for epoch 1 (EK#1) and associated ciphertext (CT#1) to each other. Once that process completes, they have a new shared secret they use to enter epoch 1, after which all newly sent messages are protected by the new secret. Each time they generate a new shared secret, they use it to enter a new epoch. Surely, every time we enter a new epoch with a new shared secret, we protect messages before that secret (FS) and after that secret (PCS), so faster generation is better? It seems simple, but there’s an interesting complexity here that deserves attention.First, let’s discuss how to do things faster. Right now, there’s a lot of capacity we’re not using: Bob sends nothing while Alice sends an EK, and Alice sends nothing while Bob sends a CT. Speeding this up isn’t actually that hard. Let’s change things so that Alice sends EK#1, and once Bob acknowledges its receipt, Alice immediately generates and sends EK#2. And once she notices Bob has received that, she generates and sends EK#3, etc. Whenever Alice sends a new message, she always has data to send along with it (new EK chunks), so she’s using its full capacity. Bob doesn’t always have a new CT to send, but he is receiving EKs as fast as Alice can send them, so he often has a new CT to send along.But now let’s consider what happens when an attacker gains access to Alice. Let’s say that Alice has sent EK#1 and EK#2 to Bob, and she’s in the process of sending EK#3. Bob has acknowledged receipt of EK#1 and EK#2, but he’s still in the process of sending CT#1, since in this case Bob sends fewer messages to Alice than vice versa. Because Alice has already generated 3 EKs she hasn’t used, Alice needs to keep the associated DK#1, DK#2, and DK#3 around. So, if at this point someone maliciously gains control of Alice’s device, they gain access to both the secrets associated with the current epoch (here, epoch 0) and to the DKs necessary to reconstruct the secrets to other epochs (here, epochs 1, 2, and 3) using only the over-the-wire CT that Bob has yet to send. This is a big problem: by generating secrets early, we’ve actually made the in-progress epochs and any messages that will be sent within them less secure against this single-point-in-time breach.To test this out, we at Signal built a number of different state machines, each sending different sets of data either in parallel or serially. We then ran these state machines in numerous simulations, varying things like the ratio of messages sent by Alice vs Bob, the amount of data loss or reordering, etc. And while running these simulations, we tracked what epochs’ secrets were exposed at any point in time, assuming an attacker were to breach either Alice’s or Bob’s secret store. The results showed that, in general, while simulations that handled multiple epochs’ secrets in parallel (IE: by sending EK#2 before receipt of CT#1) did generate new epochs more quickly, they actually made more messages vulnerable to a single breach.But Let’s Still Be EfficientThis still leaves us with a problem, though: the capacity present in messages we send in either direction is still a precious resource, and we want to use it as efficiently as possible. And our simple approach of Alice’s “send EK, receive CT, repeat” and Bob’s “receive EK, send CT, repeat” leaves lots of time where Alice and Bob have nothing to send, should that capacity be available.To improve our use of our sending capacity, we decided to take a harder look into the ML-KEM algorithm we’re using to share secrets, to see if there was room to improve. Let’s break things down more and share some actual specifics on the ML-KEM algorithm.Alice generates an EK of 1184 bytes to send to Bob, and an associated DKBob samples a new shared secret (32 bytes), which he encrypts with EK into a CT of 1088 bytes to send to AliceAlice receives the CT, uses the DK to decrypt it, and now also has access to the 32 byte shared secretDiving in further, we can break out step #3 into some sub-stepsAlice generates an EK of 1184 bytes to send to Bob, and an associated DKBob samples a new shared secret (32 bytes), which he encrypts with EK into a CT of 1088 bytes to send to Alice Bob creates a new shared secret S and sampled randomness R by sampling entropy and combining it with a hash of EKBob hashes the EK into a HashBob pulls 32 bytes of the EK, a SeedBob uses the Seed and R to generate the majority of the CTBob then uses S and EK to generate the last portion of the CTAlice receives the CT, uses the DK to decrypt it, and now also has access to the 32 byte shared secretStep 3.d, which generates 960 bytes of the 1088-byte CT, only needs 64 bytes of input: a Seed that’s 32 of EK’s bytes, and the hash of EK, which is an additional 32. If we combine these values and send them first, then most of EK and most of the CT can be sent in parallel from Alice to Bob and Bob to Alice respectively. Our more complicated but more efficient secret sharing now looks like this:Alice generates EK and DK. Alice extracts the 32-byte Seed from EKAlice sends 64 bytes EK (Seed + Hash(EK)) to Bob. Bob sends nothing during this time.Bob receives the Seed and Hash, and generates the first, largest part of the CT from them (CT)After this point, Alice sends EK (the rest of the EK minus the Seed), while Bob sends CTBob eventually receives EK, and uses it to generate the final portion of the CT (CT)Once Alice tells Bob that she has received all of CT, Bob sends Alice CT. Alice sends nothing during this time.With both sides having all of the pieces of EK and the CT that they need, they extract their shared secret and increment their epochThere are still places in this algorithm (specifically steps 2 and 6) where one side has nothing to send. But during those times, the other side has only a very small amount of information to send, so the duration of those steps is minimal compared to the rest of the process. Specifically, while the full EK is 37 chunks and the full CT is 34, the two pieces of the new protocol which must be sent without data being received (EK and CT) are 2 and 4 chunks respectively, while the pieces that can be sent while also receiving (EK and CT) are the bulk of the data, at 36 and 30 chunks respectively. Far more of our sending capacity is actually used with this approach.Remember that all of this is just to perform a quantum-safe key exchange that gives us a secret we can mix into the bigger protocol. To help us organize our code, our security proofs, and our understanding better we treat this process as a standalone protocol that we call the ML-KEM Braid.This work was greatly aided by the authors of the libcrux-ml-kem Rust library, who graciously exposed the APIs necessary to work with this incremental version of ML-KEM 768. With this approach completed, we’ve been able to really efficiently use the sending capacity of messages sent between two parties to share secrets as quickly as possible without exposing secrets from multiple epochs to potential attackers.Mixing Things Up - The Triple RatchetThere are plenty of details to add to make sure that we reached every corner - check those out in our online protocol documentation - but this basic idea lets us build secure messaging that has post-quantum FS and PCS without using up anyone’s data. We’re not done, though! Remember, at the beginning of this post we said we wanted post-quantum security without taking away our existing guarantees.While today’s Double Ratchet may not be quantum safe, it provides a high level of security today and we believe it will continue to be strong well into the future. We aren’t going to take that away from our users. So what can we do?Our answer ends up being really simple: we run both the Double Ratchet and the Sparse Post Quantum Ratchet alongside each other and mix their keys together, into what we’re calling the Triple Ratchet protocol. When you want to send a message you ask both the Double Ratchet and SPQR “What encryption key should I use for the next message?” and they will both give you a key (along with some other data you need to put in a message header). Instead of either key being used directly, both are passed into a Key Derivation Function - a special function that takes random-enough inputs and produces a secure cryptographic key that’s as long as you need. This gives you a new “mixed” key that has hybrid security. An attacker has to break both our elliptic curve and ML-KEM to even be able to distinguish this key from random bits. We use that mixed key to encrypt our message.Receiving messages is just as easy. We take the message header - remember it has some extra data in it - and send it to the Double Ratchet and SPQR and ask them “What key should I use to decrypt a message with this header?” They both return their keys and you feed them both into that Key Derivation Function to get your decryption key. After that, everything proceeds just like it always has.So we’ve got this new, snazzy protocol, and we want to roll it out to all of our users across all of their devices… but none of the devices currently support that protocol. We roll it out to Alice, and Alice tries to talk to Bob, but Alice speaks SPQR and Bob doesn’t. Or we roll it out to Bob, but Alice wants to talk to Bob and Alice doesn’t know the new protocol Bob wants to use. How do we make this work?Let’s talk about the simplest option: allowing downgrades. Alice tries to establish a session with Bob using SPQR and sends a message over it. Bob fails to read the message and establish the session, because Bob hasn’t been upgraded yet. Bob sends Alice an error, so Alice has to try again. This sounds fine, but in practice it’s not tenable. Consider what happens if Alice and Bob aren’t online at the same time… Alice sends a message at 1am, then shuts down. Bob starts up at 3am, sends an error, then shuts down. Alice gets that error when she restarts at 5am, then resends. Bob starts up at 7am and finally gets the message he should have received at 3am, 4 hours behind schedule.To handle this, we designed the SPQR protocol to allow itself to downgrade to not being used. When Alice sends her first message, she attaches the SPQR data she would need to start up negotiation of the protocol. Noticing that downgrades are allowed for this session, Alice doesn’t mix any SPQR key material into the message yet. Bob ignores that data, because it’s in a location he glosses over, but since there’s no mixed in keys yet, he can still decrypt the message. He sends a response that lacks SPQR data (since he doesn’t yet know how to fill it in), which Alice receives. Alice sees a message without SPQR data, and understands that Bob doesn’t speak SPQR yet. So, she downgrades to not using it for that session, and they happily talk without SPQR protection.There’s some scary potential problems here… let’s work through them. First off, can a malicious middleman force a downgrade and disallow Alice and Bob from using SPQR, even if both of them are able to? We protect against that by having the SPQR data attached to the message be MAC’d by the message-wide authentication code - a middleman can’t remove it without altering the whole message in such a way that the other party sees it, even if that other party doesn’t speak SPQR. Second, could some error cause messages to accidentally downgrade sometime later in their lifecycle, due either to bugs in the code or malicious activity? Crucially, SPQR only allows a downgrade when it first receives a message from a remote party. So, Bob can only downgrade if he receives his first message from Alice and notices that she doesn’t support SPQR, and Alice will only downgrade if she receives her first reply from Bob and notices that he doesn’t. After that first back-and-forth, SPQR is locked in and used for the remainder of the session.Finally, those familiar with Signal’s internal workings might note that Signal sessions last a really long time, potentially years. Can we ever say “every session is protected by SPQR”, given that SPQR is only added to new sessions as they’re being initiated? To accomplish this, Signal will eventually (once all clients support the new protocol) roll out a code change that enforces SPQR for all sessions, and that archives all sessions which don’t yet have that protection. After the full rollout of that future update, we’ll be able to confidently assert complete coverage of SPQR.One nice benefit to setting up this “maybe downgrade if the other side doesn’t support things” approach is that it also sets us up for the future: the same mechanisms that allow us to choose between SPQR or no-SPQR are designed to also allow us to upgrade from SPQR to some far-future (as yet unimagined) SPQRv2.Making Sure We Get It RightComplex protocols require extraordinary care. We have to ensure that the new protocol doesn’t lose any of the security guarantees the Double Ratchet gives us. We have to ensure that we actually get the post-quantum protection we’re aiming for. And even then, after we have full confidence in the protocol, we have to make sure that our implementation is correct and robust and stays that way as we maintain it. This is a tall order.To make sure we got this right, we started by building the protocol on a firm foundation of fundamental research. We built on the years of research the academic community has put into secure messaging and we collaborated with researchers from PQShield, AIST, and NYU to explore what was possible with post-quantum secure messaging. In a paper at Eurocrypt 25 we introduced erasure code based chunking and proposed the high-level Triple Ratchet protocol, proving that it gave us the post-quantum security we wanted without taking away any of the security of the classic Double Ratchet. In a follow up paper at USENIX 25, we observed that there are many different ways to design a post-quantum ratchet protocol and we need to pick the one that protects user messages the best. We introduced and analyzed six different protocols and two stood out: one is essentially SPQR, the other is a protocol using a new KEM, called Katana, that we designed just for ratcheting. That second one is exciting, but we want to stick to standards to start!As we kept finding better protocol candidates - and we implemented around a dozen of them - we modeled them in ProVerif to prove that they had the security properties we needed. Rather than wrapping up a protocol design and performing formal verification as a last step we made it a core part of the design process. Now that the design is settled, this gives us machine verified proof that our protocol has the security properties we demand from it. We wrote our Rust code to closely match the ProVerif models, so it is easy to check that we’re modeling what we implement. In particular, ProVerif is very good at reasoning about state machines, which we’re already using, making the mapping from code to model much simpler.We are taking formal verification further than that, though. We are using hax to translate our Rust implementation into F* on every CI run. Once the F* models are extracted, we prove that core parts of our highly optimized implementation are correct, that function pre-conditions and post-conditions cannot be violated, and that the entire crate is panic free. That last one is a big deal. It is great for usability, of course, because nobody wants their app to crash. But it also matters for correctness. We aggressively add assertions about things we believe must be true when the protocol is running correctly - and we crash the app if they are false. With hax and F*, we prove that those assertions will never fail.Often when people think about formally verified protocol implementations, they imagine a one-time huge investment in verification that leaves you with a codebase frozen in time. This is not the case here. We re-run formal verification in our CI pipeline every time a developer pushes a change to GitHub. If the proofs fail then the build fails, and the developer needs to fix it. In our experience so far, this is usually as simple as adding a pre- or postcondition or returning an error when a value is out of bounds. For us, formal verification is a dynamic part of the development process and ensures that the quality is high on every merge.Signal is rolling out a new version of the Signal Protocol with the Triple Ratchet. It adds the Sparse Post-Quantum Ratchet, or SPQR, to the existing Double Ratchet to create a new Triple Ratchet which gives our users quantum-safe messaging without taking away any of our existing security promises. It’s being added in such a way that it can be rolled out without disruption. It’s relatively lightweight, not using much additional bandwidth for each message, to keep network costs low for our users. It’s resistant to meddling by malicious middlemen - to disrupt it, all messages after a certain time must be dropped, causing a noticeable denial of service for users. We’re rolling it out slowly and carefully now, but in such a way that we’ll eventually be able to say with confidence “every message sent by Signal is protected by this.” Its code has been formally verified, and will continue to be so even as future updates affect the protocol. It’s the combined effort of Signal employees and external researchers and contributors, and it’s only possible due to the continued work and diligence of the larger crypto community. And as a user of Signal, our biggest hope is that you never even notice or care. Except one day, when headlines scream “OMG, quantum computers are here”, you can look back on this blog post and say “oh, I guess I don’t have to care about that, because it’s already been handled”, as you sip your Nutri-Algae while your self-driving flying car wends its way through the floating tenements of Megapolis Prime.]]></content:encoded></item><item><title>qbecc is a C compiler producing Go ABI0 assembler</title><link>https://www.reddit.com/r/golang/comments/1nw7xmk/qbecc_is_a_c_compiler_producing_go_abi0_assembler/</link><author>/u/0xjnml</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 2 Oct 2025 16:05:52 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[The resulting assembler code runs on standard Go movable stacks. This is another way how to avoid the cost of CGo Go<->C context switch. However, as no silver bullets exist, the cost of running on movable stacks is not gone in full. It have shifted to the additional handling of goroutine-local allocations for addressable local variables. The purpose of this experiment is to compare the modernc.org/ccgo/v4 and qbecc approaches with respect to resulting performance differences, if any. ]]></content:encoded></item><item><title>Red Hat Investigating Breach Impacting as Many as 28,000 Customers, Including the Navy and Congress</title><link>https://linux.slashdot.org/story/25/10/02/1538229/red-hat-investigating-breach-impacting-as-many-as-28000-customers-including-the-navy-and-congress?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>dev</category><category>slashdot</category><pubDate>Thu, 2 Oct 2025 16:02:00 +0000</pubDate><source url="https://linux.slashdot.org/">Dev - Slashdot - Linux</source><content:encoded><![CDATA[A hacking group claims to have pulled data from a GitLab instance connected to Red Hat's consulting business, scooping up 570 GB of compressed data from 28,000 customers. From a report: The hack was first reported by BleepingComputer and has been confirmed by Red Hat itself. "Red Hat is aware of reports regarding a security incident related to our consulting business and we have initiated necessary remediation steps," Stephanie Wonderlick, Red Hat's VP of communications told 404 Media. 

A file released by the hackers and viewed by 404 Media suggested that the hacking group may have acquired some data related to about 800 clients, including Vodafone, T-Mobile, the US Navy's Naval Surface Warfare Center, the Federal Aviation Administration, Bank of America, AT&T, the U.S. House of Representatives, and Walmart.]]></content:encoded></item><item><title>MoneyPod operator for calculating Pods and Nodes cost</title><link>https://github.com/vlasov-y/moneypod</link><author>/u/jksI8ygD</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Thu, 2 Oct 2025 15:58:09 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Hi! 👋 I have made an operator, that exposes cost metrics in Prometheus format. Dashboard is included as well. Just sharing the happiness. Maybe someone will find it useful. It calculates the hourly Node cost basing on annotations or cloud API (only AWS is supported so far) and than calculates Pod price basing on its Node. Spot and on-demand capacity types are handled properly.]]></content:encoded></item><item><title>Why k8s needs both PVCs and PVs?</title><link>https://www.reddit.com/r/kubernetes/comments/1nw5ij0/why_k8s_needs_both_pvcs_and_pvs/</link><author>/u/redditonation</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Thu, 2 Oct 2025 14:35:43 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[So I actually get why it needs that separation. What I don't get is why PVCs are their own resource, and not just declared directly on a Pod? In that case you could still keep the PV alive and re-use it when the pod dies or restarts on another node. What do I miss?]]></content:encoded></item><item><title>N8n added native persistent storage with DataTables</title><link>https://community.n8n.io/t/data-tables-are-here/192256</link><author>XCSme</author><category>dev</category><category>hn</category><pubDate>Thu, 2 Oct 2025 14:26:23 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How can I stop Rust from dead-code eliminating Debug impls so I can call them from GDB?</title><link>https://www.reddit.com/r/rust/comments/1nw4o45/how_can_i_stop_rust_from_deadcode_eliminating/</link><author>/u/Sweet-Accountant9580</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Thu, 2 Oct 2025 14:03:06 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I’m debugging some Rust code with GDB, and I’d like to be able to call my type’s  implementation () directly from the debugger.The problem is that if the  impl isn’t used anywhere in my Rust code, rustc/LLVM seems to dead-code eliminate it. That makes it impossible to call the function from GDB, since the symbol doesn’t even exist in the binary.Is there a way to tell / to always keep those debug functions around, even if they’re not referenced, FOR EACH type that implements ?]]></content:encoded></item><item><title>Subtest grouping in Go</title><link>https://www.reddit.com/r/golang/comments/1nw4hfp/subtest_grouping_in_go/</link><author>/u/sigmoia</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 2 Oct 2025 13:56:06 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[   submitted by    /u/sigmoia ]]></content:encoded></item><item><title>Potential issues in curl found using AI assisted tools</title><link>https://mastodon.social/@bagder/115241241075258997</link><author>robhlam</author><category>dev</category><category>hn</category><pubDate>Thu, 2 Oct 2025 13:29:55 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Why are we still talking about containers? [Kelsey Hightower&apos;s take, keynote]</title><link>https://youtu.be/x1t2GPChhX8?si=cwKkv08JHs23mUkp</link><author>/u/Diligent-Respect-109</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Thu, 2 Oct 2025 13:21:37 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Kubernetes Orchestration is More Than a Bag of YAML</title><link>https://yokecd.github.io/blog/posts/yoke-resource-orchestration/</link><author>/u/davidmdm</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Thu, 2 Oct 2025 12:15:37 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Nine HTTP Edge Cases Every API Developer Should Understand</title><link>https://blog.dochia.dev/blog/http_edge_cases/</link><author>/u/ludovicianul</author><category>dev</category><category>reddit</category><pubDate>Thu, 2 Oct 2025 12:07:51 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Breaking down Go&apos;s sync package</title><link>https://mfbmina.dev/en/posts/2025-09-19_sync-break-down/</link><author>/u/mfbmina</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 2 Oct 2025 12:04:43 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[In my opinion, Go provides excellent support for concurrent work, not only due to goroutines but also because of the language’s ecosystem. A great example of this is the sync package, which helps synchronize concurrent routines. In this post, we’ll dive into everything this package has to offer.Waitgroups are used to coordinate the execution of multiple routines. They make it easy to create and ensure that all sub-routines will finish before the main routine ends. In the post about waitgroups I explain better how they work and what changed with Go version 1.25.Mutex stands for mutual exclusion locker. Its function is to lock access to a resource while an operation is being executed, preventing other routines from trying to write to that resource at the same time. For example, what is the return of the following function?If your answer was 1000, there’s a chance you might have gotten it right, but it’s unlikely. This happens because, as routines are executed concurrently, they might try to write to the resource at the same time. To ensure this doesn’t happen, simply add a mutex and lock access to that resource.Usage is quite simple: to lock access to a record you use the  function, and when finished, just use . You just need to be careful not to fall into deadlock. There’s also the  function, which validates whether an active lock exists or not, but its use case is rarer.The RW Mutex is an evolution of the mutex where there are specific locks for writing and reading. This distinction is quite useful when one or more routines need to access a resource for reading only, but don’t want the object to be modified during its execution. However, it’s important to mention that writing has higher priority than reading, and thus, Go avoids starvation.In the example above, we can have several routines calling  to get the average of the integer list. However, if a routine decides to insert another value, everyone will have to wait for that write to finish.The atomic is a subpackage of the sync package that implements concurrency support for primitive types. Currently, it supports the following types: , , , , , , , and . With it, we can simplify the example used in the mutex:It’s necessary to note that basic operations, such as addition, have been re-implemented to ensure that routines do not contend for the resource.The  is like any other normal . It provides functions to compare, swap, assign or retrieve values, with the difference that it is safe for concurrency.The documentation itself suggests it should be used in two cases:When a key is written only once, but read multiple times. An example is a cache that only grows.When multiple goroutines read and write distinct groups of keys.Any other case is better to use the traditional map with mutexes.The  type guarantees that something will be executed only once, even if multiple routines try to execute it. An example of this could be resource initialization, as demonstrated by the Go documentation.It’s important to note that if the function panics, it will not be re-executed.As the name suggests,  works based on a conditional, meaning that when something happens, it releases the execution of a routine. This execution can be released one by one using the  function or activating all at once with .First, we initialize a  with some , an interface that implements the  and  functions. In the example, we use a mutex. When initializing each goroutine, it’s necessary to ensure the lock and then we put it in a waiting state with , which releases the lock, allowing new routines to be started. When a routine is released with  or ,  acquires the  again and releases the code execution. Go’s documentation recommends that  happens inside a loop waiting for a condition, because  alone can’t tell if something has happened or not, but this is not strictly necessary. The general flow then is:goroutine acquires the lock →  →  →  →  →  →  → goroutine releases the lockThe  provides a way to deal with short-lived objects in memory. This helps relieve pressure on the GC, as memory space is always reused. The official documentation cites the  package as an example, which uses pools as temporary output buffers that adjust their size as needed.To initialize a  we need to define its initialization function. When using  we retrieve what is saved in memory and with  we write a new value to it.  is only used if there is nothing allocated into the memory.The  package provides several functionalities that are extremely useful when working with multiple goroutines. It’s possible to control execution with  and  types.  ensure everything will be executed. , , and atomic types prevent resource contention. Finally,  relieves the GC’s work when it’s possible to work with short-lived objects in memory. Without a doubt, this package is crucial for anyone working with goroutines. If you wish to understand the implementation details of this package, I recommend watching the talk presented at Gophercon UK 2025, Deep dive into the sync package by Jesus Hawthorn. There is also a talk that I presented at Golang SP about it. Tell me in the comments if you have already used this package and if it helped you in any way. If you haven’t used it yet, comment on what you thought of the post.]]></content:encoded></item><item><title>QJS: Run JavaScript in Go without CGO using QuickJS and Wazero</title><link>https://www.reddit.com/r/golang/comments/1nw0djm/qjs_run_javascript_in_go_without_cgo_using/</link><author>/u/lilythevalley</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Thu, 2 Oct 2025 10:45:10 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hey, I just released version 0.0.3 of my library called QJS.QJS is a Go library that lets us run modern JavaScript directly inside Go, without CGO.The idea started when we needed a plugin system for Fastschema. For a while, we used goja, which is an excellent pure Go JavaScript engine. But as our use cases grew, we missed some modern JavaScript features, things like full async/await, ES2023 support, and tighter interoperability.That's when QJS was born. Instead of binding to a native C library, QJS embeds the QuickJS (NG fork) runtime inside Go using WebAssembly, running securely under Wazero. This means:A fully sandboxed, memory-safe runtime.Here's a quick benchmark comparison (computing factorial(10) one million times):Please refer to repository for full benchmark details.Full ES2023 compatibility (with modules, async/await, BigInt, etc.).Secure, sandboxed webassembly execution using Wazero.Zero-copy sharing of Go values with JavaScript via ProxyValue.Expose Go functions to JS and JS functions back to Go.The project took inspiration from Wazero and the clever WASM-based design of ncruces/go-sqlite3. Both showed how powerful and clean WASM-backed solutions can be in Go.If you've been looking for a way to run modern JavaScript inside Go without CGO, QJS might suit your needs.I'd love to hear your thoughts, feedback, or any feature requests. Thanks for reading!]]></content:encoded></item><item><title>The architecture behind 99.9999% uptime in erlang</title><link>https://volodymyrpotiichuk.com/blog/articles/the-architecture-behind-99%25-uptime</link><author>/u/NoBarber9673</author><category>dev</category><category>reddit</category><pubDate>Thu, 2 Oct 2025 09:39:54 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[
Hey there, have you ever wondered how to build the most stable application in the world? What characteristics does such an application have, and what architecture styles make it possible? It’s pretty impressive how apps like Discord and WhatsApp can handle millions of concurrent users, while some others struggle with just a few thousand. Today, we’ll take a look at how Erlang makes it possible to handle a massive workload while keeping the system alive and stable.
I believe that the best way to describe something complex is to start from the simple things and then gradually move to the complex ones. So let’s start with a simple example.
Imagine such a wonderful day:
You are driving your Porsche Panamera and decide to call your good friend to chat about the upcoming party.  
Your phone sends a signal to the nearest cell tower.  
The network figures out the best route to reach your friend.  
The system establishes the connection and rings your friend’s phone.  
Once your friend picks up, you both start speaking and having a great time.  
Everything works perfectly… until it doesn’t. While you’re driving, your phone switches to a different cell tower, and suddenly the connection drops.
And it would be great if your broken call didn’t mess up other calls in the network (John on the other line definitely doesn’t want to deal with your issues, right?). Ideally, you’d just retry the call and keep chatting with your bro about the upcoming party.
If we turn this situation into a system design lesson, we can pull out a couple of thoughts about reliable systems:
What you do shouldn’t hurt others in the system. In other words, things should be isolated so they don’t affect each other.  
We need mechanisms to monitor and manage those isolated things in a way that lets the system recover from unexpected failures.  
At first glance, this may seem pretty simple. You can achieve something like this in almost any popular programming language. But the how is very different, and so are the feelings you get while developing and debugging such systems.
Let’s check what the options
Imagine that we are trying to build a Discord. This is a highload system which handles millions of calls simultaneously. We need to provide the best user experience so people do not leave us and go to Microsoft Teams for example.
How would you build such system?
Imagine you as a software engineer started working on the system, and you try to handle all calls in one main loop, like this:
  
At first look, this seems pretty simple and elegant. However, in reality each call has a connection that needs to be handled. If you put all of them into a single synchronous loop, then while that loop is busy handling one call, all the other calls are forced to wait.
Some systems allow you to create OS threads to handle work in parallel on multiple CPU cores. OS threads are small work units managed by the OS kernel that can run in the background so the main thread of the process isn’t blocked. They are scheduled preemptively, which means the OS can pause one thread and run another at any time, even on a single CPU core.
Great! You might think, “Let’s give each call its own thread!”
While this seems like a simple way to run things concurrently, there are serious trade-offs:
All threads in a process share the same memory. When you share the same memory, you can have race conditions when threads trying to work on the same data in parallel:   
To prevent this, you must use the locks to the data, however it would affect how the threads are waiting for each other:
  
See how threads mostly waiting, not working!
Even if you used the locks, there can be a situation when one of the threads put corrupted data in the shared memory and other threads received shared corrupted data. How would you handle such case?
OS threads are really heavy in terms of memory. Each OS thread takes at least 512 KB just to exist, without doing anything. To compute how many “empty” threads you can have, just divide your RAM by the per-thread memory. It will give you amount of calls that you would have on your machine. If your machine has 8GB of available memory - it would be like 8gb / 512kb = 16,384. Huh, that’s not a millions unfortunately…
We need to observe the threads: monitor their status, track failures, and react when something goes wrong. With shared state, how would you detect issues in the system, restart only the affected processes (or all if necessary), and ensure everything keeps running smoothly? Pretty complex, right?
What if we introduce an event-driven architecture, where your server communicates with other I/O systems via asynchronous events:
  

The event loop won’t be blocked by a single task, allowing you to handle thousands of them on demand, step by step. For CPU-heavy tasks you can use threads, which help prevent blocking the main thread. 
This kind of model already shows us why so many modern languages like Node.JS, Python lean toward event-driven design: it’s efficient and helps avoid bottlenecks. But it also leaves open questions - how do you supervise those tasks? How do you recover from failures? How do you keep track of thousands of concurrent jobs without going crazy? Only after asking those questions it becomes clear why Erlang is interesting.
How erlang solves such problems
Erlang is built on top of the BEAM virtual machine. “Virtual” means it runs on top of your operating system and provides abstractions for things like processes, scheduling and message passing.
Processes in virtual machine operate in isolated memory chunks and communicate with each other via messages (see  Actor model). Think of them as people sitting in their own rooms, doing their work and sending emails to communicate (no shared memory). 
The idea behind this design is that each process can fail or succeed independently, and the system can handle millions of them concurrently without affecting the others.
Let’s take a look at the main structure of each process:
Stack – keeps track of function calls.  
Heap – stores the process internal data.  
Mailbox – a message queue that allows the process to receive messages from other processes.  
PCB (Process Control Block) – metadata about the process, used internally by Erlang.  
So if you want to spawn a process in the system, you’re really just creating this minimal internal structure that the BEAM needs: its unique identificator, mailbox, stack, heap, and some metadata. It’s lightweight because the process isn’t created by the OS, there is no system call and no need to touch the kernel. Instead, it’s created inside the BEAM virtual machine itself, which takes care of scheduling and managing all these processes.
In theory, creating an Erlang process takes only 327 words (where word is 4 bytes on 32-bit systems or 8 bytes on 64-bit systems), that’s the raw overhead of its internal structure from the above. Compare that to an OS thread, which usually reserves at least 512 KB (and often up to 8 MB) just for the existence.
If you can spawn millions of processes, the next question is: how do you keep track of them? Erlang gives you not only cheap isolated processes, but also tools to see, monitor and control them while everything is running. 
You can set up relationships between these lightweight processes and define how they should behave if some of them misbehave. This feature is available as a ready-to-use abstraction in the erlang libraries (a set of generic behaviors), but it’s also something you can build on top of the features available in Erlang and extend on your own. 
Imagine you’re setting up call connections on your server. Sometimes network errors happen, and you need to restart the connections until they work properly. In Erlang, with linked processes, you can catch a linked process abnormal activity and take the necessary actions yourself - for example, restart it:
Here we are trying to spawn 10 calls where each can fail with equal probability and if they fail, we want to restart them until they restarted succesfully. Here is a demo:
And when you have such a mechanism, you can start building more complex systems on top of it. You can create a supervision tree, where each process monitors another process using built-in behaviors in Erlang (See https://www.erlang.org/doc/apps/stdlib/supervisor). This way, every process is under control — if something goes wrong, that process can be restarted without affecting the whole system. Plus, you always have a clear picture of what’s going on in your system.
How does virtual machine schedule millions of processes?
To understand how this works, let’s go back to the early days of Erlang. The first two decades of Erlang releases, between 1986 and 2006, it supported using at most one CPU core. This means that only one process could execute at a time on that core. However, they had an ability to execute hundreds of thousands processes very fast without blocking single CPU core with long running tasks. How?
Imagine that you have a queue of waiting processes to be executed:
Create an array of values from 1 to 10000  
In this case we will take those processes one by one in first-in-first-out order (sorted by the date of the scheduling):
We will execute first process - it’s pretty simple operation, and we will take only step to complete it.  
We will execute second process - while we execute it, we see that we make array like , then , then  and so on.
In some moment of time, we will understand that this task takes more operations than we would expect to complete it in one batch. In such case, for example on number 5000, we save the current call stack and step where we stop the execution for current process
and intermidiate data in the process heap, and we move to another process! We don’t want to block our CPU with long running task, so we give the time
for other waiting processes.  
We execute third process - which is also pretty fast.  
We return to process number 2, in our heap we already have array with values from 1 to 3000, and we continue to process our array until we finish
it or again we see that this process tooks a many operations to do, we pause it and give other processes their time slice.  
Let’s check this in action with simple demo code, where we simulate the case from above:
And let’s try to execute it:
  
As you see, all the processes get the time for their execution, and they have not blocked each other while executing long running task. I mentioned this technique earlier with OS threads — it’s called preemptive scheduling. It’s used by schedulers in both the BEAM and the OS, where each scheduler runs on a single CPU core and allocates CPU time to every task waiting to be executed.
In erlang, settings like how many function calls a process can execute before the scheduler switches to another task, along with other configuration, are stored in the Process Control Block I mentioned earlier. You can inspect them using:
That’s basically magic behind the concurrent processes on one CPU core. For you, as user of this system, it may looks like you running parallel execution, but in fact, they are running sequentially giving each other space to complete their work. Worth to add, that starting from May 2006, you can run multiple schedulers with multiple CPU cores. It will execute the code truly in parallel between them.
One of the features I love the most
Have you ever thought you could change a part of your application on the fly without redeploying it? Yeah, it’s like changing the tires on a moving car… but safer. And what if I told you that you can debug server processes, functions, and even add new functions to a running system without any downtime? Erlang lets you do that.
To enable hot code load functionality, Erlang has something called the code server — a process that tracks module states and can keep two versions of the same module in memory. When you load the new version into the memory - processes running the current (old) version will finish their work on that version, while processes started after a new load will run the new version. This way, you can debug and update your system very smoothly without downtime.
Also, there is a concept of  release upgrades and downgrades . Basically, we can specify a rollout strategy for every release so our code doesn’t need to drop connections at all. It’s a pretty big topic because you need to manually show how to transform the state from the old version to the new version, what to do with processes that are currently running, and so on. However, on my opinion, it’s much simpler to use rolling upgrades with node restarts (which is also provides zero downtime).
In Erlang, distribution is built right into the system. Just like you can supervise processes, you can also monitor and use other nodes. Nodes can send messages to each other or call functions remotely using RPC (Remote procedure calls), allowing you to coordinate work, handle failures, and scale across machines.
For example, you can build a cluster of nodes that share the workload via load balancing. If one node goes down, the others can seamlessly take over its tasks. Let’s build a simple load balancer to see how it works.
First of all, we will create a load balancer process responsible for getting a random node from the available list and passing it to the caller:
After that, we will start several nodes manually (on the cloud, this could be done automatically on demand) with specific names like these:
and pass them to our load balancer process as initial argument, so our nodes are aware of each other:
  
We see that our erlang nodes are set up correctly and that we’re ready to route our connections. In the next step, we need to create a  module, which will take the node received from the load balancer and perform a remote procedure call on the selected node:
Now we’re ready to accept connections and observe how the application balances requests across the available nodes:
  
As you can see, our requests are distributed randomly across the available nodes. If one of them fails, erlang detects it, removes it from the node list, and load balancer routes traffic to the remaining healthy nodes.
That was a lot of stuff, right? Erlang gives us so many features that in other languages would require special tooling or extra libraries. It makes communication in the system isolated, distributed, reliable, and fault-tolerant, with the ability to debug and even change code at runtime. It’s no miracle that huge, complex systems can achieve 99.999999% uptime when built with Erlang — it’s the architecture itself that allows developers to build such systems quickly and understandably.
I hope you found this article useful, maybe it even inspires you to build your first app on Erlang or Elixir. Don’t forget to subscribe, and see you next time!]]></content:encoded></item><item><title>How Israeli actions caused famine in Gaza, visualized</title><link>https://www.cnn.com/2025/10/02/middleeast/gaza-famine-causes-vis-intl</link><author>nashashmi</author><category>dev</category><category>hn</category><pubDate>Thu, 2 Oct 2025 09:23:50 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[
            Israel’s nearly two-year war pushed parts of Gaza into “man-made” famine, according to a report published in August by a United Nations-backed initiative, deepening the Palestinians’ struggle for survival under relentless bombing, mass displacement and the spread of disease.
    
            The report by the Integrated Food Security Phase Classification (IPC), a UN-backed expert panel that assesses global food insecurity and malnutrition, helped to fuel growing international outcry over Israel’s campaign in Gaza following the Hamas-led October 7, 2023, attacks –and was cited by some of countries that recently made moves towards formally recognizing a Palestinian state. The IPC forecast that by the end of September nearly a third of Gaza’s total population would face famine conditions, although it has not yet provided an update on that forecast.
    
            In Gaza governorate alone – the largest by population of five in the Gaza Strip – more than half a million people were condemned to a cycle of “starvation, destitution and death,” the IPC added. The Israeli assault on Gaza City, which Israeli Prime Minister Benjamin Netanyahu says is targeting one of Hamas’ “remaining strongholds” has choked relief operations for starving Palestinians, according to rights workers.
    
            Michael Fakhri, the UN’s special rapporteur on the right to food, accused Israel of using hunger “as a weapon against Palestinians,” in violation of international law.
    
            “Israel is using food and aid as a weapon to humiliate, weaken, displace and kill Palestinians in Gaza,” Fakhri told CNN on August 28.
    
            Israel rejected the IPC’s findings, with the Israeli agency that oversees the entry of aid into Gaza claiming the report was“false” and based on “partial, biased” data “originating from Hamas.” Netanyahu slammed the UN-backed report, in a statement from his office, adding that “Israel does not have a policy of starvation.”
    
            Israel has since insisted that it has stepped up the entry of aid into Gaza. But aid agencies say that Israel’s intensification of the war, particularly around Gaza City, has compounded the misery faced by Palestinians. Here is a look, in five charts, at how the situation described by the IPC materialized.
    The IPC projected that famine would spread to Deir Al-Balah, central Gaza and further south, in Khan Younis by the end of September, affecting nearly 641,000 people.
    
            Up to June 2026, at least 132,000 children under the age of five are expected to suffer from acute malnutrition, including more than 41,000 severe cases of children at heightened risk of death, the IPC added.
    
            Under the IPC – a five-phase indicator used to measure the severity of food insecurity – a famine can only be declared if three thresholds are met: at least 20% of households face extreme food shortages, the proportion of children assessed as acutely malnourished reaches a certain threshold, and at least two in every 10,000 people die each day from starvation, or from malnutrition and disease.
    
            Israel accused the IPC of lowering the second threshold of acutely malnourished children for a famine declaration, which the IPC has denied.
    
            Researchers use three methods for assessing child malnutrition – either a child’s height and weight, their BMI, or a child’s mid-upper arm circumference, known as MUAC. The IPC used the latter, a metric employed since 2019, to determine that at least 15% of children aged six to 59 months have a mid-upper arm circumference of less than 125mm or edema, the agency told CNN. The thresholds for famine classification are “standard and were not modified for Gaza,” the IPC told CNN, adding that the MUAC metric “is the measurement most frequently available and has strong correlation with mortality outcomes,” and was also used in famine classifications in Sudan and South Sudan this decade.
    
            Human rights advocates say Israel’s destruction of health infrastructure and intensified hostilities have hampered efforts to document the full scope of famine in Gaza.
    
            After more than 700 days of war, 455 Palestinians have died of malnutrition or starvation, including 151 children, the health ministry in Gaza reported on  October 1. One hundred and seventy-seven of the total number have died of malnutrition or starvation since the IPC confirmed famine on August 15, it said.
    
            Israel’s vast web of bureaucratic impediments, including delayed approvals, arduous border checks and the arbitrary rejection of items, throttles the amount of aid that makes it to the other side of the border and sends food costssoaring, the UN and aid agencies say.
    
            After visiting the region in late August, US Senators Chris Van Hollen and Jeff Merkley, both Democrats, warned that Netanyahu’s government was “implementing a plan to ethnically cleanse Gaza of Palestinians” and accused Israel of using food “as a weapon of war.” Israel has denied the allegations.
    
            “The findings from our trip lead to the inescapable conclusion that the Netanyahu government’s war in Gaza has gone far beyond the targeting of Hamas to imposing collective punishment on the Palestinians there, with the goal of making life for them unsustainable,” said the report, published on September 11. “That is why it restricts the delivery of humanitarian assistance.”
    
            Israeli authorities have said trucks “remain uncollected” at the border with Gaza – accusing the UN of failing to coordinate the entry of vehicles into the strip.
    
            But Sam Rose, the acting director of affairs for the UN agency for Palestinian refugees (UNRWA) in Gaza, says Israel – which has near-total jurisdiction over what goods enter and exit Gaza – has controlled “to the calorie” the volume, type and overall flow of food into the enclave. “The system is designed not to function smoothly,” he said.
    
            Israeli authorities “know and analyze each truck that goes into Gaza, the weight and the calories,” a senior official with COGAT, the Israeli agency that controls the entry of aid into the enclave, said in September. According to a COGAT statement published in response to the IPC famine declaration, “analysis of contents of food aid trucks that entered the Gaza Strip reveal that 4,400 calories per person per day entered Gaza since the beginning of August.”
    
            However, as of May, Palestinians were consuming just 1,400 calories per day – or “67 per cent of what a human body needs to survive,” at 2,300 calories, the UN reported in June.
    
            Last October, Israel’s government banned UNRWA from operating in areas under its control, a prohibition that went into effect in January, having accused the agency of failing to stop Hamas’ alleged theft of aid. An internal US government review found no evidence of widespread theft by Hamas of US-funded humanitarian aid in Gaza.
    
            When the trickle of relief does enter the strip, aid workers face intensified hostilities, damaged roads and limited fuel supplies – impeding internal distribution efforts, minimizing viable routes and blocking access to displaced Palestinians, said Rose.
    
            Israel says UN aid makes up only part of the relief that gets into Gaza. A senior COGAT official told a briefing in early September that 27% of the trucks entering Gaza are UN vehicles, claiming it was “a lie” that the UN had brought in 600 aid trucks a day before the war.
    
            “There is no famine in Gaza. Period,” the official said, adding that “Israel and the IDF are trying to strengthen the humanitarian situation in Gaza with partners.”
    
            In May, the US and Israeli-backed Gaza Humanitarian Foundation (GHF) established a program that now plans to operate up to five distribution sites in the enclave, all but one in southern Gaza – which rely on private military contractors and largely replaced 400 UN-led hubs.
    
            Relief and health workers say these other methods of delivering food in Gaza, including the GHF sites and aid pallet drops from planes, are dehumanizing and inaccessible for many Palestinians, and expose them to injury or death.
    
            At least 1,172 people were killed “near militarized supply sites” between May 27 and September 9, the UN said on September 10, with another 1,084 deaths along convoy supply routes. In August, UN experts called for the immediate closure of GHF-operated sites in Gaza and accused Israeli forces of opening “indiscriminate fire” on people seeking aid there. The advocates warned the hubs are “especially difficult” for women, children, people with disabilities and elderly Palestinians to access.
    
            GHF has defended its work in Gaza and said earlier in September that it was the only organization in Gaza able to deliver food “at scale without interference.” The organization also said that it had “repeatedly sought collaboration with UN agencies and international NGOs to deliver aid side-by-side” but that the UN had “declined those offers.” The Israeli military has acknowledged firing warning shots toward crowds in some instances and denied responsibility for other casualties near aid hubs.
    
            The US and Israel plan to set up 12 additional sites across the enclave, an Israeli official told CNN in August. However, there is no indication that the new sites have been established. In September, GHF said it had sought IDF permission to open sites in northern Gaza but that Israel had not granted the permission.
    
            “With parents injured and their siblings starving, many teenagers and young adults are taking the risk,” Mohammed Khaleel, an American surgeon who was deployed to Gaza earlier this year, told CNN in August.
    
            “We’ve even heard some people report that they will go and accept their fate. Dying from a gunshot may be preferable to dying from starvation,” he added.
    
            Israel’s two-year offensive in Gaza had left just 1.5% of cropland accessible and undamaged as of July 28, according to the UN – largely preventing Palestinians from cultivating produce.
    
            That destruction, coupled with Israel’s fishing ban and intensified assault in the north, has   further limited the sources of food available to hundreds of thousands of displaced Palestinians.
    
            “It is not by chance that Israel has focused its starvation tactics in northern Gaza,” Fakhri, the UN special rapporteur, said. “They have announced their intent to push people from the north to the south of Gaza… Just as now, the focus of their starvation campaign on Gaza City correlates with their invasion plans.”
    
            The military’s invasion of Gaza City will collapse an “already fragile” aid supply chain, warned Arif Husain, chief economist at the World Food Programme.
    
            Relief agencies need a ceasefire, unimpeded humanitarian access, large-scale multi-sector aid, protection of civilians and infrastructure – and restoration of commercial and local food systems – to reverse famine in Gaza, said Husain.
    
            “We are already at the brink. Another escalation – especially in Gaza City – could push the situation into unimaginable catastrophe,” he added. “It will not only result in more deaths but destroy any foundation for future recovery.”
    ]]></content:encoded></item><item><title>PixiEditor is now available on Flathub</title><link>https://pixieditor.net/blog/2025/10/02/september-status/</link><author>/u/flabbet</author><category>dev</category><category>reddit</category><pubDate>Thu, 2 Oct 2025 08:36:10 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Hi, hello, and welcome to the September PixiEditor Status update! On today’s menu:New renderer and a story of great failureFlatpak is one of the package managers for Linux, which is designed to be the universal package manager for Linux software, especially GUI apps.Up until now, PixiEditor was distributed via  and  formats. It was suboptimal, as integrating PixiEditor to your Linux desktop required more effort.Flatpak solves that issue. It automatically integrates with your desktop environment and manages updates. So if you are on Linux, give it a try!Due to the nature of PixiEditor, rendering is a  tricky part. Not only does Node Graph allows for arbitrary customizations, shaders and operations, PixiEditor has vector capabilities (rendering crisp vectors on small documents)
and animations.Here’s a quick comparison of new renderer with heavy animationSingle threaded renderingPixiEditor renders on the UI Thread, which means that heavy graphs can lag or even freeze the UI.The logical solution was to make rendering on a separate thread.My little render thread journeyIn short, Render Thread is a thing that runs in the background, independently of the main app and renders graphics at it’s own pace. So whenever the main app requests a new frame,
render thread grabs this request, renders and hands the frame to the main app.And I really wish it was that simple. This will be a little technical, so if you are just interested in new features, feel free to skip it.PixiEditor uses Vulkan and OpenGL for rendering. However, neither Vulkan nor OpenGL is “standalone”, we must first access Avalonia’s GPU context to hook our own rendering. We use Avalonia’s composition renderer to update the UI with rendered stuff.Current renderer performs rendering “on demand”, meaning that each UI control sends a render request with its surface to draw on. It has a few benefits:No additional intermediate surfaces,Out of the box “render only what you see” approachMore error-prone (surfaces might get disposed mid-flight)Potentially rendering the same frame multiple timesIt’s harder to manage renders and perform optimizationsIt’s virtually impossible to make rendering on other threadSo the very first step was to invert the rendering logic, so we have more control over each rendered frame.Refresh request —> UI control surface —> Render to control’s surfaceRefresh request —> Render into intermediate surface —> UI control renders intermediate surfaceHaving one managing system that decides what gets rendered and in what resolution opens up a lot of optimization possibilities.For example if you have Node Graph opened and layer’s panel visible, the renderer will render layer’s preview only once in slightly higher resolution (because node previews are bigger), instead of rendering it twice
in different resolutions.Great, so we have a system that produces bitmaps/surfaces with all previews and frames, that are just grabbed by UI controls to display. So the next logical step is to render these asynchronously.Before I decided to jump into building separate thread solution, I tried better rendering scheduling in the Avalonia’s UI Thread dispatcher. It is still one thread, but any lags should be “spread out” over a few frames.At first it seemed to work really great, playing animations for heavy graphs was smooth and the UI was responsive. Unfortunately, after some time I discovered that manipulating heavy graphs (like moving a layer)
resulted in degraded performance. Rendering a singular layer into the bitmap wasn’t slower than in previous renderer, but scheduler fired less often, so it seemed like it was laggy.The only logical solution left at that time, was to implement separate render thread.Remember the part where I mentioned we use Avalonia’s GPU context and compositor?
Yeah, to get a separate render thread I had to properly synchronize not only access to GPU API, but also render thread’s render loop with Avalonia’s compositor.Well, it was. After many days of freezes, crashes and debugging, it seemed to finally work. I developed it on a test project, but still.Until I ran PixiEditor with it…It worked like a hot mess…Something had to be done, I think the whole rework already took me a full 2 weeks, where one whole week was dedicated to render thread. Shipping renderer that provides worse performance is unacceptable, even
if it’s better in some scenarios.At that point, I already invested so much time into this. I decided to give up separate render thread for now. However the whole point of the rework was to fix freezes and improve the performance.I got my shit together and facts straight.> New rework solves laggy animation performance
> Manipulation performance is degraded due to background scheduler update rate> Can’t I just fire rendering immediately when user manipulates the canvas and do background rendering for animation?5 lines of code and problem solved.It doesn’t completely replace separate thread solution. It still runs on the main thread, but it’s already much better. At least according to my tests.(to any brave soul, if you want to play around render thread solution I made, check the  branch both for  and )GPU chunks and direct viewport renderingThis optimization is quite exciting for me. It’s only related to drawing/painting performance, but it’s been on my mind since the beginning of version 2.0.PixiEditor uses chunks for layers. So instead of storing full resolution layer it only stores chunks that have something in them. For example, if you have a 1024x1024 document, one layer of this document
consists of 4 chunks that are 256x256 each. If you only draw on one chunk, others won’t be stored in memory.This system was developed for version 1.0 and lost its meaning during transition to 2.0. Due to the performance reasons we had to use intermediate, full resolution texture before drawing to the screen.
Which kills any benefits of chunks.Why did it work like that? Well, version 1.0 had CPU-only rendering, and CPU chunks were a leftover from 1.0. I tried making them GPU a while ago, but it had major problems back then.Fortunately, I managed to properly implement GPU chunk textures, which significantly improves rendering performance. Thanks to this, I was able to get rid of intermediate surface and draw the chunks
to the screen directly.Direct screen rendering opens a possibility for another amazing optimization technique called occlusion culling. In the new renderer, PixiEditor will only draw chunks you see on the screen, if you zoom the
viewport, chunks outside the visible area won’t be rendered.Here’s a preview with shrunk visible area to see the effectFurthermore, if you have a document with a graph, that consists of layers only, drawing shouldn’t allocate more memory than amount of chunks required. So creating huge documents by itself won’t cause memory problems for PixiEditor.
Saving .pixi files will also save filled chunks only.Please note that the new renderer is not yet released. It will require some more testing before it lands in the development version.Slice Text, Character Position and Text InfoWe’ve added a few new nodes for text manipulation.And here’s a quick demo of what you can do with themThe Posterize node reduces the number of colors in an image, creating a stylized effect. It works by mapping the colors in the input image to a limited set of colors based on the specified number of levels.We added a node for that. There are 2 modes available: RGB and Luminance. The latter will produce grayscale posterization.We are still working on fixes and improvements after the 2.0 release along with a Brush Engine. Hopefully I’ll have more news about it in October status update. is fully funded by the community - please consider supporting us with the !
Support NowAlso, thanks to Sebasthem for letting us use a photo of this cute kitty! 
Sebasthem’s InstagramThank you for reading, see you next month!]]></content:encoded></item><item><title>Running Blender on Linux with Termux:X11</title><link>https://www.reddit.com/r/linux/comments/1nvxlvg/running_blender_on_linux_with_termuxx11/</link><author>/u/OutrageousPassion678</author><category>dev</category><category>reddit</category><pubDate>Thu, 2 Oct 2025 07:51:19 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[This runs 100% local and offline. Phone is a moto edge 2024 running Debian with xfce4 on Termux with X11. Increased the scale in X11 settings for better usability and performance. Screen recording here: https://imgur.com/a/hG2jls8]]></content:encoded></item><item><title>Immich v2.0.0 – First stable release</title><link>https://github.com/immich-app/immich/discussions/22546</link><author>Alexvb</author><category>dev</category><category>hn</category><pubDate>Thu, 2 Oct 2025 06:25:43 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Built a desktop app with Tauri 2.0 - impressions after 6 months</title><link>https://www.reddit.com/r/rust/comments/1nvvoee/built_a_desktop_app_with_tauri_20_impressions/</link><author>/u/CodeWithInferno</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Thu, 2 Oct 2025 05:51:39 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Used Tauri to build Lokus, a note-taking app. Thought I'd share my experience since Tauri 2.0 is still relatively new. Previously built desktop apps with Electron. Hated the bloat. Tried Tauri for this project. - Bundle size: 10MB vs 100MB+ with Electron - Memory usage: ~50MB vs ~200MB - Startup time: sub-1 second consistently - Native feel on each platform - Rust backend = actual performance for heavy operations (search, graph layout) - Hot reload works great - Debugging Rust<->JS bridge can be painful - Smaller ecosystem than Electron - Some platform-specific quirks (especially Linux) - IPC serialization needs careful planning - Documentation is good but not as extensive as Electron - Full-text search across 10k files: ~50ms (would be 500ms+ in pure JS) - Graph layout calculations in Web Worker + Rust: 60fps with 1000+ nodes - File operations are instant (no Node.js overhead) React Frontend <-> Tauri IPC <-> Rust Backend ├─ File System ├─ Search Engine ├─ Plugin Manager └─ MCP Server  Absolutely. The performance gains are worth the learning curve. Especially for apps that do heavy computation. - If your app is simple CRUD, Electron might be easier - If you need extensive native integrations, Tauri 2.0 shines - If bundle size matters, Tauri is a no-brainerHappy to answer questions about the Rust/Tauri experience!]]></content:encoded></item><item><title>Welcome and Opening Remarks</title><link>https://www.youtube.com/watch?v=o0tRD1joNuQ</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><category>k8s</category><enclosure url="https://www.youtube.com/v/o0tRD1joNuQ?version=3" length="" type=""/><pubDate>Thu, 2 Oct 2025 00:46:16 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Welcome & Opening Remarks
Kyverno Maintainers and Community Members welcome all event participants]]></content:encoded></item><item><title>Orchestrating Policy-Driven Tests with Kyverno, Argo Events, and Testkube -- Sonali Srivastava</title><link>https://www.youtube.com/watch?v=cNn-OtEGDvQ</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><category>k8s</category><enclosure url="https://www.youtube.com/v/cNn-OtEGDvQ?version=3" length="" type=""/><pubDate>Thu, 2 Oct 2025 00:46:16 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Orchestrating Policy-Driven Tests with Kyverno, Argo Events, and Testkube -- Sonali Srivastava, Technology Evangelist at Improving
As Kubernetes adoption grows, each deployment introduces potential security, compliance, and operational risks that manual governance cannot scale to address. DevOps teams deploy code multiple times a day, while security and compliance teams demand thorough reviews. The result is often a trade-off: either slowing innovation or weakening security posture. Over time, configurations drift from intended standards, while frameworks like SOC 2, PCI DSS, and GDPR demand continuous proof of compliance. Policy-as-Code helps close this gap by codifying rules such as resource limits and compliance checks directly in Kubernetes. In this talk, we'll show how policies can do more than enforce guardrails; they can trigger automated tests. Using Kyverno for policy enforcement, Argo Events for event capture, and Testkube for test orchestration, we'll demonstrate how violations can automatically run smoke or regression tests, providing actionable feedback instead of simply blocking deployments.]]></content:encoded></item><item><title>How Kyverno has Changed Platform Engineering Security -- Pedro Ignacio, Itaú Unibanco</title><link>https://www.youtube.com/watch?v=bWDuJ94NhGo</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><category>k8s</category><enclosure url="https://www.youtube.com/v/bWDuJ94NhGo?version=3" length="" type=""/><pubDate>Thu, 2 Oct 2025 00:46:16 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

How Kyverno has Changed Platform Engineering Security -- Pedro Ignacio, Senior Platform Engineer, Itaú Unibanco
In this session we're going to explore how Kyverno, applied in the context of Kubernetes clusters has changed the way we implement guardrails and controls into our platforms. We'll explore the most important issues Kyverno addresses within the platform engineering landscape as well as see real-world examples of how the solution is being used to protect environments and to guarantee security best practices bettering the overall Developer experience while delivering more secure applications faster.]]></content:encoded></item><item><title>Thank You &amp; Closing Remarks</title><link>https://www.youtube.com/watch?v=YJXThy4lksA</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><category>k8s</category><enclosure url="https://www.youtube.com/v/YJXThy4lksA?version=3" length="" type=""/><pubDate>Thu, 2 Oct 2025 00:46:16 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Thank You & Closing Remarks
Kyverno Maintainers and Community Members give thanks to event organizing committee and share details about the Kyverno Project and contribution opportunities in closing]]></content:encoded></item><item><title>Kyverno Gets Smarter: Writing Dynamic Policies with CEL -- Koray Oksay</title><link>https://www.youtube.com/watch?v=P7e5cy5Vi9E</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><category>k8s</category><enclosure url="https://www.youtube.com/v/P7e5cy5Vi9E?version=3" length="" type=""/><pubDate>Thu, 2 Oct 2025 00:46:16 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Kyverno Gets Smarter: Writing Dynamic Policies with CEL -- Koray Oksay, Kubernetes Consultat at Kubermatic
Kyverno’s YAML-first policy model is easy to adopt, but as complexity grows, traditional match and pattern rules can become flaky or overly verbose. CEL (Common Expression Language), a game-changing addition that brings dynamic, context-aware logic, now supports Kyverno policies to overcome this. In this session, you’ll learn how CEL unlocks a new level of flexibility in Kyverno. We will explore how to write cleaner, smarter policies by embedding CEL expressions into validation conditions, preconditions, and match logic. You’ll see real-world use cases where CEL simplifies policy writing, improves performance, and enables previously impossible or challenging tasks in pure YAML. Whether you’re writing policies for security, governance, or multi-tenant control, CEL gives you the power to go beyond boilerplate YAML and build dynamic, context-sensitive guardrails without leaving Kubernetes.]]></content:encoded></item><item><title>Green Platform Engineering: Sustainable Kubernetes with Kyverno Policies -- Rajeev Samuel Devadas</title><link>https://www.youtube.com/watch?v=Kje6kQK2J3k</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><category>k8s</category><enclosure url="https://www.youtube.com/v/Kje6kQK2J3k?version=3" length="" type=""/><pubDate>Thu, 2 Oct 2025 00:46:16 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Green Platform Engineering: Sustainable Kubernetes with Kyverno Policies -- Rajeev Samuel Devadas, Application Consultant Architect at IBM Corp's Hybrid Cloud Transformation Division
Platform engineering teams increasingly prioritize sustainability alongside developer experience. This presentation explores how Kyverno policies enable organizations to build environmentally conscious Internal Developer Platforms (IDPs) that reduce resource waste while enhancing developer productivity. We'll demonstrate how policy-as-code approaches help teams achieve their sustainability goals without sacrificing innovation or deployment velocity. Our analysis shows that implementing resource optimization policies through Kyverno reduces cluster resource consumption by 40% while improving application performance. We'll showcase practical policies that automatically right-size workloads, enforce resource limits, and prevent overprovisioning. The session includes real examples of mutation policies that inject sustainability best practices, such as automatic horizontal pod autoscaler configurations and efficient container image selection based on size and vulnerability metrics. We'll present a comprehensive framework for building self-service platforms where Kyverno policies guide developers toward sustainable choices. This includes integration patterns with developer portals like Backstage, where policy validations provide immediate feedback on resource efficiency. We'll demonstrate how generate policies can automatically create namespace quotas and limit ranges that balance developer freedom with environmental responsibility. Advanced examples will cover using Kyverno's background scans to identify and remediate resource waste in existing deployments. Attendees will learn strategies for implementing graduated policy enforcement that educates developers while preventing unsustainable practices. We'll share metrics dashboards that visualize the environmental impact of policy decisions and demonstrate how platform teams can use Kyverno to report on sustainability KPIs. This session equips platform engineers with practical tools to build developer platforms that deliver exceptional experiences while contributing to organizational sustainability goals through intelligent policy automation.]]></content:encoded></item><item><title>I Broke My Cluster with Kyverno (So You Don’t Have To) -- Vinod Kumar, Arcesium India</title><link>https://www.youtube.com/watch?v=JZLcTBaFebo</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><category>k8s</category><enclosure url="https://www.youtube.com/v/JZLcTBaFebo?version=3" length="" type=""/><pubDate>Thu, 2 Oct 2025 00:46:16 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

I Broke My Cluster with Kyverno (So You Don’t Have To) -- Vinod Kumar, Principal Cloud Engineer at Arcesium India
Working with Kyverno is powerful but with great policy comes great responsibility! In this light-hearted, fast-paced talk, I will share how I accidentally broke my Kubernetes cluster (more than once!) using Kyverno policies that were too strict, too recursive, or just misunderstood. This talk will walk through: (1) The top 3 mistakes I made (e.g., blocking all workloads, infinite policy loops, and bad patching) (2) How to debug and recover from Kyverno policy mishaps (3) Pro tips to write safe, testable, and non-destructive policies Whether you are just starting out or already using Kyverno in production, you will walk away with laughs, learnings, and policy patterns that won’t bring your cluster to its knees.]]></content:encoded></item><item><title>Scaling Developer Self-Service: Kyverno Policies for Inclusive Platform Engineering- Divya Chaudhary</title><link>https://www.youtube.com/watch?v=6YIL-gBjTKg</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><category>k8s</category><enclosure url="https://www.youtube.com/v/6YIL-gBjTKg?version=3" length="" type=""/><pubDate>Thu, 2 Oct 2025 00:46:16 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon North America in Atlanta (November 10-13). Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io

Scaling Developer Self-Service: Kyverno Policies for Inclusive Platform Engineering -- Divya Chaudhary, Software Development Manager at Amazon
Digital transformation demands scalable platforms that serve diverse developer populations effectively. This presentation explores how Kyverno policies can create inclusive self-service experiences that parallel successful financial inclusion models—reducing barriers, enabling microservices at scale, and formalizing shadow IT practices. We'll demonstrate how thoughtfully designed Kyverno policies dramatically reduce operational friction for developers, similar to how digital payment systems transformed financial access. Through practical examples, attendees will learn to implement policies that provide guardrails without gatekeeping, enabling teams with varying expertise levels to deploy safely. The session covers using Kyverno's mutation capabilities to automatically inject best practices, reducing the knowledge barrier for junior developers while maintaining platform standards. Critical implementation strategies include creating proportional policies that scale requirements with risk levels, implementing multi-channel policy education through generate policies and developer portals, and leveraging Kyverno's reporting features to identify and address adoption barriers. We'll showcase real-world patterns where policy-as-code reduced deployment failures by standardizing configurations while preserving developer flexibility. The presentation concludes with metrics-driven approaches to measure policy effectiveness in expanding platform access, ensuring your Kubernetes environment serves as a catalyst for innovation rather than a barrier to entry.]]></content:encoded></item><item><title>ChatLoopBackOff: Episode 69 (Logging-Operator)</title><link>https://www.youtube.com/watch?v=H2MOv1uwrgI</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><category>k8s</category><enclosure url="https://www.youtube.com/v/H2MOv1uwrgI?version=3" length="" type=""/><pubDate>Wed, 1 Oct 2025 22:21:55 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[In this episode of ChatLoopBackOff, CNCF Ambassador Paige Cruz takes her first look at Logging Operator, a CNCF Sandbox project built to simplify and standardize log management on Kubernetes. Designed to configure and control log collectors like Fluentd and Fluent Bit, Logging Operator enables teams to define log flows as Kubernetes resources—bringing observability closer to cluster-native workflows. Paige will explore how the operator manages pipelines, and what resources are available to contributors and adopters. Where do you think this project is headed in the CNCF landscape? If you’re interested in logging pipelines, Kubernetes-native automation, or evaluating emerging CNCF projects, this first-time exploration is for you.]]></content:encoded></item><item><title>ChatLoopBackOff: Episode 70 (KCP)</title><link>https://www.youtube.com/watch?v=uWM4w_qwtmk</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><category>k8s</category><enclosure url="https://www.youtube.com/v/uWM4w_qwtmk?version=3" length="" type=""/><pubDate>Wed, 1 Oct 2025 22:17:38 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[In this episode of ChatLoopBackOff, CNCF Ambassador Henrik Rexed explores KCP, a CNCF Sandbox project that rethinks multi-cluster Kubernetes management. KCP provides a lightweight control plane designed for workloads and APIs that span beyond a single cluster. This maes it easier to build, test, and operate software across diverse environments. Henrik will look into how KCP models APIs, its approach to workload placement and tenancy, and what resources exist for contributors and adopters. If you’re curious about scaling across teams and clusters, or want to understand the building blocks of this emerging project, join us for a first look at KCP in action.]]></content:encoded></item><item><title>CNL: kcp – Kubernetes-Like Control Planes for Declarative APIs</title><link>https://www.youtube.com/watch?v=OVjTKxPc92Y</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><category>k8s</category><enclosure url="https://www.youtube.com/v/OVjTKxPc92Y?version=3" length="" type=""/><pubDate>Wed, 1 Oct 2025 21:34:33 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[In the Cloud Native world declarative APIs are ubiquitous, enshrined in the Kubernetes Resource Model (KRM). Kubernetes Operators are built around this concept and Platform Engineering, an emerging discipline in the ecosystem, often centers around it as well.

This continued success of the KRM raises a question: Why not use the Kubernetes API without the intention to orchestrate containers? kcp, a CNCF Sandbox project, adds stronger multi-tenancy and additional API management capabilities on top of the Kubernetes API server code. It supercharges the KRM to be used as a generic control plane for any kind of declarative APIs.

Our webinar on kcp explores the fundamental concepts of kcp, its usage of the KRM and how to publish and reconcile Kubernetes-like APIs to a multitude of users. Join us on this webinar and gain insights into unlocking all new possibilities with kcp!]]></content:encoded></item><item><title>OpenAI’s new slop machine is open for business…</title><link>https://www.youtube.com/watch?v=hkSj-QapfZo</link><author>Fireship</author><category>dev</category><enclosure url="https://www.youtube.com/v/hkSj-QapfZo?version=3" length="" type=""/><pubDate>Wed, 1 Oct 2025 18:38:26 +0000</pubDate><source url="https://www.youtube.com/channel/UCsBjURrPoezykLs9EqgamOA">Dev - Fireship</source><content:encoded><![CDATA[Try out Blitzy’s codegen platform for enterprise codebases for free - https://blitzy.com

OpenAI's new Sora 2 model is the most accurate, realistic and controllable than anything we've seen before.

In today's video, we'll take a look at what Sora 2 means video content creators and whether they're about to go the way of the dodo. 

#Coding #programming #tech 

💬 Chat with Me on Discord

https://discord.gg/fireship

🔗 Resources
-   https://openai.com/index/sora-2/

🔥 Get More Content - Upgrade to PRO

Upgrade at https://fireship.io/pro
Use code YT25 for 25% off PRO access 

🎨 My Editor Settings

- Atom One Dark 
- vscode-icons
- Fira Code Font

🔖 Topics Covered
- OpenAI Sora 2
- What can Sora 2 do?
- The future of generative video]]></content:encoded></item><item><title>Jane Goodall has died</title><link>https://www.latimes.com/obituaries/story/2025-10-01/jane-goodall-chimpanzees-dead</link><author>jaredwiener</author><category>dev</category><category>hn</category><pubDate>Wed, 1 Oct 2025 18:10:39 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Jane Goodall, the trailblazing naturalist whose intimate observations of chimpanzees in the African wild produced powerful insights that transformed basic conceptions of humankind, has died. She was 91.A tireless advocate of preserving chimpanzees’ natural habitat, Goodall died on Wednesday morning in California of natural causes, the Jane Goodall Institute announced on its Instagram page. “Dr. Goodall’s discoveries as an ethologist revolutionized science,” the Jane Goodall Institute said in a statement. A protege of anthropologist Louis S.B. Leakey, Goodall made history in 1960 when she discovered that chimpanzees, humankind’s closest living relatives, made and used tools, characteristics that scientists had long thought were exclusive to humans.She also found that chimps hunted prey, ate meat, and were capable of a range of emotions and behaviors similar to those of humans, including filial love, grief and violence bordering on warfare.In the course of establishing one of the world’s longest-running studies of wild animal behavior at what is now Tanzania’s Gombe Stream National Park, she gave her chimp subjects names instead of numbers, a practice that raised eyebrows in the male-dominated field of primate studies in the 1960s. But within a decade, the trim British scientist with the tidy ponytail was a National Geographic heroine, whose books and films educated a worldwide audience with stories of the apes she called David Graybeard, Mr. McGregor, Gilka and Flo.“When we read about a woman who gives funny names to chimpanzees and then follows them into the bush, meticulously recording their every grunt and groom, we are reluctant to admit such activity into the big leagues,” the late biologist Stephen Jay Gould wrote of the scientific world’s initial reaction to Goodall.But Goodall overcame her critics and produced work that Gould later characterized as “one of the Western world’s great scientific achievements.”Tenacious and keenly observant, Goodall paved the way for other women in primatology, including the late gorilla researcher Dian Fossey and orangutan expert Birutė Galdikas. She was honored in 1995 with the National Geographic Society’s Hubbard Medal, which then had been bestowed only 31 times in the previous 90 years to such eminent figures as North Pole explorer Robert E. Peary and aviator Charles Lindbergh.In her 80s she continued to travel 300 days a year to speak to schoolchildren and others about the need to fight deforestation, preserve chimpanzees’ natural habitat and promote sustainable development in Africa. She was in California as part of her speaking tour in the U.S. at the time of her death.Goodall was born April 3, 1934, in London and grew up in the English coastal town of Bournemouth. The daughter of a businessman and a writer who separated when she was a child and later divorced, she was raised in a matriarchal household that included her maternal grandmother, her mother, Vanne, some aunts and her sister, Judy.She demonstrated an affinity for nature from a young age, filling her bedroom with worms and sea snails that she rushed back to their natural homes after her mother told her they would otherwise die.When she was about 5, she disappeared for hours to a dark henhouse to see how chickens laid eggs, so absorbed that she was oblivious to her family’s frantic search for her. She did not abandon her study until she observed the wondrous event.“Suddenly with a plop, the egg landed on the straw. With clucks of pleasure the hen shook her feathers, nudged the egg with her beak, and left,” Goodall wrote almost 60 years later. “It is quite extraordinary how clearly I remember that whole sequence of events.”When finally she ran out of the henhouse with the exciting news, her mother did not scold her but patiently listened to her daughter’s account of her first scientific observation.Later, she gave Goodall books about animals and adventure — especially the Doctor Dolittle tales and Tarzan. Her daughter became so enchanted with Tarzan’s world that she insisted on doing her homework in a tree.“I was madly in love with the Lord of the Jungle, terribly jealous of his Jane,” Goodall wrote in her 1999 memoir, “Reason for Hope: A Spiritual Journey.” “It was daydreaming about life in the forest with Tarzan that led to my determination to go to Africa, to live with animals and write books about them.”Her opportunity came after she finished high school. A week before Christmas in 1956 she was invited to visit an old school chum’s family farm in Kenya. Goodall saved her earnings from a waitress job until she had enough for a round-trip ticket.She arrived in Kenya in 1957, thrilled to be living in the Africa she had “always felt stirring in my blood.” At a dinner party in Nairobi shortly after her arrival, someone told her that if she was interested in animals, she should meet Leakey, already famous for his discoveries in East Africa of man’s fossil ancestors.She went to see him at what’s now the National Museum of Kenya, where he was curator. He hired her as a secretary and soon had her helping him and his wife, Mary, dig for fossils at Olduvai Gorge, a famous site in the Serengeti Plains in what is now northern Tanzania.Leakey spoke to her of his desire to learn more about all the great apes. He said he had heard of a community of chimpanzees on the rugged eastern shore of Lake Tanganyika where an intrepid researcher might make valuable discoveries.When Goodall told him this was exactly the kind of work she dreamed of doing, Leakey agreed to send her there.It took Leakey two years to find funding, which gave Goodall time to study primate behavior and anatomy in London. She finally landed in Gombe in the summer of 1960.On a rocky outcropping she called the Peak, Goodall made her first important observation. Scientists had thought chimps were docile vegetarians, but on this day about three months after her arrival, Goodall spied a group of the apes feasting on something pink. It turned out to be a baby bush pig.Two weeks later, she made an even more exciting discovery — the one that would establish her reputation. She had begun to recognize individual chimps, and on a rainy October day in 1960, she spotted the one with white hair on his chin. He was sitting beside a mound of red earth, carefully pushing a blade of grass into a hole, then withdrawing it and poking it into his mouth.When he finally ambled off, Goodall hurried over for a closer look. She picked up the abandoned grass stalk, stuck it into the same hole and pulled it out to find it covered with termites. The chimp she later named David Graybeard had been using the stalk to fish for the bugs.“It was hard for me to believe what I had seen,” Goodall later wrote. “It had long been thought that we were the only creatures on earth that used and made tools. ‘Man the Toolmaker’ is how we were defined ...” What Goodall saw challenged man’s uniqueness.When she sent her report to Leakey, he responded: “We must now redefine man, redefine tool, or accept chimpanzees as human!”Goodall’s startling finding, published in Nature in 1964, enabled Leakey to line up funding to extend her stay at Gombe. It also eased Goodall’s admission to Cambridge University to study ethology. In 1965, she became the eighth person in Cambridge history to earn a doctorate without first having a bachelor’s degree.In the meantime, she had met and in 1964 married Hugo Van Lawick, a gifted filmmaker who had traveled to Gombe to make a documentary about her chimp project. They had a child, Hugo Eric Louis — later nicknamed Grub — in 1967.Goodall later said that raising Grub, who lived at Gombe until he was 9, gave her insights into the behavior of chimp mothers. Conversely, she had “no doubt that my observation of the chimpanzees helped me to be a better mother.”She and Van Lawick were married for 10 years, divorcing in 1974. The following year she married Derek Bryceson, director of Tanzania National Parks. He died of colon cancer four years later.Within a year of arriving at Gombe, Goodall had chimps literally eating out of her hands. Toward the end of her second year there, David Graybeard, who had shown the least fear of her, was the first to allow her physical contact. She touched him lightly and he permitted her to groom him for a full minute before gently pushing her hand away. For an adult male chimpanzee who had grown up in the wild to tolerate physical contact with a human was, she wrote in her 1971 book “In the Shadow of Man,” “a Christmas gift to treasure.”Her studies yielded a trove of other observations on behaviors, including etiquette (such as soliciting a pat on the rump to indicate submission) and the sex lives of chimps. She collected some of the most fascinating information on the latter by watching Flo, an older female with a bulbous nose and an amazing retinue of suitors who was bearing children well into her 40s.Her reports initially caused much skepticism in the scientific community. “I was not taken very seriously by many of the scientists. I was known as a [National] Geographic cover girl,” she recalled in a CBS interview in 2012.Her unorthodox personalizing of the chimps was particularly controversial. The editor of one of her first published papers insisted on crossing out all references to the creatures as “he” or “she” in favor of “it.” Goodall eventually prevailed.Her most disturbing studies came in the mid-1970s, when she and her team of field workers began to record a series of savage attacks.The incidents grew into what Goodall called the four-year war, a period of brutality carried out by a band of male chimpanzees from a region known as the Kasakela Valley. The marauders beat and slashed to death all the males in a neighboring colony and subjugated the breeding females, essentially annihilating an entire community.It was the first time a scientist had witnessed organized aggression by one group of non-human primates against another. Goodall said this “nightmare time” forever changed her view of ape nature.“During the first 10 years of the study I had believed ... that the Gombe chimpanzees were, for the most part, rather nicer than human beings,” she wrote in “Reason for Hope: A Spiritual Journey,” a 1999 book co-authored with Phillip Berman. “Then suddenly we found that the chimpanzees could be brutal — that they, like us, had a dark side to their nature.”Critics tried to dismiss the evidence as merely anecdotal. Others thought she was wrong to publicize the violence, fearing that irresponsible scientists would use the information to “prove” that the tendency to war is innate in humans, a legacy from their ape ancestors. Goodall persisted in talking about the attacks, maintaining that her purpose was not to support or debunk theories about human aggression but to “understand a little better” the nature of chimpanzee aggression.“My question was: How far along our human path, which has led to hatred and evil and full-scale war, have chimpanzees traveled?”Her observations of chimp violence marked a turning point for primate researchers, who had considered it taboo to talk about chimpanzee behavior in human terms. But by the 1980s, much chimp behavior was being interpreted in ways that would have been labeled anthropomorphism — ascribing human traits to non-human entities — decades earlier. Goodall, in removing the barriers, raised primatology to new heights, opening the way for research on subjects ranging from political coalitions among baboons to the use of deception by an array of primates.Her concern about protecting chimpanzees in the wild and in captivity led her in 1977 to found the Jane Goodall Institute to advocate for great apes and support research and public education. She also established Roots and Shoots, a program aimed at youths in 130 countries, and TACARE, which involves African villagers in sustainable development.She became an international ambassador for chimps and conservation in 1986 when she saw a film about the mistreatment of laboratory chimps. The secretly taped footage “was like looking into the Holocaust,” she told interviewer Cathleen Rountree in 1998. From that moment, she became a globe-trotting crusader for animal rights. In the 2017 documentary “Jane,” the producer  pored through 140 hours of footage of Goodall that had been hidden away in the National Geographic archives. The film won a Los Angeles Film Critics Assn. Award, one of many honors it received.In a ranging 2009 interview with Times columnist Patt Morrison, Goodall mused on topics from traditional zoos — she said most captive environments should be abolished — to climate change, a battle she feared humankind was quickly losing, if not lost already. She also spoke about the power of what one human can accomplish.“I always say, ‘If you would spend just a little bit of time learning about the consequences of the choices you make each day’ — what you buy, what you eat, what you wear, how you interact with people and animals — and start consciously making choices, that would be beneficial rather than harmful.”As the years  passed, Goodall continued to track Gombe’s chimps, accumulating enough information to draw the arcs of their lives — from birth through sometimes troubled adolescence, maturity, illness and finally death.She wrote movingly about how she followed Mr. McGregor, an older, somewhat curmudgeonly chimp, through his agonizing death from polio, and how the orphan Gilka survived to lonely adulthood only to have her babies snatched from her by a pair of cannibalistic female chimps.Her reaction in 1972 to the death of Flo, a prolific female known as Gombe’s most devoted mother, suggested the depth of feeling that Goodall had for the animals. Knowing that Flo’s faithful son Flint was nearby and grieving, Goodall watched over the body all night to keep marauding bush pigs from violating her remains.“People say to me, thank you for giving them characters and personalities,” Goodall once told CBS’s “60 Minutes.” “I said I didn’t give them anything. I merely translated them for people.”Woo is a former Times staff writer.]]></content:encoded></item><item><title>Don&apos;t avoid workplace politics</title><link>https://terriblesoftware.org/2025/10/01/stop-avoiding-politics/</link><author>matheusml</author><category>dev</category><category>hn</category><pubDate>Wed, 1 Oct 2025 17:36:43 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Say the word “politics” to most engineers and watch their face scrunch up like they just bit into a lemon. We’ve all been conditioned to believe that workplace politics is this dirty game played by manipulative ladder-climbers while the “real” engineers focus on the code.I used to think the same way. For years as an engineer, I wore my hatred of politics like a badge of honor. I was above all that nonsense. I just wanted to ship. Politics was for those other people, the ones who didn’t have what it takes technically.Now I think the opposite: politics isn’t the problem; bad politics is. And pretending politics doesn’t exist? That’s how bad politics wins.Politics is just how humans coordinate in groups. It’s the invisible network of relationships, influence, and informal power that exists in every organization. You can refuse to participate, but that doesn’t make it go away. It just means decisions get made without you.Think about the last time a terrible technical decision got pushed through at your company. Maybe it was adopting some overcomplicated architecture, or choosing a vendor that everyone knew was wrong, or killing a project that was actually working. I bet if you dig into what happened, you’ll find it wasn’t because the decision-makers were stupid. It’s because the people with the right information weren’t in the room. They “didn’t do politics.”Meanwhile, someone who understood how influence works was in that room, making their case, building coalitions, showing they’d done their homework. And their idea won. Not because it was better, but because they showed up to play while everyone else was “too pure” for politics.Ideas don’t speak. People do. And the people who understand how to navigate organizational dynamics, build relationships, and yes, play politics? Their ideas get heard.When you build strong relationships across teams, understand what motivates different stakeholders, and know how to build consensus, you’re doing politics. When you take time to explain your technical decisions to non-technical stakeholders in language they understand, that’s politics. When you grab coffee with someone from another team to understand their challenges, that’s politics too.Good politics is just being strategic about relationships and influence in the service of good outcomes.The best technical leaders are incredibly political. They just don’t call it that. They call it “stakeholder management” or “building alignment” or “organizational awareness.” But it’s politics, and they’re good at it.The engineers who refuse to engage with politics often complain that their companies make bad technical decisions. But they’re not willing to do what it takes to influence those decisions. They want a world where technical merit alone determines outcomes. That world doesn’t exist and never has.This isn’t about becoming a scheming backstabber. As I wrote in Your Strengths Are Your Weaknesses, the same trait can be positive or negative depending on how you use it. Politics is the same way. You can use political skills to manipulate and self-promote, or you can use them to get good ideas implemented and protect your team from bad decisions.Here’s what good politics looks like in practice:Building relationships before you need them. That random coffee with someone from the data team? Six months later, they’re your biggest advocate for getting engineering resources for your data pipeline project.Understanding the real incentives. Your VP doesn’t care about your beautiful microservices architecture. They care about shipping features faster. Frame your technical proposals in terms of what they actually care about. Your manager is juggling competing priorities you don’t see. Keep them informed about what matters, flag problems early with potential solutions, and help them make good decisions. When they trust you to handle things, they’ll fight for you when it mattersCreating win-win situations. Instead of fighting for resources, find ways to help other teams while getting what you need. It doesn’t have to be a zero-sum game. If you do great work but nobody knows about it, did it really happen? Share your wins, present at all-hands, write those design docs that everyone will reference later.The alternative to good politics isn’t no politics. It’s bad politics winning by default. It’s the loud person who’s wrong getting their way because the quiet person who’s right won’t speak up. It’s good projects dying because nobody advocated for them. It’s talented people leaving because they couldn’t navigate the organizational dynamics.Stop pretending you’re above politics. You’re not. Nobody is. The only question is whether you’ll get good at it or keep losing to people who already are.]]></content:encoded></item><item><title>Gmail will no longer support checking emails from third-party accounts via POP</title><link>https://support.google.com/mail/answer/16604719?hl=en</link><author>sumanep</author><category>dev</category><category>hn</category><pubDate>Wed, 1 Oct 2025 16:25:58 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Starting January 2026, Gmail will no longer provide support for the following: This feature allows you to get special features like spam protection or inbox organization applied to your third-party email account. Learn more about Gmailify. Unlike IMAP connections, with POP, emails are downloaded, and you decide how often you want to download new emails. As an alternative, you can still link your third-party accounts in the Gmail app.These changes help provide the most secure and current options to access your messages in Gmail.Learn about changes to GmailifyYou won’t be able to get specific features in Gmail applied to your third-party account, like:Learn about changes to POP connectionsGmail will no longer support checking emails from third-party accounts through POP.The option to "Check mail from other accounts" will no longer be available in Gmail on your computer.To continue to receive messages from your other account in Gmail, you need to set up IMAP access.
    Check your email provider’s documentation for instructions on how to enable IMAP for your account.Frequently asked questionsNo. All messages synced before the deprecation stay in Gmail.Yes. For third-party accounts like Yahoo! and Outlook, you can add them to the Gmail mobile app on Android and iPhone and iPad.]]></content:encoded></item><item><title>Show HN: Autism Simulator</title><link>https://autism-simulator.vercel.app/</link><author>joshcsimmons</author><category>dev</category><category>hn</category><pubDate>Wed, 1 Oct 2025 14:48:31 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: ChartDB Agent – Cursor for DB schema design</title><link>https://app.chartdb.io/ai</link><author>guyb3</author><category>dev</category><category>hn</category><pubDate>Wed, 1 Oct 2025 13:38:36 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item></channel></rss>