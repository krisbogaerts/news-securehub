{"id":"6C2W1azD1rBsV4qJANNfruBrvGgdqLWrPGWtk6Mp1WgfCNcfvBZHFQw","title":"The System Design Newsletter","displayTitle":"Dev - System Design Newsletter","url":"https://newsletter.systemdesign.one/feed","feedLink":"https://newsletter.systemdesign.one/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":7,"items":[{"title":"API Versioning - A Deep Dive","url":"https://newsletter.systemdesign.one/p/api-versioning","date":1759232597,"author":"Neo Kim","guid":83,"unread":true,"content":"<p>Get my system design playbook for FREE on newsletter signup:</p><p><em>This post outlines API versioning strategies.</em></p><p>Building application programming interfaces () is easy.</p><p>But keeping them stable and predictable as the system evolves is difficult.</p><p>API consumers depend on API contracts, and even a tiny change can break many integrations. For example, renaming a field, changing a response format, or changing an endpoint could stop someone’s production system from working correctly.</p><p>Business requirements change, so it’s necessary to communicate the API adjustments clearly.</p><p>That’s where  comes in. It provides a structured approach for evolving APIs without leaving the API consumers behind.</p><p>She’s a software architect, Microsoft MVP, with a passion for distributed systems, .NET, and system design.</p><p>Check out her blog and social media:</p><p>You’ll find her speaking at developer conferences around the world and blogging online.</p><p>API Versioning sets guidelines for introducing changes, shows how extensive those changes are, and ensures consumers can transition to new versions at their own pace.</p><p>But here’s the catch: just adding a version number in the URL doesn’t automatically prevent all the problems.</p><p>I’ve seen many APIs across different domains that expose a in their URLs.</p><p>They add new data, change property names, change their types, remove fields, and so on. But years later, they run the same version even though the API has changed a lot.</p><p>And many APIs end up  in the URL forever because the same organization controls the consumer apps as well.</p><p>So why add a version at all?</p><p>When different teams at the same company are the API owners and API consumers, version upgrades are “just” code changes. Teams can synchronize deployments, push breaking changes, and update consumers with little risk of breaking changes.</p><p>However, when you expose the API publicly and don’t own the consumers, the scenario changes completely. It means you cannot simply break compatibility.</p><ul><li><p>Service Level Agreements ()</p></li></ul><p>…all of which force you to design carefully and communicate clearly.</p><p>It’s better to avoid spending nights answering support calls because someone’s app broke when you changed an API endpoint.</p><p>With <a href=\"https://getstream.io/?utm_source=newsletter&amp;utm_medium=referral&amp;utm_content=&amp;utm_campaign=neokim\">Stream’s</a> SDKs for React, React Native, Flutter, iOS, Android, and more, you can add production-ready , , , and  to your app—with just a few lines of code. Backed by a global edge network and a generous free Maker plan, start prototyping today and scale to millions.</p><p>It’s helpful to think of APIs as contracts.</p><p>The contract describes the shape of the data, the form of the endpoints, and the behavior of the API. The client and the provider both agree to this contract.</p><p>However, the owner adds a new field when business requirements change or when they fix a bug. It means that API contracts change.</p><p>And versioning is how we say:</p><ul><li><p>Here’s the old contract, still valid.</p></li><li><p>Here’s the new contract, with improvements.</p></li></ul><p>There are three paths forward as the API evolves:</p><h4>1. Release a new version in a new location</h4><p>This means creating  while keeping  around.</p><p>The users can use the old version as it is. And switch only when they want the new features. This approach is safe and straightforward for clients, but it creates a lot of work for the API owner.</p><p>The API owner has to maintain several versions at the same time. For example, fix bugs, apply security updates, or implement features on all supported versions.</p><p>Besides, this approach might become expensive and slow down future development.</p><h4>2. Release a backward-compatible version</h4><p>In this approach, the API changes in a way that doesn’t break existing clients.</p><p>This means , such as including new fields, optional parameters, or endpoints in a response. As everything works the same way as earlier, consumers don’t have to update anything.</p><p>This strategy allows gradual growth but might be limiting when large design shifts are necessary.</p><p>In this scenario, the API changes require .</p><p>Although it sounds like the worst option, sometimes it’s unavoidable. For example, there might be a need for a different data model, or a serious security flaw needs to be addressed. Or the original API design had issues they couldn't fix easily.</p><p>In reality, <strong>we need a mix of all three</strong>, based on the change type and its impact.</p><p>But before we go further, let’s understand these technical terms first:</p><p>It means additive changes to an API without incrementing its version.</p><p>You add new fields, parameters, or endpoints, but never remove or rename anything. It’s easy for consumers because they don’t have to change their integrations.</p><p>It means acknowledging when a change breaks compatibility.</p><p>You create a new version using a URL, header, or parameter. Thus making it possible to see old behavior in one version and new behavior in another.</p><p>I’ll walk you through different approaches to explicit versioning of an API, their benefits, and their drawbacks next.</p><p>This is one of the most common approaches.</p><pre><code>https://coolapi.com/api//orders</code></pre><p>It solves the problem of clarity and visibility because the consumer can easily understand which version they’re using by simply looking at the path.</p><p>Thus making it easier for new consumers to follow documentation and configure routing logic.</p><p>However, this clarity comes at the cost of stability. Here are some of its downsides:</p><ul><li><p>Maintaining many versions creates redundancy and overhead for the API owner.</p></li><li><p>It pollutes URLs with version information.</p></li></ul><p>Although this approach looks simple to implement, here’s the challenge:</p><p>Roy Fielding, the creator of REST, said that it represents evolvability. That means the ability to change how a resource looks or behaves without changing its identity. </p><p>And Tim Berners-Lee said, “<a href=\"https://www.w3.org/Provider/Style/URI\">Cool URIs don’t change</a>”. It means once you publish a URL, don’t change it. While people and systems should be able to rely on it forever.</p><p>If we link those together, versioning the URL path might become unnecessary.</p><p>So let’s just take a step back for a moment.</p><ul><li><p>A REST API represents resources.</p></li><li><p>If  points to an order, it should always point to that order, even if the response structure changes.</p></li><li><p>But adding  or  in the path () breaks the principle, because it treats the same thing (an order) as if it had two unique identities.</p></li><li><p>The resource (an order) hasn’t changed into something else, but only its representation has changed.</p></li></ul><p>Here's how to best use path-based versioning:</p><ul><li><p>Keep older versions alive for a transition period.</p></li><li><p>Announce clear deprecation timelines and provide migration guides.</p></li><li><p>Use shared libraries for common logic so that bug fixes don’t need to be copied everywhere.</p></li></ul><p>Besides, HTTP itself provides mechanisms for communicating versioning and deprecation.</p><p>Here are two of the most useful techniques:</p><pre><code>Sunset: Wed, 01 Jul 2026 00:00:00 GMT</code></pre><p>It tells the consumers that the current version is scheduled to retire on 01 July 2026. And combining this with headers like , the client gets enough time to plan the migration.</p><p>Redirects are another simple way to guide clients from one version to another.</p><pre><code>301 Moved Permanently\n\nLocation: /api/v2/orders</code></pre><p>This allows you to respond to a request with a 301 status code and a Location header, which specifies where the resource can be found.</p><p>Thus helping the consumers who don’t update their code right away by guiding them to the newest version.</p><p>But redirects work well when endpoints have changed only a little; otherwise, clients will need code changes.</p><h3>2. Query Parameter Versioning</h3><p>This approach keeps the base URL stable and includes the version as a URL parameter.</p><pre><code>https://coolapi.com/api/orders?</code></pre><p>It solves the <strong>flexibility problem at the resource level</strong>.</p><p>Here are some of its advantages:</p><ul><li><p>The API owner can version endpoints or even individual resources without changing the overall API structure.</p></li><li><p>The consumer doesn’t have to update the base URLs, and different versions can coexist on the same path.</p></li></ul><p>But this approach has its downsides. Here are some of them:</p><ul><li><p>Many parameters can make it messy.</p></li><li><p>Routers rely on paths, not query strings; so you’ll need custom logic in the gateway or application to handle different versions.</p></li><li><p>Documentation has to be precise; otherwise, consumers may forget to include the query parameter and end up confused.</p></li><li><p>Caching becomes difficult because of cache pollution, misconfiguration, or wrong cache hits.</p></li><li><p>There’s a risk of consumers receiving the wrong version, as intermediate servers might ignore query parameters.</p></li></ul><p>Here are some best practices for query parameter versioning:</p><ul><li><p>Configure your CDN or cache to always include the query parameters in the cache key.</p></li><li><p>Document the parameters in each example call and check their presence on the server side to avoid silent issues.</p></li><li><p>Normalize query strings where possible to avoid duplicate cache entries. For example, treat ` and as the same.</p></li></ul><p>Although this approach gives you resource level control, it might complicate your codebase and caching strategy.</p><h3>3. Message Payload Versioning</h3><p>This technique includes the version as part of the request and response body itself.</p><pre><code>{\n  ,\n  “data”: {\n    “id”: 123,\n    “status”: “shipped”\n  }\n}</code></pre><p>Payload versioning enables the management of <strong>long-lived data in asynchronous systems</strong>. It means the system stores the payload with version information for later replay in event-driven or queue-based architectures.</p><ul><li><p>You publish an event  to a queue.</p></li><li><p>A service reads it right away or maybe 1 year later.</p></li><li><p>Because the message includes the version, the service quickly understands which format (schema) to use for deserialization.</p></li></ul><p>However, this approach mixes concerns because versioning is part of business data.</p><p>Besides, the consumer has to understand and process versions (schemas) correctly. This adds extra complexity for REST APIs with short request/response lifecycles.</p><p>Here’s how to best use message payload versioning:</p><ul><li><p>Use it only for asynchronous or event-driven systems.</p></li><li><p>Treat messages as fixed facts and create schema registries. This helps consumers safely de-serialize old formats.</p></li></ul><p>Only use payload versioning if you store and reuse messages later (like in queues or logs).</p><p>For standard REST APIs, where responses are used immediately and then discarded, it just makes things harder.</p><h3>4. Header-Based Versioning</h3><p>This approach passes the version information in the request headers.</p><pre><code>GET /api/orders\n\n</code></pre><p>Another way is to version APIs through the  header.</p><p>The client specifies the resource version by requesting a specific media type. This is known as </p><pre><code>Accept: application/vnd.example+json;api-version=2\n\nAccept: application/vnd.github.v2+json</code></pre><p>The API will then respond with the resource formatted for version 2 of the contract.</p><p>Here are some advantages of header-based versioning:</p><ul><li><p>Allows clients to upgrade to new API versions at their own pace.</p></li><li><p>Keeps URIs stable and self-describing.</p></li><li><p>Aligns with HTTP semantics as metadata belongs in headers.</p></li></ul><p>However, this approach has its downsides. Here are some of them:</p><ul><li><p>Harder to debug, log, and cache as version information isn’t visible in the URL.</p></li><li><p>Some proxies or middleware might strip or block custom headers if not set up properly.</p></li></ul><p>Besides, caching gets tricky when the version is in the  header.</p><p>CDNs, reverse proxies, and browsers might not know that different headers mean different versions. To solve this, you add a <a href=\"https://datatracker.ietf.org/doc/html/rfc7231\">header</a>. It tells the cache to keep separate copies for each version and not to mix them up.</p><p>Header-based versioning may look hard at first, but it’s the cleanest long-term choice.</p><p>Your URLs stay stable, while versions are handled in headers. This keeps APIs consistent, avoids clutter, and follows REST principles. You need extra setup for caching, but the payoff is a smaller, easy-to-maintain API with less technical debt.</p><p>Now that we’ve covered versioning types, let’s explore versioning formats.</p><p>A versioning format is the way you label API versions to show what changed or when it changed.</p><p>Semantic Versioning () uses the format . This helps to describe the changes included in a release.</p><ul><li><p>MAJOR: Breaking changes (clients must actively migrate).</p></li><li><p>MINOR: Backward-compatible changes (new fields, endpoints).</p></li></ul><p>For example,  indicates the second major release, fourth minor, and first patch.</p><p>This format gives consumers expectations:</p><ul><li><p>But 2.4 → 2.5 should be safe.</p></li></ul><p>Calendar versioning ()uses dates instead of numbers.</p><ul><li><p>Ubuntu 24.04: released in April 2024 (YY.MM). </p></li><li><p>Python 3.12.20231001: released on 1 October 2023 (YYYYMMDD).</p></li></ul><p>This format tells you the release date, which makes it easier to track freshness rather than compatibility.</p><p>Hash versioning () uses hashes (like Git commit IDs) instead of numbers or dates. It points to the exact state of the code at a specific time.</p><ul><li><p>v-237a2b4f: shortened Git commit hash of the build.</p></li></ul><p>This format is precise for traceability, but harder for humans to understand changes.</p><p>It’s one thing to talk theory, but let’s look at how companies are designing their APIs: </p><ul><li><p>New accounts are automatically “pinned” to the latest version.</p></li><li><p>Versions are in (<a href=\"https://stripe.com/blog/api-versioning\">CalVer</a>) date-based format: <code>Stripe-Version: 2023-10-16</code>.</p></li><li><p>Clients can override the version on a per-request basis to test or migrate.</p></li><li><p>URLs stay clean as the versioning information lives in headers.</p></li></ul><p>Stripe’s approach makes upgrades safe and client-driven. This aligns well with the principle: <em>don’t break your consumers without their consent</em>.</p><ul><li><p>Also uses  with date identifiers (<code>X-GitHub-Api-Version: 2022-11-28</code>).</p></li><li><p>The base URL never changes: https://api.github.com</p></li><li><p>They support each version for , giving consumers time to migrate.</p></li><li><p>Additive changes roll out to all versions; breaking changes trigger a new version.</p></li><li><p>Keeps URIs stable, avoids version churn, and treats versions as snapshots in time.</p></li><li><p>If the header is missing, the API defaults to the latest version.</p></li></ul><p>GitHub’s approach offers stable URLs and predictable migrations while reducing unnecessary versions. It highlights another philosophy: <em>API as incremental snapshots in time</em>.</p><h2>When to Increase the Version?</h2><p>There’s no fixed rule. Raise a new version only when the API contract changes meaningfully.</p><ul><li><p>How many consumers depend on this?</p></li><li><p>Will the change break them?</p></li><li><p>Do SLAs or legal obligations apply?</p></li><li><p>Can you maintain many versions?</p></li></ul><p>Be pragmatic; don’t version every deployment. Do it only when the change affects consumers.</p><h2>Tooling for API Versioning</h2><p>Versioning isn’t just about design. It also needs tools for routing, documentation, and consistency.</p><p>An API gateway manages traffic and routes requests to the correct services. It makes it easy to run different API versions in parallel.</p><h3>2. Documentation Frameworks</h3><p>Documentation shows consumers which version they’re using and what changed.</p><p>A documentation framework is a tool that helps you write and publish clear API docs so consumers know how to use different versions.</p><ul><li><p>You can publish documentation for both old and new API versions, and tools like Swagger will simply show what changed between them.</p></li></ul><p>Libraries reduce boilerplate and enforce consistent versioning across endpoints. </p><ul><li><p>It lets you define versions in routes or headers, and send requests to the right version of your API automatically.</p></li></ul><p>These tools don’t replace a strategy, but make it easy to manage versions at scale.</p><h2>When to Use Which Strategy</h2><p>There’s no single “best” way to version APIs. Each strategy solves a unique problem.</p><p>The right choice depends on your context:</p><ul><li><p>Path-based versioning: internal systems where you control all consumers.</p></li><li><p>Query parameter versioning: resource-level flexibility, and can tolerate routing complexity.</p></li><li><p>Header-based versioning: safest for external or long-lived clients you don’t control.</p></li><li><p>Payload versioning: event-driven systems where messages are replayed later.</p></li></ul><p>And with version formats:</p><ul><li><p>SemVer: shows impact (breaking, additive, bug fix).</p></li><li><p>CalVer: shows freshness (release).</p></li><li><p>HashVer: shows traceability (commit or build).</p></li></ul><p>The additive strategy is good enough for most use cases.</p><p>So pick the strategy that causes the fewest surprises to consumers. Stability and clear migration paths build trust. That trust is more valuable than any technical detail.</p><p>👋 I’d like to thank <a href=\"https://www.linkedin.com/in/irinascurtu/\">Irina</a> for writing this newsletter!</p><p>It’ll help you become a better software engineer.</p><p>Subscribe to get simplified case studies delivered straight to your inbox:</p><p><strong>Want to advertise in this newsletter? </strong>📰</p><p>Thank you for supporting this newsletter.</p><p>You are now 170,001+ readers strong, very close to 172k. Let’s try to get 172k readers by 10 October. Consider sharing this post with your friends and get rewards.</p><ul><li><p>Block diagrams created with <a href=\"https://app.eraser.io/auth/sign-up?ref=neo\">Eraser</a></p></li></ul>","contentLength":16367,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/67c9275b-cdf0-476f-8e0a-41ccacc08602_1280x720.png","enclosureMime":"","commentsUrl":null},{"title":"How Kafka Works","url":"https://newsletter.systemdesign.one/p/how-kafka-works","date":1758798180,"author":"Neo Kim","guid":82,"unread":true,"content":"<p>Get my system design playbook for FREE on newsletter signup:</p><p><em>This post outlines the Kafka architecture.</em></p><p>Open any article about Kafka today, and you will see the same words being used to describe it:</p><blockquote><p><em>“It’s an open-source, distributed, durable, very scalable, fault-tolerant pub/sub message system with rich integration and stream processing capabilities.”</em></p></blockquote><p>While it’s true, it isn’t helpful for a novice reader being introduced to Kafka for the first time. Today, we will explain Kafka by  what every word in that definition means. Every definition will start with the 💡 symbol. At the end, you will have a complete high-level understanding of Apache Kafka.</p><p>He’s an Apache Kafka committer and software engineer who worked at Confluent (the Kafka company) during its hypergrowth period. </p><p>Check out his newsletter and social media:</p><p>Currently, he has taken a break from engineering to consult companies and write extensively about Apache Kafka and data engineering.</p><p>Apache Kafka is one of the most popular open-source projects in the data infrastructure space. It is a standard tool in every data engineer’s catalog, used by over 70% of Fortune 500 companies and 150,000 organizations.</p><p>Kafka is a messaging system that was originally developed by LinkedIn in 2010. In 2011, it was open-sourced and donated to the Apache Foundation.</p><p>Today, Confluent is a public company and widely known as the company behind Kafka. It has expanded its business to offer a more complete data streaming platform on top of Kafka.</p><p>Nowadays, Kafka is more than a simple messaging system: it’s a larger ecosystem of components that form a. It is frequently called the swiss army knife of data infrastructure.</p><p> is an AI code review tool that runs directly in your terminal. It provides intelligent code analysis, catches issues early, and integrates seamlessly with AI coding agents like Claude Code, Codex CLI, Cursor CLI, and Gemini to ensure your code is production-ready before it ships.</p><ul><li><p>Enables pre-commit reviews of both staged and unstaged changes, creating a multi-layered review process.</p></li><li><p>Fits into existing Git workflows. Review uncommitted changes, staged files, specific commits, or entire branches without disrupting your current development process.</p></li><li><p>Reviews specific files, directories, uncommitted changes, staged changes, or entire commits based on your needs.</p></li><li><p>Supports programming languages including JavaScript, TypeScript, Python, Java, C#, C++, Ruby, Rust, Go, PHP, and more.</p></li><li><p>Offers free AI code reviews with rate limits so developers can experience senior-level reviews at no cost.</p></li><li><p>Flags hallucinations, code smells, security issues, and performance problems.</p></li><li><p>Supports guidelines for other AI generators, AST Grep rules, and path-based instructions.</p></li></ul><p>Why did Kafka become so widely used?</p><p>It solved a very important problem - the problem of data integration at scale. LinkedIn had to connect different services to one another. A naive way of achieving this would be to create many custom point-to-point integrations (called ) between each service. That would have resulted in an  mess that would break often and be impossible to maintain when N is in the hundreds</p><p>Apache Kafka flips this problem on its head. Instead of creating custom pipelines per connection, it encourages organizations to:</p><ol><li><p>Store their data in a central location (Kafka)</p></li><li><p>Use a single standard API (the Kafka API)</p></li><li><p>Have applications subscribe and consume this data in real time</p></li></ol><p>This decouples writers from readers, as writers simply publish to Kafka, and readers subscribe to Kafka.</p><p>The data gets durably persisted to disk for a limited amount of time (e.g., 7 days). Kafka is ideal and was built with read-fanout use cases in mind, where the same message needs to get read by multiple systems. As such, it’s common for the system’s read throughput to be a multiple of its write throughput.</p><blockquote><p>💡 <em><strong>pub-sub messaging system (2/8)</strong></em> - this is what a pub/sub messaging system is</p></blockquote><p>With Kafka, organizations don’t need to maintain dozens of fragile custom point-to-point pipelines that break whenever a single VM restarts. The data can be written to Kafka once and read as many times as necessary by whatever system needs it.</p><blockquote><p>In this article, we won’t talk further about the use cases of Kafka. If you’re more interested in the reason behind Kafka, I recently covered in-depth why LinkedIn created Kafka. It made the front page of Hacker News.</p></blockquote><p>Okay, let’s dive into Kafka now! To truly understand the system, we need to start from the basics.</p><p>Let’s examine its core data structure:</p><p>It is append-only; you can only add records to the end of the log (no deletes or updates allowed). Reads go from left to right, in the order the records were added.</p><p>Each record in the log has a unique monotonically increasing number called an . The offset refers to the record and denotes its order.</p><p>The API of the log data structure is very simple:</p><p>Kafka keeps the log structure on disk. The log’s sequential operations work very well with HDDs. Hard drives offer very high throughput for sequential reads and writes. This differs from random reads and writes, where HDDs don’t perform well.</p><ul><li><p>: an entry in the log. I use these words interchangeably when describing data in Kafka.</p></li></ul><p>Each message is essentially a key-value pair; it consists of a `` and a `` (although other metadata like offset, timestamp, and custom headers exist too). The key is optional; it is valid for a message to only have a value.</p><p>The key thing to remember is that the key/value pairs are .</p><p>Kafka does not inherently support types (e.g., int64, string, etc.) nor schemas (specific message structures).</p><p>It is the client-side code’s responsibility to apply schemas:</p><ul><li><p>: producer clients convert (serialize) the objects into bytes.</p></li><li><p>: consumer clients parse (deserialize) the raw bytes from the network into the object.</p></li></ul><p>One log is not enough. You want to separate your data into categories. Just as in a database, you would create separate tables for user accounts and customer orders; in Kafka, you would create separate topics.</p><p>It’s common for a Kafka cluster to have hundreds to thousands of topics.</p><p>Kafka is a distributed system designed to scale much further than what a single machine can handle. As such, it uses .</p><p>A topic is sharded into one or more partitions.</p><p>Each partition is a separate instance of the log data structure.</p><p>While a topic can have just one partition, it’s very common for it to have dozens, since this helps with parallelization of reads (more on that later).</p><p>Kafka doesn’t use HTTP. It uses its own TCP-based protocol. This means that you need more custom code to send and receive requests; you can’t just use any HTTP library.</p><p>Kafka provides its own libraries that implement the underlying protocol. The main clients you’d care about are the  and the .</p><ul><li><p>Producer: the class that’s used for writing data to Kafka</p></li><li><p>Consumer: the class that’s used for reading data from Kafka</p></li></ul><p>The Apache Kafka project offers a Java library that implements these:</p><pre><code>import org.apache.kafka.clients.producer.KafkaProducer;\n\nimport org.apache.kafka.clients.consumer.KafkaConsumer;</code></pre><p>The Producer class allows you to  to a topic. You can explicitly choose the partition or allow Kafka to do it automatically for you.</p><p>The Consumer, on the other hand, lets you  to and the stream of messages coming in from a topic (or specific partition):</p><p>This is simply the most important API I can show you concisely; a lot more exist. Kafka may seem simple, but it has many details to learn to be effective with it.</p><h2>Kafka as a Distributed System</h2><p>Apache Kafka is designed to be a  system - one meant to scale horizontally by adding more nodes. As such, any normal deployment of Kafka consists of at least .</p><ul><li><p>: an instance of the Kafka server. This is what we call a node in the system.</p></li><li><p>: all the brokers in the system.</p></li></ul><p>Kafka has a ton of interesting performance optimizations (more on this in another article). Its greatest strength is its horizontal scalability.</p><p>The <a href=\"https://topicpartition.io/definitions/the-log\">Log data structure</a> is key to Kafka’s scalability - writes on it are O(1) and lock-free. This is because records are simply  and cannot be updated, nor individually deleted.</p><p>Messages within a partition are independent of each other. They have no higher-level guarantees like unique keys. This reduces the need for locking and allows Kafka to append to the Log structure as fast as the disk will allow.</p><p>Because each partition is a separate log, and you can add more brokers to the cluster, your scale is limited by how many brokers you can add.</p><p>Nothing theoretically stops you from having a Kafka cluster that accepts and then scaling it 2x to .</p><p>A partition in Kafka doesn’t live only on one broker - it is replicated across many.</p><p>A configurable setting (called ) denotes how many copies should exist. The default and most common is .</p><p>In other words, we have three copies (called ) of the Log data structure. These replicas live on the disks of different brokers.</p><p>Replication is done for many reasons, one of which is data durability: when three copies of the data exist, one disk failing won’t lead to data loss.</p><p>In modern cloud deployments, brokers are spread across different <a href=\"https://blog.2minutestreaming.com/p/basic-aws-networking-costs#:~:text=AZ%20(Availability%20Zone)%20%2D%20a%20physically%20isolated%20location%20with%20one%20or%20more%20data%20centers%2C%20inside%20a%20region\">availability zones</a>. This ensures very high durability and availability. Even in the unlikely event of a whole data center burning down, the Kafka cluster would still survive.</p><p>Once you start maintaining copies of data in a distributed system, you open yourself to a lot of edge cases. Keeping new data in sync is tricky. The copies must match, and the system needs to somehow agree on the latest state.</p><blockquote><p>There is a whole class of complex algorithms in computer science called <a href=\"https://sre.google/sre-book/managing-critical-state/\">distributed consensus</a>, which handle this.</p></blockquote><p>Kafka uses a straightforward single-leader replication model. At any time, one replica serves as the leader. The other two replicas act as followers (i.e., hot standbys).</p><p>Only the leader accepts new writes - it serves as the source of truth of the log. The followers actively replicate data from the leader. Reads can be served from both the leader and its followers.</p><p>When a broker node goes offline, the system notices it. Other brokers then take over the leadership of the partitions the dead broker led. This is how Kafka offers high availability.</p><p>In a distributed system, all nodes must agree on the latest state of the cluster. Brokers must coordinate on certain metadata changes, like electing a new leader. This is again a distributed consensus problem. Because it’s too complex for the purposes of introducing Kafka, we will simply gloss over how Kafka solves it.</p><p>Kafka uses a <strong>centralized coordination model</strong>. The central coordinator is none other than… .</p><p>Kafka durably stores all metadata changes in a special  topic called . This storage model inherits all the benefits from topics. It gets fault-tolerance, durability, and most importantly for metadata, .</p><p>Each record in the log represents a  (a delta). When replayed fully in the same order, a node can deterministically rebuild the same cluster end state.<p>Here is a visual example of how it works in Kafka:</p></p><p>In other words, the cluster metadata topic partition is the source of truth for the latest metadata in Kafka.</p><p>Every broker in the cluster is subscribed to this topic. In real time, each broker pulls the latest committed updates. When a new record is fetched, the broker applies it to its in-memory metadata. This builds the broker’s idea of the latest state of the cluster.</p><p>If every broker is a follower of the partition, a natural question arises - who is the leader? What node gets to decide what new metadata is written to this partition?</p><p>Controllers serve as the control plane for a Kafka cluster. They’re special kinds of brokers that don’t host regular topics - they only do specific cluster-management operations. Usually, you’d deploy three controller brokers.</p><p>At any one time, there is only one active controller - the leader of the log. Only the active controller can write to the log. The other controllers serve as hot standbys (followers).</p><p>The active controller is responsible for making all metadata decisions in the cluster, like electing new partition leaders (when a broker dies), creating new topics, changing configs at runtime, etc.</p><p>Most importantly, it’s responsible for determining broker liveness.</p><p>Every broker issues  to the active controller. If a broker does not send heartbeats for 6 seconds straight, it is fenced off from the cluster by the controller. The controller then assigns other brokers to act as the leaders for the partitions the fenced (dead) broker led.</p><p>The careful reader will now ask:</p><blockquote><p>If the active controller is responsible for electing partition leaders, who’s responsible for electing the  leader?</p></blockquote><p>The  partition is special. A custom distributed consensus algorithm is used to elect its leader.</p><p>Leader election in a distributed system is a subset of the consensus problem. Many consensus algorithms exist, like Raft, Paxos, Zab, and so on.</p><p>Kafka uses its own <a href=\"https://raft.github.io/\">Raft</a>-inspired algorithm called KRaft (Kafka Raft).</p><ol><li><p>Elect the active controller 👑</p><ol><li><p>The controller nodes comprise a Raft quorum</p></li><li><p>The quorum runs a Raft election protocol to elect a leader of the  partition. The leader of that partition is the active controller</p></li></ol></li><li><p>Agree on the latest state of the metadata log</p><ol><li><p>Metadata updates are first appended to the Raft log on the active controller.</p></li><li><p>They are marked committed only when a majority of the quorum has persisted them.</p></li></ol></li></ol><p>The active controller determines the leaders for  regular topic partitions. It writes it to the metadata log, and once committed by the controller quorum, the decision is set in stone.</p><p>In other words, the way leader election in Kafka works is:</p><ul><li><p>Leader election  (picking the active one) is done through a variant of Raft (KRaft)</p></li><li><p>Leader election  is done through the controller.</p></li></ul><p>KRaft is a relatively recent algorithm in Apache Kafka. For many years, Kafka <a href=\"https://stanislavkozlovski.medium.com/apache-kafkas-distributed-system-firefighter-the-controller-broker-1afca1eae302\">used ZooKeeper</a>. Back then, there was just one controller. It performed the same tasks as today, but critically also had the responsibilities of a regular broker. Its decisions were persisted in <a href=\"https://en.wikipedia.org/wiki/Apache_ZooKeeper\">ZooKeeper</a>, which used the Zab consensus algorithm behind the scenes.</p><p>This coordinator-based leader election model differs from other systems. For example, RedPanda uses a separate Raft quorum per partition.</p><h2>Other Features That Set Kafka Apart</h2><p>A key motivation in Kafka’s design was to add the ability to replay historical data and decouple data retention from clients. Alternative messaging systems would store messages as long as no client has consumed them, and once consumed, delete them.</p><p>Kafka flips this model - it offers a simple time-based SLA as the message retention policy. A message is automatically deleted if it has been retained in the broker longer than a certain period, typically 7 days. The fact that the Log data structure’s O(1) performance doesn’t degrade with a larger data size makes this feasible.</p><p>Through this model, Kafka offers the feature of  - the ability to reprocess old historical messages. That is extremely useful in cases where, for example, a consumer has had a dormant bug in it for a while and erroneously processed messages. When the bug is fixed, the correct logic can be rerun on the same messages.</p><p>Unfortunately, at scale, it becomes extremely tricky to manage so much historical data. A cluster with 1 GB/s of producer bandwidth would collect 1,772 TB worth of data across the cluster. Even spread across 100 brokers, it’s still 17TB worth of data that each broker would have to host on its disk.</p><ul><li><p>❌ The system becomes inelastic. This happens because any action or incident requires a massive amount of data to be moved. That takes a long time.</p></li></ul><p>The Kafka community found an ingenious way to solve <a href=\"https://aiven.io/blog/16-ways-tiered-storage-makes-kafka-better#1-simpler-operations\">all these problems</a> with one simple idea → outsource them to S3.</p><p>While it may sound overly simple or lazy, it is an <strong>extremely elegant solution</strong>. S3 is a <a href=\"https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives\">marvel of software engineering</a> - it is maintained by hundreds of bright Amazon engineers. It is most likely the largest scale storage system known to man.</p><p>Kafka uses a pluggable interface to store cold data in a secondary storage tier. All three cloud object stores <a href=\"https://github.com/Aiven-Open/tiered-storage-for-apache-kafka\">are supported</a> as the secondary tier, and you are free to extend it further.</p><p>In essence, the data path in modern Kafka looks like this:</p><ul><li><p>: Write a message to a Kafka broker, which gets replicated across the replicas in the cluster. The message is stored on disk across all three nodes.</p><ul><li><p>The message is asynchronously offloaded to S3 (the secondary cold tier)</p></li></ul></li><li><p>: After a configurable amount of time (e.g., 12 hours), the message is deleted from the brokers. Its only source of truth is left in S3. It expires from S3 after a separate configurable period.</p></li></ul><p>You can still read the cold historical data from Kafka using the regular APIs. The only change is that the broker now fetches it from S3 instead of its own disk. </p><p>This results in slightly higher latencies when fetching historical data, but can be alleviated through caching. Latency for hot data can improve because it makes it cost-effective to deploy performant SSDs (instead of HDDs). Throughput remains the same very high number. Kafka as a system becomes much more elastic because it no longer needs to move massive amounts of data whenever new brokers are added or removed.</p><p>Storing large amounts of data in Kafka also ends up becoming more than 10x cheaper.</p><h3>Consumer Groups &amp; Read Parallelization</h3><p>Recall that the log is read sequentially and in order:</p><ul><li><p>A topic is split into partitions because it’s Big Data™ - a single node shouldn’t be able to consume the whole topic. You need many consumers to handle the high volume of topic data.</p></li><li><p> is meant to read from a partition at a time. This is done to ensure message order without needing locks.</p></li><li><p>These consumers need to coordinate on how to split partitions between each other.</p></li><li><p>At the same time, Kafka’s goal is to allow  consumption (multiple readers) of  partition(s) for high read-fanout cases.</p></li></ul><p>Kafka addresses this through. These groups are a set of consumer client instances (typically on separate nodes) that operate as one coherent unit. They distribute the work among themselves.</p><p>Each consumer group reads topics independently at its own pace. Consumers within the same group split the partitions between each other.</p><p>Consumer Groups support dynamic membership - you can scale consumption up or down by adding or removing members at runtime.</p><p>In essence, read throughput in Kafka can be scaled in two different ways:</p><ol><li><p>Add more  to your group</p><ul><li><p>e.g., your topic went from 10 MB/s to 20 MB/s of throughput, and your two existing consumers can’t keep up. Add more so they take up the extra load.</p></li></ul></li><li><ul><li><p>e.g., your topic is being consumed, but you’d like a new, separate application type to process the data too. For example, a nightly accounting job wants to catch up with the last day’s worth of payments.</p></li></ul></li></ol><h3>The Consumer Group Membership Protocol</h3><p>Consumers within a group form a <em><strong>distributed processing system. </strong></em>Unsurprisingly, we hit more distributed systems’ problems - how do we coordinate the consumers? They need to:</p><ul><li><p>Establish consensus on progress (up to what offset did they read to)</p></li><li><p>Handle liveness (did a consumer go offline, and how do we take over its work)</p></li><li><p>Handle dynamic membership (did a new consumer come online)</p></li><li><p>Distribute work between themselves (which consumer will take which partition)</p></li></ul><p>Kafka consumers within the same group don’t talk to each other. They coordinate indirectly through a Kafka broker. The broker that leads a certain group is called the .</p><p>Kafka again uses a <strong>centralized coordination model</strong> - the Group Coordinator makes the decisions. Consumers heartbeat to the coordinator and, through a pull-based model, inform themselves of what work they should do.</p><h4>The __consumer_offsets Topic</h4><p>The Group Coordinator also acts as the “database” which stores the progress of each consumer.</p><p>Consumer Groups store a simple mapping of ` in a special Kafka topic called . This helps them save the progress on what record they have </p><p>When a consumer reads messages, it commits the offset up to which it has processed the log via the coordinator broker. This regular checkpointing allows for smooth failovers. In the event of failure, the consumer can restart and resume from where it ended, or another consumer can come and pick the work back up.</p><p>The special offsets topic has many partitions spread throughout brokers. Each group is associated with a particular partition. The leader of the particular partition that the group is associated with acts as the Group Coordinator for that group. In that sense, every broker in the cluster can act as a coordinator for some consumer group. This prevents hot spots where one broker handles all the consumer groups in the cluster.</p><p>The consumer group protocol is a critical part of Kafka.</p><p>It is generic enough to support other use cases beyond consumer groups. It therefore provides<strong> a way for external distributed systems to establish leader election and durably persist state through Kafka</strong>.</p><p>Keep this in mind: the next three systems we’ll discuss depend on the group protocol to work as distributed systems.</p><p>But first, a little about transactions:</p><h3>Transactions &amp; Exactly Once Processing</h3><p>I will try to keep this brief because it can get pretty complicated - Kafka supports . But they aren’t quite like database transactions - it’s more about message visibility control.</p><p>A transaction in Kafka means that:</p><ul><li><p>A producer can send many messages. Those messages can go to different topics or partitions. They can also reach various brokers.</p></li><li><p>Those messages will  either be committed or aborted across all brokers.</p></li></ul><p>Technically, this happens In other words, the messages are still written to the topics, but consumers can be configured to skip non-committed ones. This is client-side filtering at work.</p><p>Marking transactions as committed or aborted is achieved through a two-phase commit protocol. It again relies on a centralized coordination model. A Transaction Coordinator broker makes this work. It’s pretty complex.</p><p>The important thing with transactions is that they enable message deduplication in common cases.</p><ul><li><p>: if the network drops the broker response packets, or the broker restarts, the same producer client will idempotently write its message without creating duplicates.</p></li><li><p>: if the producer itself restarts from a clean state, it will fetch its monotonically increasing ID and bump an epoch. This way, a potentially old zombie instance with the old epoch can’t interfere with the transaction.</p></li></ul><p>This doesn’t remove all cases of duplicates. Edge cases from external systems can still exist.</p><p>However, when reads and writes only involve Kafka (and no other external system), exactly once processing is possible.💡</p><p>This is actively supported and used in Kafka Streams, as we will cover shortly 👇</p><ul><li><p>: the brokers, controllers, and coordinators (back-end).</p></li><li><p>: the Kafka client libraries (producer, consumer).</p></li></ul><p>These are two more that we will focus on now:</p><ul></ul><p>A  is an application that:</p><ul><li><p>continuously processes messages</p></li><li><p>works performantly and at scale</p></li><li><p>emits the results in real-time</p></li><li><p>supports complex stateful operations like windowed aggregations and joins of multiple streams of data</p></li></ul><p> is a high-level <a href=\"https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java\">client-side stream processing</a> Java library for Kafka. It offers rich higher-level stream processing APIs that basically do this:</p><ol><li><p>continuously read a stream of messages from a set of Kafka input topics</p></li><li><p>perform processing on these messages (map, filter, joins, sums, stateful window aggregations, etc.)</p></li><li><p>continuously write the results into an output topic</p></li></ol><p>Here is a simple pseudocode example of its declarative API:</p><p>This code continuously counts the sum of human page views over the last minute and produces it in a new topic. Here is an example of what the Kafka topics would look like:</p><p>This API is intended to be used within your own Java applications. It works like the consumer. It helps you scale by spreading the stream processing work over multiple applications (just like consumer groups do). One difference is that it also lets you spread the work through . It uses the same consumer group protocol underneath to coordinate work between instances.</p><p>It is technically possible to achieve this with your own code using the simple producer/consumer libraries, but it’d be a lot of work. Kafka Streams is a higher-level abstraction above both clients with a ton of extra processing, orchestration, and stateful logic on top. 👌</p><p>Kafka Streams only works with Kafka. It takes input from a Kafka topic and sends output to another Kafka topic. This setup allows Kafka Streams to guarantee exactly once processing by using Kafka Transactions. Practically speaking, this means that it can  process data.</p><p>For example, it could read a set of payment messages in a Kafka topic, calculate the sum, and persist the result in another Kafka topic, with a 100% guarantee that no message was lost or double-counted in the process.</p><blockquote><p>If interested in more, here is a quick introduction to <a href=\"https://bigdata.2minutestreaming.com/p/what-is-kafka-streams-api-guide\">Kafka Streams</a>.</p></blockquote><p>Kafka does not support types (e.g., int64) nor schemas. Messages are just raw bytes. To process a message, like summing order payment values, you need to be able to parse the message structure and the exact value field. How is this achieved, then?</p><p>There’s no “official” way, because the open source Apache project does not support schemas. There is a common convention, though. That is to use an external HTTP service with a database that stores the schemas.</p><p>In practice, a Kafka topic acts as this database. It stores a simple pair of . Kafka clients connect to the service, download the schema, and use it whenever they serialize/deserialize messages.</p><p>The first such registry was a <a href=\"https://github.com/confluentinc/schema-registry\">source-available project</a> by Confluent called Schema Registry. However, without official Apache support or a truly open-source license, the ecosystem has fragmented. There are many different service implementations today. Many Kafka users also opt to manage schemas in their own unconventional ways.</p><p>The way the end-to-end path conventionally works with schemas:</p><ol><li><p>Producers decide on a schema, associate it with a topic, and register it in the registry</p></li><li><p>Producers then serialize the message in the correct structure (including the unique schema ID in the message) and write it to Kafka</p></li><li><p>Consumers download the message, parse the schema ID, then fetch (and cache) the schema from the registry</p></li><li><p>Consumers use the schema to deserialize the message</p></li></ol><p>Kafka <a href=\"https://bigdatastream.substack.com/p/why-was-apache-kafka-created\">was created to solve LinkedIn’s data integration problem</a>. It is meant to move data between different systems. This can be difficult because each system can have its own API, its own protocol (TCP, HTTP, JDBC, etc.), its own format (XML, JSON, Protobuf, Avro), and different compatibility guarantees.</p><p>Kafka Connect helps standardize this. Connect is both a framework (set of APIs) and a  for plugins that connect Kafka with external systems.</p><p>For the end user, it’s a no-code/low-code framework to plumb popular systems to Kafka and back (think ElasticSearch, Snowflake, PostgreSQL, BigQuery).</p><p>This ensures a single, standardized way to integrate systems together. The tricky bits of code that guarantee fault-tolerance, ordering, and exactly-once processing are written once (in the form of plugins) and battle-tested.</p><p>Connect has three main terms one should know about:</p><ul><li><p>: the simple nodes that form a distributed Connect Cluster</p></li><li><p>: a worker that acts as the manager of the cluster. It exposes a REST API with which users can check the status of tasks, start new ones, etc</p></li><li><p>: the plugins (or libraries) that run on the workers. They contain the code needed to integrate with other systems</p><ul><li><p>A  Connector reads data from an external system and writes it to Kafka (System-&gt;Kafka)</p></li><li><p>A  Connector reads data from Kafka and writes it to an external system (Kafka-&gt;System)</p></li></ul></li></ul><p>The end user spins up a cluster with several Worker nodes. They install the particular Connector plugin jars on these nodes. Then, they start the integration with a simple HTTP POST request.</p><p>This again forms another distributed processing system. The Herder leader election, general cluster membership, and the distribution of new tasks throughout the group of Workers are all done transparently via Kafka’s Consumer Group protocol.</p><p>Essentially, Connect is a lot of plugin-specific integration logic on top of the regular KafkaProducer and KafkaConsumer APIs. <a href=\"https://www.confluent.io/hub/\">Hundreds of Connector plugins</a>&nbsp;exist, which give Kafka its incredibly rich integration capabilities.</p><p>With that, we’ve gone over all the important internals of Apache Kafka. I repeat the introductory sentence, which you can now hopefully understand much better:</p><blockquote><p>💡 Kafka is an open-source, distributed, durable, very scalable, fault-tolerant pub/sub messaging system with rich integration and stream processing capabilities.</p></blockquote><p>It achieves this through many internal details, including but not limited to:</p><ul><li><p>The Producer &amp; Consumer libraries &amp; APIs</p></li><li><p>Topics, Partitions, and Replicas</p></li><li><p>Brokers and KRaft Controller Quorums</p></li><li><p>Idempotency, Transactions, and Exactly-Once Processing</p></li><li><p>Consumer Groups &amp; the Consumer Group Protocol</p></li></ul><p>This is why Kafka is the Swiss army knife of data engineering.</p><p>It is a very active open-source project that is constantly evolving. A few notable features that are currently being worked on are:</p><ul><li><p><a href=\"https://blog.2minutestreaming.com/p/apache-kafka-share-group-queues-kip-932\">Queues</a>: the ability to read a partition with queue-like semantics. Queues have no ordering but allow for multiple consumers to read from the same log with per-record acknowledgement. This is different from Kafka’s exclusive one-consumer-per-partition model. In that model, consumers read data in order and only know “I’ve read  this message”.</p></li><li><p><a href=\"https://blog.2minutestreaming.com/p/diskless-kafka-topics-kip-1150\">Diskless Topics</a>: the ability to host topic partitions in a leaderless way. This happens by leveraging object storage (S3) as the data layer instead of brokers’ disks. This feature can cut cloud costs by + (!), further boost scalability, and simplify managing Kafka.</p></li></ul><p>Many alternative proprietary systems compete with Apache Kafka. The one common denominator is that they all use the same Kafka protocol and API. They just implement it differently. These include, but are not limited to, Confluent Kora, AWS MSK, RedPanda, WarpStream, BufStream, Aiven Inkless, AutoMQ, and more.</p><p>The space is incredibly rich and rapidly evolving. If you’re further interested in the world of event streaming and Apache Kafka, make sure to stay in touch. 🙂</p><p>👋 I'd like to thank <a href=\"http://linkedin.com/in/stanislavkozlovski\">Stanislav</a> for writing this newsletter!</p><p>It’ll help you master Kafka quickly.</p><p>Subscribe to get simplified case studies delivered straight to your inbox:</p><p><strong>Want to advertise in this newsletter? </strong>📰</p><p>Thank you for supporting this newsletter.</p><p>You are now 170,001+ readers strong, very close to 171k. Let’s try to get 171k readers by 28 September. Consider sharing this post with your friends and get rewards.</p><p>Apache®, Apache Kafka®, Kafka, and the Kafka logo are either registered trademarks or trademarks of the Apache Software Foundation in the United States and/or other countries. No endorsement by The Apache Software Foundation is implied by the use of these marks.</p>","contentLength":30872,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/f3135c16-7ad0-4de4-a4d7-483c91e84c36_1280x720.png","enclosureMime":"","commentsUrl":null},{"title":"5 Rate Limiting Strategies Explained, Simply 🚦","url":"https://newsletter.systemdesign.one/p/rate-limiting","date":1758306054,"author":"Neo Kim","guid":81,"unread":true,"content":"<p>Get my system design playbook for FREE on newsletter signup:</p><p><em>This post outlines important rate limiting strategies. You will find references at the bottom of this page if you want to go deeper.</em></p><p>Once upon a time, there was a tiny bank.</p><p>They had only a few customers.</p><p>And offered services through a mobile app.</p><p>Yet some people attempted to log in to customers’ accounts by guessing passwords.</p><p>So they solved this problem by blocking specific IP addresses using firewall rules.</p><p>But one day, their app became extremely popular.</p><p>And it became difficult to block many malicious IP addresses quickly.</p><p>So they set up .</p><p>It means slowing down requests instead of blocking them.</p><p>Yet it doesn’t stop abuse because someone could queue many requests and waste resources.</p><p>So they installed a rate limiter.</p><p> means controlling the number of requests a user can make within a time window.</p><p>For example, their app allows only 5 login attempts an hour by a user. While future attempts of the user get blocked until the time window passes.</p><p>This technique prevents abuse and server overload.</p><p>Imagine rate limiting as tickets to a movie theater. A show sells only 50 tickets. Once the tickets are sold, you’ve got to wait for the next showtime to get in.</p><ol><li><p>The user sends requests through a rate limiter</p></li><li><p>The rate limiter tracks requests from a user by their IP address, user ID, or API key</p></li><li><p>The extra requests get rejected if the limit exceeds (response status code:)</p></li><li><p>The counter resets after the time window ends</p></li></ol><p> is partnering with MiniMax for a free TTS API week.</p><p>Developers &amp; builders can get:</p><p>• Free &amp; unlimited access to all MiniMax voices until Sept 22</p><p>• 20% off MiniMax API for a year if you try it this week</p><p>• $10 Vapi credit with code VAPIMINIMAX10</p><p>Explore next-gen TTS with the free API now.</p><p>There are different strategies to implement a rate limiter. </p><p>It’s one of the most popular rate limiting strategies.</p><ol><li><p>Each user gets a bucket of tokens</p></li><li><p>And new tokens get added to the bucket at a fixed rate</p></li><li><p>While each request from the user needs 1 token to go through</p></li><li><p>The request gets blocked if the bucket is empty</p></li></ol><p>This strategy is easy to implement and understand. </p><p>Also it allows spiky traffic. For example, imagine the bucket has 5 tokens and refills at 1 token per hour. A user can make 5 requests in a few seconds, but then they’d have to wait 1 hour to make an extra request.</p><p>Yet this strategy affects the user experience by making them wait a long time if new tokens get added slowly. While refilling tokens quickly might affect the security.</p><p>So it’s necessary to control the speed at which tokens get added based on the domain and server capacity.</p><p>This strategy ensures fair usage of resources by smoothing traffic.</p><ol><li><p>Each user gets a bucket with a tiny hole at its bottom</p></li><li><p>Incoming requests are like water poured into the bucket</p></li><li><p>While water drips through the hole, which means a request got processed</p></li><li><p>Also water drips at a constant rate, even without new requests, representing server capacity</p></li><li><p>The bucket overflows if so much water is poured in quickly, which means rejected requests</p></li></ol><p>Yet unused capacity gets lost even without processing requests with this approach.</p><p>Besides it’ll block requests during a traffic burst. So use this strategy when traffic is predictable, and when it’s okay to block some requests for users during a traffic burst.</p><p>Ready for the next technique?</p><p>This strategy works well with predictable traffic.</p><ol><li><p>Time gets divided into equal blocks; for example, 1-hour windows</p></li><li><p>Each block has a request limit: 5 requests per hour</p></li><li><p>Requests get counted within the current block</p></li><li><p>Extra requests get blocked if the count exceeds the limit</p></li><li><p>The counter resets to 0 when a new block starts</p></li></ol><p>Although it’s easy to implement, it allows traffic bursts on window boundaries. </p><p>For example, a user could double their allowed requests by sending them at the end of one window and the start of the next. Thus overloading the server.</p><p>So use this strategy when the server can handle occasional traffic bursts.</p><p>This strategy offers fairness and works better with traffic bursts.</p><ol><li><p>Define a rolling time window, for example, a 1-hour window</p></li><li><p>Each window has a request limit: 5 requests per hour</p></li><li><p>A counter then keeps track of the current and previous windows</p></li><li><p>The counter combines the current and previous windows to  the number of requests in the last hour</p></li><li><p>While a new request gets blocked if the count exceeds the limit</p></li></ol><p>Put simply, it approximates the number of requests within the last hour from the current time.</p><p>Yet it’s more complex to implement than the fixed window, and the rate limiter count is approximate.</p><p>So use this strategy to rate limit efficiently in systems with moderate traffic.</p><p>It’s like the sliding window counter, except it keeps a log of all request timestamps. Thus offering accuracy.</p><ol><li><p>A rolling time window gets defined, for example, a 1-hour window</p></li><li><p>Each window has a request limit: 5 requests per hour</p></li><li><p>Old requests outside the window get removed for each new request</p></li><li><p>The remaining requests inside the window get counted</p></li><li><p>A new request gets blocked if the count has already reached the limit</p></li></ol><p>This strategy is precise and fair.</p><p>Yet it’s memory and CPU-intensive. So use it only when strict precision and fairness are necessary.</p><p>A rate limiter protects infrastructure from abuse and ensures high availability.</p><p>Yet it’s necessary to choose the right rate limiting strategy to avoid blocking users unnecessarily. </p><p>Also users from a building might share the same IP address. So it’s better to rate limit a person using an API key instead of their IP address for accuracy and fairness.</p><p>The right strategy for rate limiting depends on simplicity and the fairness needs.</p><p>A banking app needs precision and fairness. So it’s better to use the sliding window log strategy for it.</p><p>Subscribe to get simplified case studies delivered straight to your inbox:</p><p><strong>Want to advertise in this newsletter? </strong>📰</p><p>Thank you for supporting this newsletter.</p><p>You are now 171,001+ readers strong, very close to 172k. Let’s try to get 172k readers by 20 September. Consider sharing this post with your friends and get rewards.</p>","contentLength":6045,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/f50746ad-071e-4b2c-ae7b-4ad67d4e0d79_1280x720.png","enclosureMime":"","commentsUrl":null},{"title":"How Sidecar Pattern Works ✨","url":"https://newsletter.systemdesign.one/p/sidecar-pattern","date":1758191619,"author":"Neo Kim","guid":80,"unread":true,"content":"<p>Get my system design playbook for FREE on newsletter signup:</p><p><em>This post outlines the sidecar pattern. You will find references at the bottom of this page if you want to go deeper.</em></p><p>Once upon a time, a single server was enough to run an entire site.</p><p>The clients connected to it over the internet.</p><p>But as the internet grew, traffic to some sites skyrocketed.</p><p>And it became hard to scale those sites reliably.</p><p>So they installed more servers and added logic for logging, monitoring, and security.</p><p>Yet it became difficult to update the app’s code without affecting its reliability.</p><p>It’s a design pattern. And runs a small service alongside the app to help with tasks such as logging, monitoring, or security.</p><p>Imagine a motorcycle with a sidecar. The driver steers the vehicle, while the sidecar passenger carries the map, radio, or bag.</p><p>Similarly, the sidecar pattern decouples the operational features from the app’s logic.</p><p> is an AI code review tool that runs directly in your terminal. It provides intelligent code analysis, catches issues early, and integrates seamlessly with AI coding agents like Claude Code, Codex CLI, Cursor CLI, and Gemini to ensure your code is production-ready before it ships.</p><ul><li><p>Enables pre-commit reviews of both staged and unstaged changes, creating a multi-layered review process.</p></li><li><p>Fits into existing Git workflows. Review uncommitted changes, staged files, specific commits, or entire branches without disrupting your current development process.</p></li><li><p>Reviews specific files, directories, uncommitted changes, staged changes, or entire commits based on your needs.</p></li><li><p>Supports programming languages including JavaScript, TypeScript, Python, Java, C#, C++, Ruby, Rust, Go, PHP, and more.</p></li><li><p>Offers free AI code reviews with rate limits so developers can experience senior-level reviews at no cost.</p></li><li><p>Flags hallucinations, code smells, security issues, and performance problems.</p></li><li><p>Supports guidelines for other AI generators, AST Grep rules, and path-based instructions.</p></li></ul><ul><li><p>The app runs the core logic and handles requests.</p></li><li><p>Although detached, the sidecar and app share the same storage and networking environment.</p></li><li><p>Sidecar runs alongside the app, handling tasks such as logging, monitoring, and security.</p></li><li><p>They communicate with each other through a local network or shared resources, such as a configuration file or shared memory.</p></li><li><p>The sidecar and app start, stop, and scale together for reliability.</p></li></ul><p>Also it’s possible to update or replace the sidecar without affecting the app.</p><p>There are 2 ways to deploy a sidecar:</p><ul><li><p>As a separate container alongside the app.</p></li><li><p>As a separate process on the same server.</p></li></ul><p>Yet the way a sidecar gets deployed depends on the use case and infrastructure setup.</p><p>Here are three popular use cases of the sidecar pattern:</p><p>The sidecar acts as a traffic manager. It controls incoming and outgoing requests for the app.</p><ol><li><p>Imagine the app wants to call another service, for example, the payments API.</p></li><li><p>It sends a request to the sidecar instead of calling the API directly.</p></li><li><p>The sidecar then forwards the request to the correct API.</p></li><li><p>It also automatically retries if the request fails due to a timeout or other error.</p></li></ol><p>Besides the sidecar <a href=\"https://en.wikipedia.org/wiki/TLS_termination_proxy\">decrypts</a> the incoming traffic before sending it to the app.</p><p>This avoids the need for retry logic or security logic inside the app itself.</p><p>It’s also used to add <a href=\"https://newsletter.systemdesign.one/p/how-does-https-work\">HTTPS</a> support to legacy services.</p><p>A popular implementation of the sidecar traffic proxy is <a href=\"https://www.envoyproxy.io/\">Envoy</a>. It usually gets deployed as a separate container alongside the app.</p><p>Log management and monitoring increase the app complexity. </p><p>A sidecar solves this problem by collecting logs and sending them to a central system.</p><ol><li><p>The app writes logs to its local file, standard output, or a stream.</p></li><li><p>Sidecar collects these logs from the app and combines them if needed.</p></li></ol><p>It lets the developer view all logs in a single place without changing the app’s code.</p><p>A popular implementation of the sidecar for logging and monitoring is <a href=\"https://www.fluentd.org/\">Fluentd</a>. It’s possible to deploy it as a separate process or container alongside the app.</p><p>It’s unsafe for an app to store sensitive data, such as passwords or API tokens. </p><p>A sidecar solves this problem by managing sensitive data for the app.</p><ol><li><p>The sidecar gets secrets, such as passwords or certificates, from a secure system.</p></li><li><p>It then provides them to the app at runtime through a file, environment variable, or shared memory.</p></li></ol><p>This technique keeps the secrets separate from the app’s code for security.</p><p>A popular implementation of the sidecar for security management is <a href=\"https://developer.hashicorp.com/vault/tutorials/vault-agent\">Vault Agent</a>.</p><p>Here’s how the sidecar pattern helps with a microservices architecture:</p><ul><li><p>Sidecar handles extra tasks, while the microservice contains only business logic. Thus keeping it simple.</p></li><li><p>Each microservice has the same sidecar setup. Thus ensuring consistent logging, monitoring, or security features.</p></li><li><p>A sidecar usually runs outside the app. So it doesn’t matter if one microservice runs java and another runs python. Thus making it language independent.</p></li></ul><p>Yet everything comes with a tradeoff, and the sidecar too.</p><ul><li><p>It consumes extra CPU, memory, and network capacity.</p></li><li><p>It adds extra latency to each request.</p></li><li><p>It introduces the risk of hidden failures because a functional app may look broken just from a sidecar failure.</p></li><li><p>It increases operational complexity as it’s necessary to configure, monitor, and update the sidecar.</p></li><li><p>It brings synchronization challenges as the app and sidecar should start, stop, and scale together.</p></li></ul><p>The sidecar pattern became popular with microservices architecture.</p><p>Yet a monolithic app can also use it to handle operational tasks for reliability.</p><p>The sidecar pattern is useful if the app runs in different languages and frameworks. But avoid using it if the app has resource limitations and needs fast communication. </p><p>Subscribe to get simplified case studies delivered straight to your inbox:</p><p><strong>Want to advertise in this newsletter? </strong>📰</p><p>Thank you for supporting this newsletter.</p><p>You are now 171,001+ readers strong, very close to 172k. Let’s try to get 172k readers by 20 September. Consider sharing this post with your friends and get rewards.</p>","contentLength":6017,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/de60cc97-ab5e-46f7-a630-b88970a05a46_1280x720.png","enclosureMime":"","commentsUrl":null},{"title":"11 System Design Concepts Explained, Simply","url":"https://newsletter.systemdesign.one/p/11-system-design-concepts-explained","date":1758032323,"author":"Neo Kim","guid":79,"unread":true,"content":"<p>Get my system design playbook for FREE on newsletter signup:</p><p><em>This post outlines 11 must-know system design concepts.</em></p><p>Systems design is an important part of all software engineering tasks you will work on in your career.</p><p>While junior developers may get by with writing functional code, senior engineers are valued for their ability to keep the bigger picture in mind.</p><blockquote><p><em>How will this system handle a million users?</em></p><p><em>What happens when a critical component fails?</em></p><p><em>How do we maintain performance as users and their data grow exponentially?</em></p></blockquote><p>These are some questions that form the foundation of every ‘effortlessly’ running big tech app out there.</p><p>He's a self-taught software engineer and works as an emergency physician.</p><p>If you're getting started with system design, I highly recommend getting his book: . It'll help you learn the fundamentals quickly.</p><p>(You'll also get a 25% discount when you use the code—SYSTEMS25.)</p><p>Systems design teaches you how to answer those questions well. It is how you go from “<em>barely making things work</em>” to “<em>making things work at scale</em>”.</p><p> is an AI code review tool that runs directly in your terminal. It provides intelligent code analysis, catches issues early, and integrates seamlessly with AI coding agents like Claude Code, Codex CLI, Cursor CLI, and Gemini to ensure your code is production-ready before it ships.</p><ul><li><p>Enables pre-commit reviews of both staged and unstaged changes, creating a multi-layered review process.</p></li><li><p>Fits into existing Git workflows. Review uncommitted changes, staged files, specific commits, or entire branches without disrupting your current development process.</p></li><li><p>Reviews specific files, directories, uncommitted changes, staged changes, or entire commits based on your needs.</p></li><li><p>Supports programming languages including JavaScript, TypeScript, Python, Java, C#, C++, Ruby, Rust, Go, PHP, and more.</p></li><li><p>Offers free AI code reviews with rate limits so developers can experience senior-level reviews at no cost.</p></li><li><p>Flags hallucinations, code smells, security issues, and performance problems.</p></li><li><p>Supports guidelines for other AI generators, AST Grep rules, and path-based instructions.</p></li></ul><p>Here are all the key concepts that you need to understand systems design and become a better engineer:</p><p><a href=\"https://en.wikipedia.org/wiki/Scalability\">Scalability</a> is a system’s ability to handle increased load while maintaining acceptable performance.</p><p>These are two ways to scale systems:</p><ol><li><p>: adding more machines/ servers to handle increased load by distributing work across them.</p></li><li><p>: increasing the power of existing machines/ servers by adding more CPU, RAM, or storage to handle increased load.</p></li></ol><blockquote><p><em>Horizontal scaling means ‘scaling out’ to different servers.</em></p><p><em>Vertical scaling means ‘scaling up’ existing servers.</em></p></blockquote><h3>2. Throughput &amp; Bandwidth</h3><p>Both terms are used interchangeably, but they have different meanings.</p><p><a href=\"https://en.wikipedia.org/wiki/Bandwidth_(computing)\">Bandwidth</a> refers to the amount of data that can potentially travel through a network within a period.</p><p><a href=\"https://en.wikipedia.org/wiki/Network_throughput\">Throughput</a> refers to the amount of data that actually transfers during a specified period.</p><h3><strong>3. Concurrency vs. Parallelism</strong></h3><p>Many engineers confuse these terms, so it’s important to understand their meanings clearly.</p><p><a href=\"https://en.wikipedia.org/wiki/Parallel_computing\">Parallelism</a> refers to executing multiple tasks simultaneously across different processor cores or machines.</p><p><a href=\"https://en.wikipedia.org/wiki/Concurrency_(computer_science)\">Concurrency</a> means executing multiple tasks simultaneously, either by running them in parallel or by rapidly switching between them on the same processor core.</p><blockquote><p><em>Parallelism is a subset of concurrency.</em></p></blockquote><h3><strong>4. Consistency, Availability &amp; Partition Tolerance</strong></h3><p>These three terms are crucial for understanding the trade-offs and limitations when designing distributed systems.</p><ul><li><p>: The guarantee that a system of all machines connected in a distributed manner sees the same data at the same time.</p></li></ul><ul><li><p>: The degree to which a system remains operational and responsive to requests.</p></li></ul><ul><li><p>The capability of a distributed system to continue operating despite network failures between the connected machines.</p></li></ul><p>These three concepts are related to each other through the CAP theorem.</p><p>Let’s understand what it is next.</p><p>The <a href=\"https://en.wikipedia.org/wiki/CAP_theorem\">CAP theorem</a> states that in a distributed system, you can guarantee at most two out of these three properties (consistency, availability, and partition tolerance) but never all three simultaneously.</p><p>This leads to systems that either have:</p><ul><li><p>High Consistency and Partition Tolerance (for example, databases such as MongoDB and HBase).</p></li><li><p>High Consistency and Availability (for example, traditional relational databases such as PostgreSQL that run on a single machine).</p></li><li><p>High availability and Partition tolerance (for example, databases such as Cassandra and CouchDB).</p></li></ul><p>The CAP theorem is further extended by the PACELC theorem.</p><p>While the CAP theorem states that with network partitioning, a distributed system must choose between availability and consistency.</p><p>The <a href=\"https://en.wikipedia.org/wiki/PACELC_design_principle\">PACELC theorem</a> adds to this by stating that, where there is no network partitioning, the distributed system still faces a trade-off between Latency and Consistency.</p><p>The acronym PACELC stands for Partition (P), Availability (A), Consistency (C), Else (E), Latency (L), Consistency (C).</p><p>We have not yet discussed what latency is, so let’s talk about it next.</p><p>In distributed systems, <a href=\"https://en.wikipedia.org/wiki/Latency_(engineering)\">Latency</a> is the time it takes for a request to go through the system and return a response.</p><h3><strong>8. Techniques That Reduce Latency</strong></h3><p>Multiple techniques are used to decrease latency. Some of them are:</p><ul><li><p>: a technique where frequently accessed results are stored in memory in a cache server rather than constantly querying the central database (a slow operation) for results.</p></li></ul><ul><li><p>a techniquewhere data is divided into smaller subsets called shards and distributed across multiple machines/ nodes.</p></li></ul><p>In this way, when the system is queried, each query only reaches the nodes that store the relevant shards, which process the request, reducing load and response time.</p><ul><li><p> a technique of distributing incoming network requests across multiple servers to prevent any single server from becoming overloaded.</p></li></ul><p>There are multiple algorithms for load balancing, and some popular ones are:</p><ul><li><p>Sending requests to servers in a fixed cyclical order.</p></li><li><p>Sending requests to the server with the fewest active connections.</p></li><li><p>Sending requests to the server with the fastest response time.</p></li><li><p>: Sending requests to servers in a fixed cyclical order, but proportional to each server's capacity.</p></li></ul><p>Next, let’s learn about databases.</p><h3><strong>9. Relational vs. Non-Relational Databases</strong></h3><p>The two main types of databases are:</p><ul><li><p>These databases organize data into tables with predefined schemas and use SQL (Structured Query Language) to query them. For example, PostgreSQL and MySQL.</p></li><li><p>These databases do not rely on fixed table schemas and store and query data in flexible ways. For example, MongoDB, Redis, and Cassandra.</p></li></ul><h3><strong>10. Transactions &amp; Their Types</strong></h3><p>The next concept that you must know about is a <a href=\"https://en.wikipedia.org/wiki/Database_transaction\">Transaction</a>, which is a logical unit of work that groups one or multiple operations.</p><p>The two types of transactions in distributed systems are:</p><ul></ul><p>The acronym ACID stands for:</p><ul><li><p>: A transaction either completes entirely or fails, and no partial transactions occur.</p></li><li><p>: A system remains in a valid state before and after each transaction, as per all rules and constraints.</p></li><li><p>: Transactions running in parallel do so as if they’re the only ones executing. No two transactions interfere with each other’s intermediate states.</p></li><li><p>: Once a transaction is committed, its changes are permanently saved and are unaffected by system failures.</p></li></ul><p>The acronym BASE stands for:</p><ul><li><p>: The system remains operational even when parts of it fail, although some data may be temporarily inaccessible.</p></li><li><p>: Data can be temporarily inconsistent across different parts of the system. These temporary inconsistencies are resolved at a later time.</p></li><li><p>: Given sufficient time, all parts of the system synchronize and eventually converge to a consistent state.</p></li></ul><p>Relational/ SQL databases use ACID transactions to maintain strict data integrity and relationships.</p><p>NoSQL databases use BASE transactions to achieve high scalability and performance by relaxing consistency requirements.</p><p>BASE transactions help large-scale applications, such as those from Amazon, Google, and Meta, work around the CAP/PACELC trade-offs by prioritizing availability and low latency over strict consistency.</p><p>These are sets of rules that allow different applications to communicate with each other.</p><p>Think of an API as a waiter in a restaurant. A client requests the waiter. The waiter (the API) takes the order to the kitchen (the Server), and gets back with the food for the client (the Response).</p><p>Three types of Web API architectural styles are popular and important to know about:</p><ul><li><p>This is the most common architectural style for designing web APIs. REST APIs use standard HTTP methods (, , , ), are stateless (so each request contains all the information for the server to process it), and usually return data in JSON format.</p></li></ul><ul><li><p>An older, highly structured API protocol that uses XML for message format. It is popularly used in enterprise environments because of its built-in error handling and security features.</p></li></ul><ul><li><p>: This is a newer API architectural style developed by Facebook (now Meta) that allows clients to request the exact data they need. Unlike REST, where multiple endpoints may be required, GraphQL uses a single endpoint to query for specific data requirements.</p></li></ul><p>Now that you understand the basic concepts that are useful in designing systems that work at scale, it’s time to explore how they are applied.</p><p>Look at the apps you use every day and think about how they might work behind the scenes.</p><p><em>How does Substack handle millions of blogs?</em></p><p><em>How does WhatsApp deliver messages instantly worldwide?</em></p><p><em>How does your bank app handle millions of users without crashing?</em></p><p><em>Why doesn’t YouTube buffer when millions watch videos all at once?</em></p><p>After understanding the design principles behind these existing systems, start applying these concepts in your own work.</p><p>Practice redesigning small projects you’ve built before. Talk to other engineers to get their perspectives on how they would handle a problem.</p><p>Remember, every expert was once a beginner. I am sure that you will design systems that handle millions of users before you know it.</p><p>👋 I'd like to thank <a href=\"https://www.linkedin.com/in/ashishbamania/\">Ashish</a> for writing this newsletter!</p><p>Don't forget to  at a special 25% discount. It’ll help you understand system design fundamentals quickly.</p><p>Unlock access to every deep dive article by becoming a paid subscriber:</p><p><strong>Want to advertise in this newsletter? </strong>📰</p><p>Thank you for supporting this newsletter.</p><p>You are now 171,001+ readers strong, very close to 172k. Let’s try to get 172k readers by 20 September. Consider sharing this post with your friends and get rewards.</p>","contentLength":10547,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/022bc317-6132-4360-b3ed-af627124fe2e_1280x720.png","enclosureMime":"","commentsUrl":null},{"title":"9 Best Practices for API Security ⚔️","url":"https://newsletter.systemdesign.one/p/api-security-best-practices","date":1757668571,"author":"Neo Kim","guid":78,"unread":true,"content":"<p>Get my system design playbook for FREE on newsletter signup:</p><p><em>This post outlines API security best practices. You will find references at the bottom of this page if you want to go deeper.</em></p><p><em>I created the block diagrams in this newsletter with</em></p><p>Once upon a time, there was a two-person startup.</p><p>Yet they had only a few customers and limited features.</p><p>So they ran their site using a tiny number of application programming interfaces ().</p><p>But one day, their site became extremely popular.</p><p>So they set up a microservices architecture for scalability.</p><p>Yet they knew little about API security.</p><p>And sent sensitive data in plain text through the API.</p><p>Also some of their public APIs often failed because of spiky traffic.</p><p>Besides it became difficult to keep APIs secure as they added more features.</p><p>So they researched the best practices to secure APIs.</p><p> brings real-time, AI-powered code reviews straight into VS Code, Cursor, and Windsurf. It lets you:</p><ul><li><p>Get contextual feedback on every commit, not just at the PR stage</p></li><li><p>Catch bugs, security flaws, and performance issues asyou code</p></li><li><p>Apply AI-driven suggestions instantly to implement code changes</p></li><li><p>Do code reviews in your IDE for free and in your PR for a paid subscription</p></li></ul><h2>API Security Best Practices</h2><p>Here’s what they learned:</p><p>HTTP stands for Hypertext Transfer Protocol.</p><p>Think of HTTP as a set of rules for transferring information on the Internet.</p><p>But it sends data in plaintext without encryption. It means someone on the public network can easily access or change the information. Thus making it insecure. </p><p>So it’s necessary to have Hypertext Transfer Protocol Secure (HTTPS) on the API.</p><p>HTTPS encrypts data before sending it. Imagine  as HTTP running over an extra protocol to keep information secure.</p><p>The extra protocol is called Transport Layer Security (). Think of TLS as a technique to encrypt data sent between the client and server.</p><ol><li><p>The client connects to the API endpoint</p></li><li><p>The server gives a digital certificate to prove its identity</p></li><li><p>The browser checks the certificate against the trusted certificate authorities</p></li><li><p>Both sides agree on a shared encryption key</p></li><li><p>Then each request gets encrypted with that key</p></li></ol><p>Here are some best practices:</p><ul><li><p>Use the latest version of TLS supported by the platform</p></li><li><p>Use HTTP Strict Transport Security ()</p></li></ul><p>HSTS gets sent as an <a href=\"https://newsletter.systemdesign.one/p/http-headers?open=false#%C2%A7strict-transport-security\">HTTP response header</a> (<code>Strict-Transport-Security</code>). It tells the browser always to use HTTPS for this domain for a specific period. </p><p>While strong ciphers, such as <a href=\"https://en.wikipedia.org/wiki/Advanced_Encryption_Standard\">AES</a> or <a href=\"https://protonvpn.com/blog/chacha20\">ChaCha20</a>, ensure the encrypted connection is secure.</p><p>Anybody on the internet can call a public API.</p><p>Yet only verified users should access the data. So API authentication is necessary; it verifies a user’s identity before giving them system access.</p><p>Here are some best practices:</p><ul><li><p>Use OAuth and OpenID Connect ()</p><ul><li><p>Think of OAuth as a standard protocol for authorization</p></li><li><p>The client asks for access to the system</p></li><li><p>The authorization server checks the identity and gives them a token</p></li><li><p>The API then checks the token before allowing any action</p></li><li><p>While OIDC is a standard way to handle user authentication on top of OAuth</p></li></ul></li><li><p>Use multi-factor authentication ()</p><ul><li><p>It adds extra steps to the authentication for security</p></li><li><p>The client logs in with their username and password</p></li><li><p>The system asks for an MFA code from the authenticator app</p></li><li><p>The authorization server gives a token only if both checks pass</p></li><li><p>The API then checks the token before allowing any action</p></li></ul></li><li><ul><li><p>Keep the token lifespan short</p></li><li><p>And refresh them often to reduce risk if a token gets stolen</p></li></ul></li></ul><p>Ready for the next technique?</p><p>Authorization decides what an authenticated client may do in a system.</p><p>Imagine  as permission to enter a building. While  is like the key to enter only specific rooms once inside.</p><p>Here are some best practices:</p><ul><li><p>Use Role-Based Access Control ()</p><ul><li><p>Group users by their responsibility or function</p></li><li><p>Give separate permissions to each role</p></li><li><p>Then assign users to specific roles so they get access permissions</p></li><li><p>The system checks the role permissions before allowing actions</p></li></ul></li><li><p>Use clear permission policies</p><ul><li><p>Write clear rules that explain who can access what and when</p></li><li><p>Review and update permissions regularly to match the latest responsibilities</p></li></ul></li></ul><p>Proper access control reduces risk. So always give only the least privilege needed to get the job done.</p><h3>4. Rate Limiting and Throttling</h3><p>A popular API might get too many requests at once. </p><p>And this can overload the server and affect its performance. So it’s necessary to rate limit and throttle requests.</p><p> means controlling the number of requests a client can make to an API within a time window. It protects the API from abuse.</p><ol><li><p>The client sends requests through a rate limiter</p></li><li><p>The rate limiter tracks requests from a client by its IP address, user ID, or API key</p></li><li><p>The extra requests get rejected if the limit exceeds (response status code:)</p></li><li><p>The <a href=\"https://en.wikipedia.org/wiki/Token_bucket\">counter</a> then resets after the time window ends</p></li></ol><p> means slowing down requests instead of dropping them. It adds a delay between requests using a queue. Thus preventing server overload.</p><p>Rate limits and throttling keep an API stable. Yet it has to be configured properly for a good user experience.</p><p>There’s a risk of some requests being malformed or malicious. So it’s necessary to validate and sanitize requests before processing them.</p><p> checks the request structure, while  removes unsafe data.</p><ul><li><p>Use schema validation to ensure the request has the expected structure</p></li><li><p>Sanitize requests using proven libraries and tools to block malicious content</p></li></ul><p>This prevents malicious scripts, SQL injections, and malformed data.</p><p> means sending malicious commands to the database via API requests. Think of it like smuggling prohibited items inside a shampoo bottle at airport security.</p><p>Only safe requests should reach the system. So validate and sanitize public APIs.</p><h3>6. Logging and Monitoring</h3><p>Prevention is better than cure.</p><p>Because of this, it’s necessary to track API activity for unusual traffic or suspicious behavior. Think of it like having security cameras and alarms for safety.</p><p>Here are some best practices:</p><ul><li><p>Log metadata, such as IP address, timestamp, and headers; but avoid sensitive data</p></li><li><p>Use dashboards or security information and event management () tools to find anomalies and raise alerts</p></li></ul><p>SIEM collects logs from different services and analyzes them to find suspicious activity.</p><p>Monitoring turns logs into early warnings. So use them to avoid API abuse before it causes damage.</p><p>Subscribe to get simplified case studies delivered straight to your inbox:</p><p>There is a risk of misconfiguration and security vulnerabilities. So it’s necessary to perform security audits regularly.</p><p>A security audit means checking the system to find and fix vulnerabilities before attackers exploit them. Think of it like hiring a locksmith to test all the doors in the house to ensure they lock properly.</p><p>Here are some best practices:</p><ul><li><p><a href=\"https://en.wikipedia.org/wiki/Fuzzing\">Fuzz test</a> APIs with unexpected inputs to catch bugs or failures</p></li><li><p><a href=\"https://en.wikipedia.org/wiki/Stress_testing_(software)\">Stress test</a> APIs by overloading them with many calls to check if they stay reliable</p></li></ul><p>Security audits expose vulnerabilities. So use them to fix issues early.</p><p>Third-party code may contain hidden vulnerabilities.</p><p>And a system is only as secure as its weakest part. So it’s necessary to keep libraries and frameworks up to date.</p><p>Imagine APIs as a city’s traffic control system. If a traffic light fails, accidents might happen.</p><p>Here are some best practices:</p><ul><li><p>Patch security problems early</p></li><li><p>Remove unused dependencies</p></li></ul><p>Ready for the best technique?</p>","contentLength":7314,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/d33057e0-6e03-4d9f-9062-7546c44ce0ea_1280x720.png","enclosureMime":"","commentsUrl":null},{"title":"7 Best Practices for API Design 🔥","url":"https://newsletter.systemdesign.one/p/best-practices-for-api-design","date":1757242837,"author":"Neo Kim","guid":77,"unread":true,"content":"<p>Get my system design playbook for FREE on newsletter signup:</p><p><em>This post outlines best practices for API design. You will find references at the bottom of this page if you want to go deeper.</em></p><p>Once upon a time, there was a tiny startup.</p><p>They had only a few customers.</p><p>So they ran their site using a simple monolith architecture.</p><p>But one day, their site became extremely popular.</p><p>So they set up a microservices architecture for scalability.</p><p>Yet they didn’t know much about API design.</p><p>And their public APIs often failed from spiky traffic.</p><p>Also they sent the entire dataset when users asked for just the latest data.</p><p>Besides it became difficult to manage APIs as they added more features.</p><p>So they decided to study the best practices to design APIs.</p><p>Still using a copy-paste website?  is the design-first, no-code website builder that lets anyone ship a production-ready site in minutes. Whether you’re starting with a template or a blank canvas, Framer gives you total creative control—no coding required. Add animations, localize with one click, and collaborate in real-time with your whole team. You can even A/B test and track clicks with built-in analytics.</p><p>Ready to build a site that looks hand-coded—without hiring a developer? Launch your site for free at Framer dot com, and use code  for a free month on Framer Pro.</p><h2>Best Practices for API Design</h2><p>Here’s what they learned:</p><p>REST stands for Representational State Transfer.</p><p>It's an architectural pattern for systems to talk with each other over the Internet. It helps to organize the data simply and clearly.</p><p>Think of REST like a book library.</p><p>There is just one entrance to borrow or return books. While books get organized into different sections by subject.</p><p>REST APIs organize data into resources similarly.</p><ul><li><p>Each data resource gets represented by an API endpoint. </p><ul><li><p>For example,  represents a collection of items.</p></li><li><p>While  represents a specific item.</p></li></ul></li><li><p>The standard HTTP methods allow interaction with the data.</p><ul><li><p>: lets you read data from the resource.</p></li><li><p>: lets you add a new item to the resource.</p></li><li><p>: lets you replace an item with newer data.</p></li><li><p>: lets you remove an item from the resource.</p></li></ul></li></ul><p>This approach makes the APIs predictable and easy to interact with.</p><p>Yet it might be limiting for some real-world actions, such as publishing a draft of a document. So it’s necessary to have a balance between REST principles and a pragmatic approach.</p><p>A status code tells the client whether a request succeeded or failed.</p><p>API Error handling means returning clear, consistent error messages if something goes wrong. Thus making it easy for the client to handle the error properly.</p><p>Imagine you’re at an airport.</p><p>A bad API error is like the departure board displaying random errors when flight time changes. It doesn’t tell you which flight, why, or what to do next.</p><p>While a good API error is like displaying the flight name and its reason behind time changes.</p><ul><li><p>The response status code tells whether a client error or a server error occurred.</p></li><li><p>And response body includes extra details about what went wrong.</p></li></ul><p>Proper error handling in APIs makes a site reliable.</p><p>Yet it needs extra effort and discipline to handle each failure scenario. Besides error message must include only enough information to avoid security risks.</p><p>Ready for the next technique?</p><p>It’s necessary to version an API so new changes don’t break existing clients.</p><p>Think of API versioning like publishing a new book edition.</p><p>The old book copies that have already been sold don’t get updated. But the new edition gets released with improvements. So readers who need the updates can get the latest edition. And those using the old one can read it without issues.</p><ul><li><p>The API URL path includes the version number.</p><ul><li><p>First version → <code>https://app.com/v1/subpath</code></p></li><li><p>Second version → <code>https://app.com/v2/subpath</code></p></li></ul></li><li><p>The old version doesn't get updated, while new changes happen on the latest version.</p></li></ul><p>Thus ensuring backward compatibility.</p><p>Here are 2 alternative approaches to implement API versioning:</p><ul><li><ul><li><p>Client includes the version in the Accept header.</p></li></ul><ul><li><p><code>Accept: application/vnd.api.v1+json</code></p></li><li><p>It means API version 1 in JSON format.</p></li></ul></li><li><ul><li><p>Client includes the version parameter in the URL.</p></li></ul><ul><li><p><code>https://app.com/subpath?version=1</code></p></li></ul></li></ul><p>But query parameters are designed for filtering or searching data. So it's better to avoid them for versioning. </p><p>API versioning makes a site reliable. Yet it increases the complexity and maintenance efforts. So use it only for breaking changes.</p><p>Rate limiting means controlling the number of requests a client can make to an API within a period. It prevents server overload and protects the API from abuse.</p><p>Imagine rate limiting as telling a person how many things they can have during a giveaway.</p><ol><li><p>The client sends requests to the server through the rate limiter.</p></li><li><p>The rate limiter tracks the number of requests from each client using a <a href=\"https://en.wikipedia.org/wiki/Token_bucket\">counter</a>.</p></li><li><p>The rate limiter rejects a request in case it exceeds the limit.</p></li></ol><p>The API includes special HTTP headers in the response about the rate-limiting rules.</p><ul><li><p> → total requests allowed.</p></li><li><p> → requests left before being rate-limited.</p></li><li><p> → time when the rate-limiting counter resets.</p></li></ul><p>While the API returns a  status code after the client exceeds the limit.</p><p>Rate limits ensure the high availability of a site. But it’s necessary to set a reasonable rate limit for a better user experience.</p><p>Besides many people might share the same IP address. So it’s better to rate limit a person using an API key instead of their IP address for accuracy and fairness.</p><p>API pagination is the technique of breaking a large dataset into smaller chunks. </p><p>It lets the client request data in smaller parts instead of all at once. Thus achieving low latency and reduced bandwidth usage.</p><p>Imagine a big book with 1,000 pages. Pagination is like reading through it one page at a time, instead of all at once.</p><p>There are 2 ways to implement pagination:</p><p>It’s the most common technique for interacting with a dataset in pages.</p><ol><li><p>The server queries the database to find the total number of items and pages.</p></li></ol><pre><code>SELECT COUNT (*) AS total\nFROM items;</code></pre><ol start=\"2\"><li><p>The client specifies the page number and the number of items per page in the request.</p><ul><li><p>For example, <code>https://app.com/subpath?page=3&amp;limit=20</code> → skip the first 40 items and return the next 20</p></li></ul></li></ol><pre><code>SELECT *\nFROM items\nORDER BY created_at DESC\nLIMIT 20 OFFSET 40;</code></pre><ol start=\"3\"><li><p>The server responds along with pagination metadata.</p></li></ol><pre><code>{\n    \"data\": [{...}],\n    \"paging\": {\n        \"total\": 300,   // total items\n        \"page\": 3,      // current page\n        \"pages\": 15     // total pages\n    }\n}</code></pre><p>Offset pagination is simple to understand and easy to implement. Also it lets the client access random pages.</p><p>But it’s slow with large datasets because the database scans the skipped records as well. Besides there’s a risk of inconsistent results if data gets added or removed during paging.</p><p>It uses a pointer () to a specific record in the dataset. It means the server returns results after the cursor instead of skipping records.</p><ol><li><p>A unique sequential field, such as ID or timestamp, gets chosen as the cursor.</p></li><li><p>The client then includes the cursor parameter in the request.</p><ul><li><p><code>https://app.com/subpath?cursor=abc&amp;limit=20</code></p></li></ul></li><li><p>The server fetches the next 20 items after the cursor using the database index.</p></li></ol><pre><code>SELECT *\nFROM items\nWHERE created_at &lt; %cursor\nORDER BY created_at DESC\nLIMIT 21; -- one extra to find the next cursor</code></pre><ol start=\"4\"><li><p>The server includes the new cursor as well in the response for future requests.</p></li></ol><pre><code>{\n    \"data\": [{...}],\n    \"next_cursor\": \"cN315672McETHy\"\n}</code></pre><p>Cursor pagination doesn't scan skipped records; instead, it uses the database index. Thus making it efficient for large datasets.</p><p>Also it gives consistent results even if data gets added or removed during paging. But it’s complex to implement and doesn’t support navigation to a specific page.</p><p>So choose the pagination technique based on the data size and data update frequency.</p><p>Ready for the best technique?</p><p>Idempotency means processing a request only once to avoid unwanted side effects.</p><p>Imagine paying for something online. And mistakenly getting charged twice for the same transaction.</p><p>An idempotent API prevents this problem.</p><ol><li><p>The client generates a unique string (<a href=\"https://en.wikipedia.org/wiki/Universally_unique_identifier\">UUID</a>) to use as the idempotency key.</p></li><li><p>The client includes this key in the HTTP request header.</p></li><li><p>The server processes the request and caches its response.</p></li><li><p>Then it stores the idempotency key on an in-memory database.</p></li></ol><p>And the in-memory database gets queried to check if a future request has been processed already.</p><p>Also the client generates a new UUID whenever the request payload changes. Thus ensuring only new requests get processed.</p><p>Here are 2 alternative approaches to implement idempotency:</p><ul><li><p>Use database constraints, such as unique keys, to prevent duplicates.</p></li><li><p>Use a message queue with built-in deduplication.</p></li></ul><p>Idempotency is essential for building reliable APIs. Yet it increases the complexity and operational costs. Besides the idempotency keys should be removed periodically to reduce memory usage. So use it mainly for critical APIs where retries could cause unwanted side effects.</p><p>API filtering means returning results that match specific conditions in request parameters.</p><ol><li><p>The client includes filter parameters in the request URL.</p><ul><li><p><code>https://app.com/subpath?type=value</code></p></li></ul></li><li><p>The server reads the parameters.</p></li><li><p>Then it queries the database with those conditions.</p></li><li><p>The server returns only the matching records.</p></li></ol><p>Filtering improves performance by reducing bandwidth usage. But it increases the complexity of server logic. So use it only if specific data from a large dataset is needed.</p><p>API sorting means ordering the results by a specific field, using request parameters.</p><ol><li><p>The client includes sorting parameters in the request URL.</p><ul><li><p><code>https://app.com/subpath?sort=value&amp;order=asc</code></p></li></ul></li><li><p>The server reads those parameters.</p></li><li><p>It then creates a database query with the ORDER BY clause.</p></li><li><p>The server returns the results sorted by the requested field.</p></li></ol><p>Sorting improves usability by ordering data. But it might slow down queries on large datasets without indexes. So use it only if ordering is important.</p><p>The internet runs using APIs.</p><p>While good APIs are consistent, predictable, and designed to scale without problems.</p><p>So it's necessary to have clear documentation, useful error messages, and proper monitoring for APIs.</p><p>Subscribe to get simplified case studies delivered straight to your inbox:</p><p><strong>Want to advertise in this newsletter? </strong>📰</p><p>Thank you for supporting this newsletter. Consider sharing this post with your friends and get rewards. Y’all are the best.</p>","contentLength":10367,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/e3cb7404-ab3d-46fe-b216-51f11da01543_1280x720.png","enclosureMime":"","commentsUrl":null}],"tags":["dev"]}