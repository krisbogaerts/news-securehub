{"id":"VMK7D8tQvwsLtnxKHzNdF9c4Yhtuqm","title":"Hacker News: Best","displayTitle":"Best of HackerNews","url":"https://hnrss.org/best","feedLink":"https://news.ycombinator.com/best","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":30,"items":[{"title":"Signal Protocol and Post-Quantum Ratchets","url":"https://signal.org/blog/spqr/","date":1759421170,"author":"pluto_modadic","guid":113,"unread":true,"content":"<p>We are excited to announce a significant advancement in the security of the Signal Protocol: the introduction of the Sparse Post Quantum Ratchet (SPQR). This new ratchet enhances the Signal Protocol’s resilience against future quantum computing threats while maintaining our existing security guarantees of forward secrecy and post-compromise security.</p><p>The Signal Protocol is a set of cryptographic specifications that provides end-to-end encryption for private communications exchanged daily by billions of people around the world. After its publication in 2013, the open source Signal Protocol was adopted not only by the Signal application but also by other major messaging products. Technical information on the Signal Protocol can be found in the specifications section of our <a href=\"https://signal.org/docs/\">docs</a> site.</p><p>In a <a href=\"https://signal.org/blog/pqxdh/\">previous blog post</a>, we announced the first step towards advancing quantum resistance for the Signal Protocol: an upgrade called PQXDH that incorporates quantum-resistent cryptographic secrets when chat sessions are established in order to protect against <a href=\"https://en.wikipedia.org/wiki/Harvest_now%2C_decrypt_later\">harvest-now-decrypt-later</a> attacks that could allow current chat sessions to become compromised if a sufficiently powerful quantum computer is developed in the future. However, the Signal Protocol isn’t just about protecting cryptographic material and keys at the beginning of a new chat or phone call; it’s also designed to minimize damage and heal from compromise as that conversation continues.</p><p>We refer to these security goals as Forward Secrecy (FS) and Post-Compromise Security (PCS). FS and PCS can be considered mirrors of each other: FS protects past messages against future compromise, while PCS protects future messages from past compromise. Today, we are happy to announce the next step in advancing quantum resistance for the Signal Protocol: an additional regularly advancing post-quantum ratchet called the Sparse Post Quantum Ratchet, or SPQR. On its own, SPQR provides secure messaging that provably achieves these FS and PCS guarantees in a quantum safe manner. We mix the output of this new ratcheting protocol with Signal’s existing Double Ratchet, in a combination we refer to as the Triple Ratchet.</p><p>What does this mean for you as a Signal user? First, when it comes to your experience using the app, nothing changes. Second, because of how we’re rolling this out and mixing it in with our existing encryption, eventually all of your conversations will move to this new protocol without you needing to take any action. Third, and most importantly, this protects your communications both now and in the event that cryptographically relevant quantum computers eventually become a reality, and it allows us to maintain our existing security guarantees of forward secrecy and post-compromise security as we proactively prepare for that new world.</p><h2>The Current State of the Signal Protocol</h2><p>The original Signal ratchet uses hash functions for FS and a set of elliptic-curve Diffie Hellman (ECDH) secret exchanges for PCS. The hash functions are quantum safe, but elliptic-curve cryptography is not. An example is in order: our favorite users, Alice and Bob, establish a long-term connection and chat over it regularly. During that session’s lifetime, Alice and Bob regularly agree on new ECDH secrets and use them to “ratchet” their session. Mean ol’ Mallory records the entire (encrypted) communication, and really wants to know what Alice and Bob are talking about.</p><p>The concept of a “ratchet” is crucial to our current non-quantum FS/PCS protection. In the physical world, a ratchet is a mechanism that allows a gear to rotate forward, but disallows rotation backwards. In the Signal Protocol, it takes on a similar role. When Alice and Bob “ratchet” their session, they replace the set of keys they were using prior with a new set based on both the older secrets and a new one they agree upon. Given access to those new secrets, though, there’s no (non-quantum) way to compute the older secrets. By being “one-way”, this ratcheting mechanism provides FS.</p><p>The ECDH mechanism allows Alice and Bob to generate new, small (32 bytes) data blobs and attach them to every message. Whenever each party receives a message from the other, they can locally (and relatively cheaply) use this data blob to agree on a new shared secret, then use that secret to ratchet their side of the protocol. Crucially, ECDH also allows Alice and Bob to both agree on the new secret without sending that secret itself over their session, and in fact without sending anything over the session that Mallory could use to determine it. <a href=\"https://en.wikipedia.org/wiki/Diffie%E2%80%93Hellman_key_exchange\">This description of Diffie-Hellman key exchange</a> provides more details on the concepts of such a key exchange, and <a href=\"https://en.wikipedia.org/wiki/Elliptic-curve_Diffie%E2%80%93Hellman\">this description of ECDH</a> provides specific details on the variant used by the current Signal protocol.</p><p>Sometime midway through the lifetime of Alice and Bob’s session, Mallory successfully breaches the defences of both Alice and Bob, gaining access to all of the (current) secrets used for their session at the time of request. Alice and Bob should have the benefits of Forward Secrecy - they’ve ratcheted sometime recently before the compromise, so no messages earlier than their last ratchet are accessible to Mallory, since ratcheting isn’t reversible. They also retain the benefits of Post-Compromise Security. Their ratcheting after Mallory’s secret access agrees upon new keys that can’t be gleaned just from the captured data they pass between each other, re-securing the session.</p><p>Should Mallory have access to a quantum computer, though, things aren’t so simple. Because elliptic curve cryptography is not quantum resistant, it’s possible that Mallory could glean access to the secret that Alice and Bob agreed upon, just by looking at the communication between them. Given this, Alice and Bob’s session will never “heal”; Mallory’s access to their network traffic from this point forward will allow her to decrypt all future communications.</p><h2>Mixing In Quantum Security</h2><p>In order to make our security guarantees stand up to quantum attacks, we need to mix in secrets generated from quantum secure algorithms. In PQXDH, we did this by performing an additional round of key agreement during the session-initiating handshake, then mixing the resulting shared secret into the initial secret material used to create Signal sessions. To handle FS and PCS, we need to do continuous key agreement, where over the lifetime of a session we keep generating new shared secrets and mixing those keys into our encryption keys.</p><p>Luckily there is a tool designed exactly for this purpose: the quantum-secure Key-Encapsulation Mechanism (KEM). KEMs share similar behavior to the Diffie-Hellman mechanisms we described earlier, where two clients provide each other with information, eventually deciding on a shared secret, without anyone who intercepts their communications being able to access that secret. However, there is one important distinction for KEMs - they require ordered, asymmetric messages to be passed between their clients. In ECDH, both clients send the other some public parameters, and both combine these parameters with their locally held secrets and come up with an identical shared secret. In the <a href=\"https://csrc.nist.gov/pubs/fips/203/final\">standardized</a> ML-KEM key-encapsulation mechanism, though, the initiating client generates a pair of encapsulation key (EK) and decapsulation key (DK) (analogous to a public and private key respectively) and sends the EK. The receiving client receives it, generates a secret, and wraps it into a ciphertext (CT) with that key. The initiating client receives that CT and decapsulates with its previously generated DK. In the end, both clients have access to the new, shared secret, just through slightly different means.</p><p>Wanting to integrate this quantum-secure key sharing into Signal, we could take a simple, naive approach for each session. When Alice initiates a session with Bob,</p><ul><li>Alice, with every message she sends, sends an EK</li><li>Bob, with every message he receives, generates a secret and a CT, and sends the CT back</li><li>Alice, on receiving a CT, extracts the secret with her DK and mixes it in</li></ul><p>This initially simple-looking approach, though, quickly runs into a number of issues we’ll need to address to make our protocol actually robust. First, encapsulation keys and CTs are large - over 1000 bytes each for ML-KEM 768, compared to the 32 bytes required for ECDH. Second, while this protocol works well when both clients are online, what happens when a client is offline? Or a message is dropped or reordered? Or Alice wants to send 10 messages before Bob wants to send one?</p><p>Some of these problems have well-understood solutions, but others have trade-offs that may shine in certain circumstances but fall short in others. Let’s dive in and come to some conclusions.</p><p>How does Alice decide what to send based on what Bob needs next, and vice versa? If Bob hasn’t received an EK yet, she shouldn’t send the next one. What does Bob send when he hasn’t yet received an EK from Alice, or when he has, but he’s already responded to it? This is a common problem when remote parties send messages to communicate, so there’s a good, well-understood solution: a state machine. Alice and Bob both keep track of “what state am I in”, and base their decisions on that. When sending or receiving a message, they might also change their state. For example:</p><ul><li>Alice wants to send a message, but she’s in a StartingA state, so she doesn’t have an EK. So, she generates an EK/DK pair, stores them locally, and transitions to the SendEK state</li><li>Alice wants to send a message and is in the SendEK state. She sends the EK along with the message</li><li>Alice wants to send another message, but she’s still in the SendEK state. So, she sends the EK with the new message as well</li><li>Bob receives the message with the EK. He generates a secret and uses the EK to create a CT. He transitions to the SendingCT state.</li><li>Bob wants to send a message and he’s in the SendingCT state. He sends the CT along with the message</li><li>Bob wants to send a message and he’s in the SendingCT state. He sends the CT along with the message</li></ul><p>By crafting a set of states and transitions, both sides can coordinate what’s sent. Note, though, that even in this simple case, we see problems. For example, we’re sending our (large) EK and (large) CT multiple times.</p><p>We’ve already mentioned that the size of the data we’re sending has increased pretty drastically, from 32 bytes to over 1000 per message. But bandwidth is expensive, especially on consumer devices like client phones, that may be anywhere in the world and have extremely varied costs for sending bytes over the wire. So let’s discuss strategies for conserving that bandwidth.</p><p>First, the simplest approach - don’t send a new key with every message. Just, for example, send with every 50 messages, or once a week, or every 50 messages unless you haven’t sent a key in a week, or any other combination of options. All of these approaches tend to work pretty well in online cases, where both sides of a session are communicating in real-time with no message loss. But in cases where one side is offline or loss can occur, they can be problematic. Consider the case of “send a key if you haven’t sent one in a week”. If Bob has been offline for 2 weeks, what does Alice do when she wants to send a message? What happens if we can lose messages, and we lose the one in fifty that contains a new key? Or, what happens if there’s an attacker in the middle that wants to stop us from generating new secrets, and can look for messages that are 1000 bytes larger than the others and drop them, only allowing keyless messages through?</p><p>Another method is to chunk up a message. Want to send 1000 bytes? Send 10 chunks of 100 bytes each. Already sent 10 chunks? Resend the first chunk, then the second, etc. This smooths out the total number of bytes sent, keeping individual message sizes small and uniform. And often, loss of messages is handled. If chunk 1 was lost, just wait for it to be resent. But it runs into an issue with message loss - if chunk 99 was lost, the receiver has to wait for all of chunks 1-98 to be resent before it receives the chunk it missed. More importantly, if a malicious middleman wants to stop keys from being decided upon, they could always drop chunk 3, never allowing the full key to pass between the two parties.</p><p>We can get around all of these issues using a concept called erasure codes. Erasure codes work by breaking up a larger message into smaller chunks, then sending those along. Let’s consider our 1000 byte message being sent as 100 byte chunks again. After chunk #10 has been sent, the entirety of the original 1000 byte message has been sent along in cleartext. But rather than just send the first chunk over again, erasure codes build up a new chunk #11, and #12, etc. And they build them in such a way that, once the recipient receives any 10 chunks in any order, they’ll be able to reconstruct the original 1000 byte message.</p><p>When we put this concept of erasure code chunks together with our previous state machine, it gives us a way to send large blocks of data in small chunks, while handling messages that are dropped. Crucially, this includes messages dropped by a malicious middleman: since any N chunks can be used to recreate the original message, a bad actor would need to drop all messages after #N-1 to disallow the data to go through, forcing them into a complete (and highly noticeable) denial of service. Now, if Alice wants to send an EK to Bob, Alice will:</p><ol><li>Transition from the StartingA state to the SendingEK state, by generating a new EK and chunking it</li><li>While in the SendingEK state, send a new chunk of the EK along with any messages she sends</li><li>When she receives confirmation that the recipient has received the EK (when she receives a chunk of CT), transition to the ReceivingCT state</li></ol><ol><li>Transition from the StartingB state to the ReceivingEK state when he receives its first EK chunk</li><li>Keep receiving EK chunks until he has enough to reconstruct the EK</li><li>At that point, reconstruct the EK, generate the CT, chunk the CT, and transition to the SendingCT state</li><li>From this point on, he will send a chunk of the CT with every message</li></ol><p>One interesting way of looking at this protocol so far is to consider the messages flowing from Alice to Bob as potential capacity for sending data associated with post-quantum ratcheting: each message that we send, we could also send additional data like a chunk of EK or of the CT. If we look at Bob’s side, above, we notice that sometimes he’s using that capacity (IE: in step 4 when he’s sending CT chunks) and sometimes he’s not (if he sends a message to Alice during step 2, he has no additional data to send). This capacity is pretty limited, so using more of it gives us the potential to speed up our protocol and agree on new secrets more frequently.</p><h2>A Meditation On How Faster Isn’t Always Better</h2><p>We want to generate shared secrets, then use them to secure messages. So, does that mean that we want to generate shared secrets as fast as possible? Let’s introduce a new term: an epoch. Alice and Bob start their sessions in epoch 0, sending the EKs for epoch 1 (EK#1) and associated ciphertext (CT#1) to each other. Once that process completes, they have a new shared secret they use to enter epoch 1, after which all newly sent messages are protected by the new secret. Each time they generate a new shared secret, they use it to enter a new epoch. Surely, every time we enter a new epoch with a new shared secret, we protect messages before that secret (FS) and after that secret (PCS), so faster generation is better? It seems simple, but there’s an interesting complexity here that deserves attention.</p><p>First, let’s discuss how to do things faster. Right now, there’s a lot of capacity we’re not using: Bob sends nothing while Alice sends an EK, and Alice sends nothing while Bob sends a CT. Speeding this up isn’t actually that hard. Let’s change things so that Alice sends EK#1, and once Bob acknowledges its receipt, Alice immediately generates and sends EK#2. And once she notices Bob has received that, she generates and sends EK#3, etc. Whenever Alice sends a new message, she always has data to send along with it (new EK chunks), so she’s using its full capacity. Bob doesn’t always have a new CT to send, but he is receiving EKs as fast as Alice can send them, so he often has a new CT to send along.</p><p>But now let’s consider what happens when an attacker gains access to Alice. Let’s say that Alice has sent EK#1 and EK#2 to Bob, and she’s in the process of sending EK#3. Bob has acknowledged receipt of EK#1 and EK#2, but he’s still in the process of sending CT#1, since in this case Bob sends fewer messages to Alice than vice versa. Because Alice has already generated 3 EKs she hasn’t used, Alice needs to keep the associated DK#1, DK#2, and DK#3 around. So, if at this point someone maliciously gains control of Alice’s device, they gain access to both the secrets associated with the current epoch (here, epoch 0) and to the DKs necessary to reconstruct the secrets to other epochs (here, epochs 1, 2, and 3) using only the over-the-wire CT that Bob has yet to send. This is a big problem: by generating secrets early, we’ve actually made the in-progress epochs and any messages that will be sent within them less secure against this single-point-in-time breach.</p><p>To test this out, we at Signal built a number of different state machines, each sending different sets of data either in parallel or serially. We then ran these state machines in numerous simulations, varying things like the ratio of messages sent by Alice vs Bob, the amount of data loss or reordering, etc. And while running these simulations, we tracked what epochs’ secrets were exposed at any point in time, assuming an attacker were to breach either Alice’s or Bob’s secret store. The results showed that, in general, while simulations that handled multiple epochs’ secrets in parallel (IE: by sending EK#2 before receipt of CT#1) did generate new epochs more quickly, they actually made more messages vulnerable to a single breach.</p><h2>But Let’s Still Be Efficient</h2><p>This still leaves us with a problem, though: the capacity present in messages we send in either direction is still a precious resource, and we want to use it as efficiently as possible. And our simple approach of Alice’s “send EK, receive CT, repeat” and Bob’s “receive EK, send CT, repeat” leaves lots of time where Alice and Bob have nothing to send, should that capacity be available.</p><p>To improve our use of our sending capacity, we decided to take a harder look into the ML-KEM algorithm we’re using to share secrets, to see if there was room to improve. Let’s break things down more and share some actual specifics on the ML-KEM algorithm.</p><ol><li>Alice generates an EK of 1184 bytes to send to Bob, and an associated DK</li><li>Bob samples a new shared secret (32 bytes), which he encrypts with EK into a CT of 1088 bytes to send to Alice</li><li>Alice receives the CT, uses the DK to decrypt it, and now also has access to the 32 byte shared secret</li></ol><p>Diving in further, we can break out step #3 into some sub-steps</p><ol><li>Alice generates an EK of 1184 bytes to send to Bob, and an associated DK</li><li>Bob samples a new shared secret (32 bytes), which he encrypts with EK into a CT of 1088 bytes to send to Alice<ol type=\"a\"><li> Bob creates a new shared secret S and sampled randomness R by sampling entropy and combining it with a hash of EK</li><li>Bob hashes the EK into a Hash</li><li>Bob pulls 32 bytes of the EK, a Seed</li><li>Bob uses the Seed and R to generate the majority of the CT</li><li>Bob then uses S and EK to generate the last portion of the CT</li></ol></li><li>Alice receives the CT, uses the DK to decrypt it, and now also has access to the 32 byte shared secret</li></ol><p>Step 3.d, which generates 960 bytes of the 1088-byte CT, only needs 64 bytes of input: a Seed that’s 32 of EK’s bytes, and the hash of EK, which is an additional 32. If we combine these values and send them first, then most of EK and most of the CT can be sent in parallel from Alice to Bob and Bob to Alice respectively. Our more complicated but more efficient secret sharing now looks like this:</p><ol><li>Alice generates EK and DK. Alice extracts the 32-byte Seed from EK</li><li>Alice sends 64 bytes EK (Seed + Hash(EK)) to Bob. Bob sends nothing during this time.</li><li>Bob receives the Seed and Hash, and generates the first, largest part of the CT from them (CT)</li><li>After this point, Alice sends EK (the rest of the EK minus the Seed), while Bob sends CT</li><li>Bob eventually receives EK, and uses it to generate the final portion of the CT (CT)</li><li>Once Alice tells Bob that she has received all of CT, Bob sends Alice CT. Alice sends nothing during this time.</li><li>With both sides having all of the pieces of EK and the CT that they need, they extract their shared secret and increment their epoch</li></ol><p>There are still places in this algorithm (specifically steps 2 and 6) where one side has nothing to send. But during those times, the other side has only a very small amount of information to send, so the duration of those steps is minimal compared to the rest of the process. Specifically, while the full EK is 37 chunks and the full CT is 34, the two pieces of the new protocol which must be sent without data being received (EK and CT) are 2 and 4 chunks respectively, while the pieces that can be sent while also receiving (EK and CT) are the bulk of the data, at 36 and 30 chunks respectively. Far more of our sending capacity is actually used with this approach.</p><p>Remember that all of this is just to perform a quantum-safe key exchange that gives us a secret we can mix into the bigger protocol. To help us organize our code, our security proofs, and our understanding better we treat this process as a standalone protocol that we call <a href=\"https://signal.org/docs/specifications/mlkembraid/\">the ML-KEM Braid</a>.</p><p>This work was greatly aided by the authors of the <a href=\"https://crates.io/crates/libcrux-ml-kem\">libcrux-ml-kem</a> Rust library, who graciously exposed the APIs necessary to work with this incremental version of ML-KEM 768. With this approach completed, we’ve been able to really efficiently use the sending capacity of messages sent between two parties to share secrets as quickly as possible without exposing secrets from multiple epochs to potential attackers.</p><h2>Mixing Things Up - The Triple Ratchet</h2><p>There are plenty of details to add to make sure that we reached every corner - check those out in our <a href=\"https://signal.org/docs/\">online protocol documentation</a> - but this basic idea lets us build secure messaging that has post-quantum FS and PCS without using up anyone’s data. We’re not done, though! Remember, at the beginning of this post we said we wanted post-quantum security without taking away our existing guarantees.</p><p>While today’s Double Ratchet may not be quantum safe, it provides a high level of security today and we believe it will continue to be strong well into the future. We aren’t going to take that away from our users. So what can we do?</p><p>Our answer ends up being really simple: we run both the Double Ratchet and the Sparse Post Quantum Ratchet alongside each other and mix their keys together, into what we’re calling the Triple Ratchet protocol. When you want to send a message you ask both the Double Ratchet and SPQR “What encryption key should I use for the next message?” and they will both give you a key (along with some other data you need to put in a message header). Instead of either key being used directly, both are passed into a Key Derivation Function - a special function that takes random-enough inputs and produces a secure cryptographic key that’s as long as you need. This gives you a new “mixed” key that has hybrid security. An attacker has to break both our elliptic curve and ML-KEM to even be able to distinguish this key from random bits. We use that mixed key to encrypt our message.</p><p>Receiving messages is just as easy. We take the message header - remember it has some extra data in it - and send it to the Double Ratchet and SPQR and ask them “What key should I use to decrypt a message with this header?” They both return their keys and you feed them both into that Key Derivation Function to get your decryption key. After that, everything proceeds just like it always has.</p><p>So we’ve got this new, snazzy protocol, and we want to roll it out to all of our users across all of their devices… but none of the devices currently support that protocol. We roll it out to Alice, and Alice tries to talk to Bob, but Alice speaks SPQR and Bob doesn’t. Or we roll it out to Bob, but Alice wants to talk to Bob and Alice doesn’t know the new protocol Bob wants to use. How do we make this work?</p><p>Let’s talk about the simplest option: allowing downgrades. Alice tries to establish a session with Bob using SPQR and sends a message over it. Bob fails to read the message and establish the session, because Bob hasn’t been upgraded yet. Bob sends Alice an error, so Alice has to try again. This sounds fine, but in practice it’s not tenable. Consider what happens if Alice and Bob aren’t online at the same time… Alice sends a message at 1am, then shuts down. Bob starts up at 3am, sends an error, then shuts down. Alice gets that error when she restarts at 5am, then resends. Bob starts up at 7am and finally gets the message he should have received at 3am, 4 hours behind schedule.</p><p>To handle this, we designed the SPQR protocol to allow itself to downgrade to not being used. When Alice sends her first message, she attaches the SPQR data she would need to start up negotiation of the protocol. Noticing that downgrades are allowed for this session, Alice doesn’t mix any SPQR key material into the message yet. Bob ignores that data, because it’s in a location he glosses over, but since there’s no mixed in keys yet, he can still decrypt the message. He sends a response that lacks SPQR data (since he doesn’t yet know how to fill it in), which Alice receives. Alice sees a message without SPQR data, and understands that Bob doesn’t speak SPQR yet. So, she downgrades to not using it for that session, and they happily talk without SPQR protection.</p><p>There’s some scary potential problems here… let’s work through them. First off, can a malicious middleman force a downgrade and disallow Alice and Bob from using SPQR, even if both of them are able to? We protect against that by having the SPQR data attached to the message be MAC’d by the message-wide authentication code - a middleman can’t remove it without altering the whole message in such a way that the other party sees it, even if that other party doesn’t speak SPQR. Second, could some error cause messages to accidentally downgrade sometime later in their lifecycle, due either to bugs in the code or malicious activity? Crucially, SPQR only allows a downgrade when it first receives a message from a remote party. So, Bob can only downgrade if he receives his first message from Alice and notices that she doesn’t support SPQR, and Alice will only downgrade if she receives her first reply from Bob and notices that he doesn’t. After that first back-and-forth, SPQR is locked in and used for the remainder of the session.</p><p>Finally, those familiar with Signal’s internal workings might note that Signal sessions last a really long time, potentially years. Can we ever say “every session is protected by SPQR”, given that SPQR is only added to new sessions as they’re being initiated? To accomplish this, Signal will eventually (once all clients support the new protocol) roll out a code change that enforces SPQR for all sessions, and that archives all sessions which don’t yet have that protection. After the full rollout of that future update, we’ll be able to confidently assert complete coverage of SPQR.</p><p>One nice benefit to setting up this “maybe downgrade if the other side doesn’t support things” approach is that it also sets us up for the future: the same mechanisms that allow us to choose between SPQR or no-SPQR are designed to also allow us to upgrade from SPQR to some far-future (as yet unimagined) SPQRv2.</p><h2>Making Sure We Get It Right</h2><p>Complex protocols require extraordinary care. We have to ensure that the new protocol doesn’t lose any of the security guarantees the Double Ratchet gives us. We have to ensure that we actually get the post-quantum protection we’re aiming for. And even then, after we have full confidence in the protocol, we have to make sure that our implementation is correct and robust and stays that way as we maintain it. This is a tall order.</p><p>To make sure we got this right, we started by building the protocol on a firm foundation of fundamental research. We built on the years of research the academic community has put into secure messaging and we collaborated with researchers from PQShield, AIST, and NYU to explore what was possible with post-quantum secure messaging. In <a href=\"https://eprint.iacr.org/2025/078\">a paper at Eurocrypt 25</a> we introduced erasure code based chunking and proposed the high-level Triple Ratchet protocol, proving that it gave us the post-quantum security we wanted without taking away any of the security of the classic Double Ratchet. In <a href=\"https://www.usenix.org/system/files/usenixsecurity25-auerbach.pdf\">a follow up paper at USENIX 25</a>, we observed that there are many different ways to design a post-quantum ratchet protocol and we need to pick the one that protects user messages the best. We introduced and analyzed six different protocols and two stood out: one is essentially SPQR, the other is a protocol using a new KEM, called Katana, that we designed just for ratcheting. That second one is exciting, but we want to stick to standards to start!</p><p>As we kept finding better protocol candidates - and we implemented around a dozen of them - we modeled them in <a href=\"https://bblanche.gitlabpages.inria.fr/proverif/\">ProVerif</a> to prove that they had the security properties we needed. Rather than wrapping up a protocol design and performing formal verification as a last step we made it a core part of the design process. Now that the design is settled, this gives us machine verified proof that our protocol has the security properties we demand from it. We wrote our Rust code to closely match the ProVerif models, so it is easy to check that we’re modeling what we implement. In particular, ProVerif is very good at reasoning about state machines, which we’re already using, making the mapping from code to model much simpler.</p><p>We are taking formal verification further than that, though. We are using <a href=\"https://github.com/cryspen/hax\">hax</a> to translate our Rust implementation into <a href=\"https://fstar-lang.org/\">F*</a> on every CI run. Once the F* models are extracted, we prove that core parts of our highly optimized implementation are correct, that function pre-conditions and post-conditions cannot be violated, and that the entire crate is panic free. That last one is a big deal. It is great for usability, of course, because nobody wants their app to crash. But it also matters for correctness. We aggressively add assertions about things we believe must be true when the protocol is running correctly - and we crash the app if they are false. With hax and F*, we prove that those assertions will never fail.</p><p>Often when people think about formally verified protocol implementations, they imagine a one-time huge investment in verification that leaves you with a codebase frozen in time. This is not the case here. We re-run formal verification in our CI pipeline every time a developer pushes a change to GitHub. If the proofs fail then the build fails, and the developer needs to fix it. In our experience so far, this is usually as simple as adding a pre- or postcondition or returning an error when a value is out of bounds. For us, formal verification is a dynamic part of the development process and ensures that the quality is high on every merge.</p><p>Signal is rolling out a new version of the Signal Protocol with the Triple Ratchet. It adds the Sparse Post-Quantum Ratchet, or SPQR, to the existing Double Ratchet to create a new Triple Ratchet which gives our users quantum-safe messaging without taking away any of our existing security promises. It’s being added in such a way that it can be rolled out without disruption. It’s relatively lightweight, not using much additional bandwidth for each message, to keep network costs low for our users. It’s resistant to meddling by malicious middlemen - to disrupt it, all messages after a certain time must be dropped, causing a noticeable denial of service for users. We’re rolling it out slowly and carefully now, but in such a way that we’ll eventually be able to say with confidence “every message sent by Signal is protected by this.” Its code has been formally verified, and will continue to be so even as future updates affect the protocol. It’s the combined effort of Signal employees and external researchers and contributors, and it’s only possible due to the continued work and diligence of the larger crypto community. And as a user of Signal, our biggest hope is that you never even notice or care. Except one day, when headlines scream “OMG, quantum computers are here”, you can look back on this blog post and say “oh, I guess I don’t have to care about that, because it’s already been handled”, as you sip your Nutri-Algae while your self-driving flying car wends its way through the floating tenements of Megapolis Prime.</p>","contentLength":33225,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45451527"},{"title":"How Israeli actions caused famine in Gaza, visualized","url":"https://www.cnn.com/2025/10/02/middleeast/gaza-famine-causes-vis-intl","date":1759397030,"author":"nashashmi","guid":112,"unread":true,"content":"<p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6fqcqv004o28qj5jva96b5@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Israel’s nearly two-year war pushed parts of Gaza into <a href=\"https://www.cnn.com/2025/08/22/middleeast/famine-officially-taking-place-in-gaza-says-un-backed-group-intl\">“man-made” famine</a>, according to a report published in August by a United Nations-backed initiative, deepening the Palestinians’ <a href=\"https://www.cnn.com/2025/09/09/world/video/gaza-starvation-israel-blockade-digvid\">struggle for survival</a> under relentless bombing, mass displacement and the spread of disease.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6ghafh000c356ns8msevrm@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The report by the Integrated Food Security Phase Classification (IPC), a UN-backed expert panel that assesses global food insecurity and malnutrition, helped to fuel growing international outcry over Israel’s campaign in Gaza following the Hamas-led October 7, 2023, attacks –and was cited by some of countries that recently made moves towards formally recognizing a Palestinian state. The IPC forecast that by the end of September nearly a third of Gaza’s total population would face famine conditions, although it has not yet provided an update on that forecast.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6ghafh000d356njiblv5k7@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            In Gaza governorate alone – the largest by population of five in the Gaza Strip – more than half a million people were condemned to a cycle of “starvation, destitution and death,” the IPC added. The Israeli assault on Gaza City, which Israeli Prime Minister Benjamin Netanyahu says is targeting one of Hamas’ <a href=\"https://www.cnn.com/2025/08/10/middleeast/israel-netanyahu-defends-gaza-city-plan-intl-latam-hnk\">“remaining strongholds”</a> has choked relief operations for starving Palestinians, according to rights workers.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6ghafh000e356negowim3d@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Michael Fakhri, the UN’s special rapporteur on the right to food, accused Israel of using hunger “as a weapon against Palestinians,” in violation of international law.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6ghafh000f356nzas8a8wh@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            “Israel is using food and aid as a weapon to humiliate, weaken, displace and kill Palestinians in Gaza,” Fakhri told CNN on August 28.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6ghafh000g356nqhfk9k4f@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Israel<a href=\"https://www.gov.il/en/pages/mfa-response-to-ipc-report-22-aug-2025\" target=\"_blank\"> rejected</a> the IPC’s findings, with the Israeli agency that oversees the entry of aid into Gaza claiming the report was“false” and based on “partial, biased” data “originating from Hamas.” Netanyahu slammed the UN-backed report, in a statement from <a href=\"https://x.com/IsraeliPM/status/1958889579521495144\" target=\"_blank\">his office</a>, adding that “Israel does not have a policy of starvation.”\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg85yzob00003b6sbnkeq235@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Israel has since insisted that it has stepped up the entry of aid into Gaza. But aid agencies say that Israel’s intensification of the war, particularly around Gaza City, has compounded the misery faced by Palestinians. Here is a look, in five charts, at how the situation described by the IPC materialized.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6fql6b0000356n1y8bkrz8@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\"><a href=\"https://www.ipcinfo.org/ipcinfo-website/countries-in-focus-archive/issue-134/en/\" target=\"_blank\">The IPC projected </a>that famine would spread to Deir Al-Balah, central Gaza and further south, in Khan Younis by the end of September, affecting nearly 641,000 people.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6gmigj000s356n8zyogpu9@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Up to June 2026, at least 132,000 children under the age of five are expected to suffer from acute malnutrition, including more than 41,000 severe cases of children at heightened risk of death, the IPC added.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6gmigj000t356nyq6gmaxz@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Under the IPC – a five-phase indicator used to measure the severity of food insecurity – a famine can only be declared if three thresholds <a href=\"https://www.ipcinfo.org/famine-facts/\" target=\"_blank\">are met</a>: at least 20% of households face extreme food shortages, the proportion of children assessed as acutely malnourished reaches a certain threshold, and at least two in every 10,000 people die each day from starvation, or from malnutrition and disease.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6gmigj000u356np5eubw22@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Israel accused the IPC of lowering the second threshold of acutely malnourished children for a famine declaration, which the IPC has denied.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6gmigj000v356np6xutcez@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Researchers use three methods for assessing child malnutrition – either a child’s height and weight, their BMI, or a child’s mid-upper arm circumference, known as MUAC. The IPC used the latter, a metric employed since 2019, to determine that at least 15% of children aged six to 59 months have a mid-upper arm circumference of less than 125mm or edema, the agency told CNN. The thresholds for famine classification are “standard and were not modified for Gaza,” the IPC told CNN, adding that the MUAC metric “is the measurement most frequently available and has strong correlation with mortality outcomes,” and was also used in famine classifications in Sudan and South Sudan this decade.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6gmigj000w356nhsbbxr1m@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Human rights advocates say Israel’s destruction of health infrastructure and intensified hostilities have hampered efforts to document the full scope of famine in Gaza.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6gmigj000x356n28gg2ihg@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            After more than 700 days of war, 455 Palestinians have died of malnutrition or starvation, including 151 children, the health ministry in Gaza reported on  October 1. One hundred and seventy-seven of the total number have died of malnutrition or starvation since the IPC confirmed famine on August 15, it said.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6gm1ba000q356n0jmadvxo@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Israel’s vast web of bureaucratic impediments, including delayed approvals, arduous border checks and the <a href=\"https://www.cnn.com/2024/03/01/middleeast/gaza-aid-israel-restrictions-investigation-intl-cmd\">arbitrary rejection</a> of items, throttles the amount of aid that makes it to the other side of the border and sends food costs<a href=\"https://www.ochaopt.org/content/humanitarian-situation-update-326-gaza-strip\" target=\"_blank\">soaring</a>, the <a href=\"https://www.cnn.com/2025/07/23/middleeast/israel-gaza-aid-blockade-starvation-risk-intl-hnk\">UN and aid agencies</a> say.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6gozkl0014356n5pqyq0k2@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            After visiting the region in late August, US Senators Chris Van Hollen and Jeff Merkley, both Democrats, <a href=\"https://www.cnn.com/2025/09/18/world/video/van-hollen-gaza-report-sciutto-ldn-digvid\">warned</a> that Netanyahu’s government was “implementing a plan to ethnically cleanse Gaza of Palestinians” and accused Israel of using food “as a weapon of war.” Israel has denied the allegations.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6gozkl0015356nb9cemrbd@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            “The findings from our trip lead to the inescapable conclusion that the Netanyahu government’s war in Gaza has gone far beyond the targeting of Hamas to imposing collective punishment on the Palestinians there, with the goal of making life for them unsustainable,” said the report, published on September 11. “That is why it restricts the delivery of humanitarian assistance.”\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6gozkl0016356nqxzw9g3b@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Israeli authorities have said trucks “remain uncollected” at the border with Gaza – accusing the UN of failing to coordinate the entry of vehicles <a href=\"https://www.cnn.com/2024/06/07/middleeast/gaza-humanitarian-aid-israel-dg-intl\">into the strip</a>.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6gozkl0017356n4vpmw04f@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            But Sam Rose, the acting director of affairs for the UN agency for Palestinian refugees (UNRWA) in Gaza, says Israel – which has near-total jurisdiction over what goods enter and exit Gaza – has controlled “to the calorie” the volume, type and overall flow of food into the enclave. “The system is designed not to function smoothly,” he said.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6gozkl0018356ntk1r9sqo@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Israeli authorities “know and analyze each truck that goes into Gaza, the weight and the calories,” a senior official with COGAT, the Israeli agency that controls the entry of aid into the enclave, said in September. According to a COGAT statement published in response to the IPC famine declaration, “analysis of contents of food aid trucks that entered the Gaza Strip reveal that 4,400 calories per person per day entered Gaza since the beginning of August.”\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6gozkl0019356nhih1dl27@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            However, as of May, Palestinians were consuming just 1,400 calories per day – or “67 per cent of what a human body needs to survive,” at 2,300 calories, the UN <a href=\"https://news.un.org/en/story/2025/06/1164076\" target=\"_blank\">reported</a> in June.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6gozkl001a356nhq4i3hdw@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Last October, Israel’s government banned UNRWA from operating in areas under its control, a prohibition that went into effect in January, having accused the agency of failing to stop Hamas’ alleged theft of aid. An internal US government review found <a href=\"https://www.cnn.com/2025/07/25/politics/us-government-review-no-evidence-widespread-theft-gaza-aid\">no evidence</a> of widespread theft by Hamas of US-funded humanitarian aid in Gaza.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6gozkl001b356nsgscxxji@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            When the trickle of relief does enter the strip, aid workers face intensified hostilities, damaged roads and limited fuel supplies – impeding internal distribution efforts, minimizing viable routes and blocking access to displaced Palestinians, said Rose.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6go0mg0012356nyno27kzi@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Israel says UN aid makes up only part of the relief that gets into Gaza. A senior COGAT official told a briefing in early September that 27% of the trucks entering Gaza are UN vehicles, claiming it was “a lie” that the UN had brought in 600 aid trucks a day before the war.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6gt30i001i356nna2qvbpr@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            “There is no famine in Gaza. Period,” the official said, adding that “Israel and the IDF are trying to strengthen the humanitarian situation in Gaza with partners.”\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6gt30i001j356nmu4jwg3o@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            In May, the US and Israeli-backed Gaza Humanitarian Foundation (GHF) established a program that now plans to operate up to five distribution sites in the enclave, all but one in southern Gaza – which rely on private military contractors and largely replaced 400 UN-led hubs.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6gt30i001k356na43qqarj@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Relief and health workers say these other methods of delivering food in Gaza, including the GHF sites and <a href=\"https://www.cnn.com/2024/10/21/middleeast/israel-gaza-airdropped-aid-kills-palestinian-child-intl\">aid pallet drops</a> from planes, are dehumanizing and inaccessible for many Palestinians, and expose them to injury or death.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6gt30i001l356nxnicq55w@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            At least 1,172 people were killed “near militarized supply sites” between May 27 and September 9, the UN said on <a href=\"https://www.ochaopt.org/content/humanitarian-situation-update-321-gaza-strip\" target=\"_blank\">September 10</a>, with another 1,084 deaths along convoy supply routes. In August, UN experts called for the <a href=\"https://www.un.org/unispal/document/un-experts-call-for-immediate-dismantling-of-gaza-humanitarian-foundation/\" target=\"_blank\">immediate closure</a> of GHF-operated sites in Gaza and accused Israeli forces of opening “indiscriminate fire” on people seeking aid there. The advocates warned the hubs are “especially difficult” for women, children, people with disabilities and elderly Palestinians to access.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6gt30i001m356nbrgfvia3@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            GHF has <a href=\"https://www.cnn.com/2025/07/08/us/usaid-gaza-humanitarian-foundation-israel-grant\">defended</a> its work in Gaza and <a href=\"https://www.cnn.com/2025/09/16/middleeast/israel-gaza-genocide-un-commission-report-intl\">said</a> earlier in September that it was the only organization in Gaza able to deliver food “at scale without interference.” The organization also said that it had “repeatedly sought collaboration with UN agencies and international NGOs to deliver aid side-by-side” but that the UN had “declined those offers.” The Israeli military <a href=\"https://www.cnn.com/2025/06/04/middleeast/israel-military-gaza-aid-shooting-intl-invs\">has acknowledged</a> firing warning shots toward crowds in some instances and denied responsibility for other casualties near aid hubs.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6gt30i001n356nwdsr0i6b@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The US and Israel plan to set up 12 additional sites across the enclave, an Israeli official told CNN <a href=\"https://www.cnn.com/2025/09/09/middleeast/israel-gaza-city-evacuation-netanyahu-warning-intl\">in August</a>. However, there is no indication that the new sites have been established. In September, GHF said it had sought IDF permission to open sites in northern Gaza but that Israel had not granted the permission.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6gvv0j001s356n7wmbe685@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            “With parents injured and their siblings starving, many teenagers and young adults are taking the risk,” Mohammed Khaleel, an American surgeon who was deployed to Gaza earlier this year, told CNN in August.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg83aca20003396sxhxjhf6e@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            “We’ve even heard some people report that they will go and accept their fate. Dying from a gunshot may be preferable to dying from starvation,” he added.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6gvqzo001q356np5cs9ack@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Israel’s two-year offensive in Gaza had left just 1.5% of cropland accessible and undamaged as of July 28, according to the UN – largely preventing Palestinians from cultivating produce.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6h0cfn0022356nhj7mqr81@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            That destruction, coupled with Israel’s fishing ban and intensified assault in the north, has   further limited the sources of food available to hundreds of thousands of displaced Palestinians.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6h0cfn0023356np99jyvny@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            “It is not by chance that Israel has focused its starvation tactics in northern Gaza,” Fakhri, the UN special rapporteur, said. “They have announced their intent to push people from the north to the south of Gaza… Just as now, the focus of their starvation campaign on Gaza City correlates with their invasion plans.”\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6h01dj0020356nb52m2gvo@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            The military’s invasion of Gaza City will collapse an “already fragile” aid supply chain, warned Arif Husain, chief economist at the World Food Programme.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg6h10hs0028356ny5w61xmn@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            Relief agencies need a ceasefire, unimpeded humanitarian access, large-scale multi-sector aid, protection of civilians and infrastructure – and restoration of commercial and local food systems – to reverse famine in Gaza, said Husain.\n    </p><p data-uri=\"cms.cnn.com/_components/paragraph/instances/cmg80lbt30000396s8xk54sfs@published\" data-editable=\"text\" data-component-name=\"paragraph\" data-article-gutter=\"true\">\n            “We are already at the brink. Another escalation – especially in Gaza City – could push the situation into unimaginable catastrophe,” he added. “It will not only result in more deaths but destroy any foundation for future recovery.”\n    </p>","contentLength":11747,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45447699"},{"title":"Immich v2.0.0 – First stable release","url":"https://github.com/immich-app/immich/discussions/22546","date":1759386343,"author":"Alexvb","guid":111,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45446834"},{"title":"Jane Goodall has died","url":"https://www.latimes.com/obituaries/story/2025-10-01/jane-goodall-chimpanzees-dead","date":1759342239,"author":"jaredwiener","guid":110,"unread":true,"content":"<p>Jane Goodall, the trailblazing naturalist whose intimate observations of chimpanzees in the African wild produced powerful insights that transformed basic conceptions of humankind, has died. She was 91.</p><p>A tireless advocate of preserving chimpanzees’ natural habitat, Goodall died on Wednesday morning in California of natural causes, the Jane Goodall Institute announced on its <a href=\"https://www.instagram.com/p/DPRn2HTCFYt/?igsh=NTc4MTIwNjQ2YQ%3D%3D\" target=\"_blank\">Instagram page</a>. </p><p>“Dr. Goodall’s discoveries as an ethologist revolutionized science,” the Jane Goodall Institute said in a statement. </p><p>A protege of anthropologist Louis S.B. Leakey, Goodall made history in 1960 when she discovered that chimpanzees, humankind’s closest living relatives, made and used tools, characteristics that scientists had long thought were exclusive to humans.</p><p>She also found that chimps hunted prey, ate meat, and were capable of a range of emotions and behaviors similar to those of humans, including filial love, grief and violence bordering on warfare.</p><p>In the course of establishing one of the world’s longest-running studies of wild animal behavior at what is now Tanzania’s Gombe Stream National Park, she gave her chimp subjects names instead of numbers, a practice that raised eyebrows in the male-dominated field of primate studies in the 1960s. But within a decade, the trim British scientist with the tidy ponytail was a National Geographic heroine, whose books and films educated a worldwide audience with stories of the apes she called David Graybeard, Mr. McGregor, Gilka and Flo.</p><p>“When we read about a woman who gives funny names to chimpanzees and then follows them into the bush, meticulously recording their every grunt and groom, we are reluctant to admit such activity into the big leagues,” the late biologist Stephen Jay Gould wrote of the scientific world’s initial reaction to Goodall.</p><p>But Goodall overcame her critics and produced work that Gould later characterized as “one of the Western world’s great scientific achievements.”</p><p>Tenacious and keenly observant, Goodall paved the way for other women in primatology, including the late gorilla researcher Dian Fossey and orangutan expert Birutė Galdikas. She was honored in 1995 with the National Geographic Society’s Hubbard Medal, which then had been bestowed only 31 times in the previous 90 years to such eminent figures as North Pole explorer Robert E. Peary and aviator Charles Lindbergh.</p><p>In her 80s she continued to travel 300 days a year to speak to schoolchildren and others about the need to fight deforestation, preserve chimpanzees’ natural habitat and promote sustainable development in Africa. She was in California as part of her speaking tour in the U.S. at the time of her death.</p><p>Goodall was born April 3, 1934, in London and grew up in the English coastal town of Bournemouth. The daughter of a businessman and a writer who separated when she was a child and later divorced, she was raised in a matriarchal household that included her maternal grandmother, her mother, Vanne, some aunts and her sister, Judy.</p><p>She demonstrated an affinity for nature from a young age, filling her bedroom with worms and sea snails that she rushed back to their natural homes after her mother told her they would otherwise die.</p><p>When she was about 5, she disappeared for hours to a dark henhouse to see how chickens laid eggs, so absorbed that she was oblivious to her family’s frantic search for her. She did not abandon her study until she observed the wondrous event.</p><p>“Suddenly with a plop, the egg landed on the straw. With clucks of pleasure the hen shook her feathers, nudged the egg with her beak, and left,” Goodall wrote almost 60 years later. “It is quite extraordinary how clearly I remember that whole sequence of events.”</p><p>When finally she ran out of the henhouse with the exciting news, her mother did not scold her but patiently listened to her daughter’s account of her first scientific observation.</p><p>Later, she gave Goodall books about animals and adventure — especially the Doctor Dolittle tales and Tarzan. Her daughter became so enchanted with Tarzan’s world that she insisted on doing her homework in a tree.</p><p>“I was madly in love with the Lord of the Jungle, terribly jealous of his Jane,” Goodall wrote in her 1999 memoir, “Reason for Hope: A Spiritual Journey.” “It was daydreaming about life in the forest with Tarzan that led to my determination to go to Africa, to live with animals and write books about them.”</p><p>Her opportunity came after she finished high school. A week before Christmas in 1956 she was invited to visit an old school chum’s family farm in Kenya. Goodall saved her earnings from a waitress job until she had enough for a round-trip ticket.</p><p>She arrived in Kenya in 1957, thrilled to be living in the Africa she had “always felt stirring in my blood.” At a dinner party in Nairobi shortly after her arrival, someone told her that if she was interested in animals, she should meet Leakey, already famous for his discoveries in East Africa of man’s fossil ancestors.</p><p>She went to see him at what’s now the National Museum of Kenya, where he was curator. He hired her as a secretary and soon had her helping him and his wife, Mary, dig for fossils at Olduvai Gorge, a famous site in the Serengeti Plains in what is now northern Tanzania.</p><p>Leakey spoke to her of his desire to learn more about all the great apes. He said he had heard of a community of chimpanzees on the rugged eastern shore of Lake Tanganyika where an intrepid researcher might make valuable discoveries.</p><p>When Goodall told him this was exactly the kind of work she dreamed of doing, Leakey agreed to send her there.</p><p>It took Leakey two years to find funding, which gave Goodall time to study primate behavior and anatomy in London. She finally landed in Gombe in the summer of 1960.</p><p>On a rocky outcropping she called the Peak, Goodall made her first important observation. Scientists had thought chimps were docile vegetarians, but on this day about three months after her arrival, Goodall spied a group of the apes feasting on something pink. It turned out to be a baby bush pig.</p><p>Two weeks later, she made an even more exciting discovery — the one that would establish her reputation. She had begun to recognize individual chimps, and on a rainy October day in 1960, she spotted the one with white hair on his chin. He was sitting beside a mound of red earth, carefully pushing a blade of grass into a hole, then withdrawing it and poking it into his mouth.</p><p>When he finally ambled off, Goodall hurried over for a closer look. She picked up the abandoned grass stalk, stuck it into the same hole and pulled it out to find it covered with termites. The chimp she later named David Graybeard had been using the stalk to fish for the bugs.</p><p>“It was hard for me to believe what I had seen,” Goodall later wrote. “It had long been thought that we were the only creatures on earth that used and made tools. ‘Man the Toolmaker’ is how we were defined ...” What Goodall saw challenged man’s uniqueness.</p><p>When she sent her report to Leakey, he responded: “We must now redefine man, redefine tool, or accept chimpanzees as human!”</p><p>Goodall’s startling finding, published in Nature in 1964, enabled Leakey to line up funding to extend her stay at Gombe. It also eased Goodall’s admission to Cambridge University to study ethology. In 1965, she became the eighth person in Cambridge history to earn a doctorate without first having a bachelor’s degree.</p><p>In the meantime, she had met and in 1964 married Hugo Van Lawick, a gifted filmmaker who had traveled to Gombe to make a documentary about her chimp project. They had a child, Hugo Eric Louis — later nicknamed Grub — in 1967.</p><p>Goodall later said that raising Grub, who lived at Gombe until he was 9, gave her insights into the behavior of chimp mothers. Conversely, she had “no doubt that my observation of the chimpanzees helped me to be a better mother.”</p><p>She and Van Lawick were married for 10 years, divorcing in 1974. The following year she married Derek Bryceson, director of Tanzania National Parks. He died of colon cancer four years later.</p><p>Within a year of arriving at Gombe, Goodall had chimps literally eating out of her hands. Toward the end of her second year there, David Graybeard, who had shown the least fear of her, was the first to allow her physical contact. She touched him lightly and he permitted her to groom him for a full minute before gently pushing her hand away. For an adult male chimpanzee who had grown up in the wild to tolerate physical contact with a human was, she wrote in her 1971 book “In the Shadow of Man,” “a Christmas gift to treasure.”</p><p>Her studies yielded a trove of other observations on behaviors, including etiquette (such as soliciting a pat on the rump to indicate submission) and the sex lives of chimps. She collected some of the most fascinating information on the latter by watching Flo, an older female with a bulbous nose and an amazing retinue of suitors who was bearing children well into her 40s.</p><p>Her reports initially caused much skepticism in the scientific community. “I was not taken very seriously by many of the scientists. I was known as a [National] Geographic cover girl,” she recalled in a CBS interview in 2012.</p><p>Her unorthodox personalizing of the chimps was particularly controversial. The editor of one of her first published papers insisted on crossing out all references to the creatures as “he” or “she” in favor of “it.” Goodall eventually prevailed.</p><p>Her most disturbing studies came in the mid-1970s, when she and her team of field workers began to record a series of savage attacks.</p><p>The incidents grew into what Goodall called the four-year war, a period of brutality carried out by a band of male chimpanzees from a region known as the Kasakela Valley. The marauders beat and slashed to death all the males in a neighboring colony and subjugated the breeding females, essentially annihilating an entire community.</p><p>It was the first time a scientist had witnessed organized aggression by one group of non-human primates against another. Goodall said this “nightmare time” forever changed her view of ape nature.</p><p>“During the first 10 years of the study I had believed ... that the Gombe chimpanzees were, for the most part, rather nicer than human beings,” she wrote in “Reason for Hope: A Spiritual Journey,” a 1999 book co-authored with Phillip Berman. “Then suddenly we found that the chimpanzees could be brutal — that they, like us, had a dark side to their nature.”</p><p>Critics tried to dismiss the evidence as merely anecdotal. Others thought she was wrong to publicize the violence, fearing that irresponsible scientists would use the information to “prove” that the tendency to war is innate in humans, a legacy from their ape ancestors. Goodall persisted in talking about the attacks, maintaining that her purpose was not to support or debunk theories about human aggression but to “understand a little better” the nature of chimpanzee aggression.</p><p>“My question was: How far along our human path, which has led to hatred and evil and full-scale war, have chimpanzees traveled?”</p><p>Her observations of chimp violence marked a turning point for primate researchers, who had considered it taboo to talk about chimpanzee behavior in human terms. But by the 1980s, much chimp behavior was being interpreted in ways that would have been labeled anthropomorphism — ascribing human traits to non-human entities — decades earlier. Goodall, in removing the barriers, raised primatology to new heights, opening the way for research on subjects ranging from political coalitions among baboons to the use of deception by an array of primates.</p><p>Her concern about protecting chimpanzees in the wild and in captivity led her in 1977 to found the <a href=\"https://www.janegoodall.org/\" target=\"_blank\">Jane Goodall Institute</a> to advocate for great apes and support research and public education. She also established Roots and Shoots, a program aimed at youths in 130 countries, and TACARE, which involves African villagers in sustainable development.</p><p>She became an international ambassador for chimps and conservation in 1986 when she saw a film about the mistreatment of laboratory chimps. The secretly taped footage “was like looking into the Holocaust,” she told interviewer Cathleen Rountree in 1998. From that moment, she became a globe-trotting crusader for animal rights. </p><p>In the 2017 documentary “Jane,” the producer  pored through 140 hours of footage of Goodall that had been hidden away in the National Geographic archives. The film won a Los Angeles Film Critics Assn. Award, one of many honors it received.</p><p>In a <a href=\"https://www.latimes.com/la-oe-morrison18-2009jul18-column.html\" target=\"_blank\">ranging 2009 interview</a> with Times columnist Patt Morrison, Goodall mused on topics from traditional zoos — she said most captive environments should be abolished — to climate change, a battle she feared humankind was quickly losing, if not lost already. She also spoke about the power of what one human can accomplish.</p><p>“I always say, ‘If you would spend just a little bit of time learning about the consequences of the choices you make each day’ — what you buy, what you eat, what you wear, how you interact with people and animals — and start consciously making choices, that would be beneficial rather than harmful.”</p><p>As the years  passed, Goodall continued to track Gombe’s chimps, accumulating enough information to draw the arcs of their lives — from birth through sometimes troubled adolescence, maturity, illness and finally death.</p><p>She wrote movingly about how she followed Mr. McGregor, an older, somewhat curmudgeonly chimp, through his agonizing death from polio, and how the orphan Gilka survived to lonely adulthood only to have her babies snatched from her by a pair of cannibalistic female chimps.</p><p>Her reaction in 1972 to the death of Flo, a prolific female known as Gombe’s most devoted mother, suggested the depth of feeling that Goodall had for the animals. Knowing that Flo’s faithful son Flint was nearby and grieving, Goodall watched over the body all night to keep marauding bush pigs from violating her remains.</p><p>“People say to me, thank you for giving them characters and personalities,” Goodall once told CBS’s “60 Minutes.” “I said I didn’t give them anything. I merely translated them for people.”</p><p><i>Woo is a former Times staff writer.</i></p>","contentLength":14353,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45441069"},{"title":"Don't avoid workplace politics","url":"https://terriblesoftware.org/2025/10/01/stop-avoiding-politics/","date":1759340203,"author":"matheusml","guid":109,"unread":true,"content":"<p>Say the word “politics” to most engineers and watch their face scrunch up like they just bit into a lemon. We’ve all been conditioned to believe that workplace politics is this dirty game played by manipulative ladder-climbers while the “real” engineers focus on the code.</p><p>I used to think the same way. For years as an engineer, I wore my hatred of politics like a badge of honor. I was above all that nonsense. I just wanted to ship. Politics was for those other people, the ones who didn’t have what it takes technically.</p><p>Now I think the opposite: <strong>politics isn’t the problem; bad politics is.</strong> And pretending politics doesn’t exist? That’s how bad politics wins.</p><p>Politics is just how humans coordinate in groups. It’s the invisible network of relationships, influence, and informal power that exists in every organization. You can refuse to participate, but that doesn’t make it go away. It just means decisions get made without you.</p><p>Think about the last time a terrible technical decision got pushed through at your company. Maybe it was adopting some overcomplicated architecture, or choosing a vendor that everyone knew was wrong, or killing a project that was actually working. I bet if you dig into what happened, you’ll find it wasn’t because the decision-makers were stupid. It’s because the people with the right information weren’t in the room. They “didn’t do politics.”</p><p>Meanwhile, someone who understood how influence works was in that room, making their case, building coalitions, showing they’d done their homework. And their idea won. Not because it was better, but because they showed up to play while everyone else was “too pure” for politics.</p><p>Ideas don’t speak. People do. And the people who understand how to navigate organizational dynamics, build relationships, and yes, play politics? Their ideas get heard.</p><p>When you build strong relationships across teams, understand what motivates different stakeholders, and know how to build consensus, you’re doing politics. When you take time to explain your technical decisions to non-technical stakeholders in language they understand, that’s politics. When you grab coffee with someone from another team to understand their challenges, that’s politics too.</p><p><strong>Good politics is just being strategic about relationships and influence in the service of good outcomes.</strong></p><p>The best technical leaders are incredibly political. They just don’t call it that. They call it “stakeholder management” or “building alignment” or “organizational awareness.” But it’s politics, and they’re good at it.</p><p>The engineers who refuse to engage with politics often complain that their companies make bad technical decisions. But they’re not willing to do what it takes to influence those decisions. They want a world where technical merit alone determines outcomes. That world doesn’t exist and never has.</p><p>This isn’t about becoming a scheming backstabber. As I wrote in <a href=\"https://terriblesoftware.org/2025/03/31/your-strengths-are-your-weaknesses/\">Your Strengths Are Your Weaknesses</a>, the same trait can be positive or negative depending on how you use it. Politics is the same way. You can use political skills to manipulate and self-promote, or you can use them to get good ideas implemented and protect your team from bad decisions.</p><p>Here’s what good politics looks like in practice:</p><ol><li><strong>Building relationships before you need them.</strong> That random coffee with someone from the data team? Six months later, they’re your biggest advocate for getting engineering resources for your data pipeline project.</li><li><strong>Understanding the real incentives.</strong> Your VP doesn’t care about your beautiful microservices architecture. They care about shipping features faster. Frame your technical proposals in terms of what they actually care about.</li><li> Your manager is juggling competing priorities you don’t see. Keep them informed about what matters, flag problems early with potential solutions, and help them make good decisions. When they trust you to handle things, they’ll fight for you when it matters</li><li><strong>Creating win-win situations.</strong> Instead of fighting for resources, find ways to help other teams while getting what you need. It doesn’t have to be a zero-sum game.</li><li> If you do great work but nobody knows about it, did it really happen? Share your wins, present at all-hands, write those design docs that everyone will reference later.</li></ol><p>The alternative to good politics isn’t no politics. It’s bad politics winning by default. It’s the loud person who’s wrong getting their way because the quiet person who’s right won’t speak up. It’s good projects dying because nobody advocated for them. It’s talented people leaving because they couldn’t navigate the organizational dynamics.</p><p>Stop pretending you’re above politics. You’re not. Nobody is. The only question is whether you’ll get good at it or keep losing to people who already are.</p>","contentLength":4853,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45440571"},{"title":"Gmail will no longer support checking emails from third-party accounts via POP","url":"https://support.google.com/mail/answer/16604719?hl=en","date":1759335958,"author":"sumanep","guid":108,"unread":true,"content":"<div><p>Starting January 2026, Gmail will no longer provide support for the following:</p><ul><li> This feature allows you to get special features like spam protection or inbox organization applied to your third-party email account. <a href=\"https://support.google.com/mail/answer/6304825\" rel=\"noopener\">Learn more about Gmailify</a>.</li><li> Unlike IMAP connections, with POP, emails are downloaded, and you decide how often you want to download new emails. As an alternative, you can still link your third-party accounts in the Gmail app.</li></ul><p>These changes help provide the most secure and current options to access your messages in Gmail.</p><h2>Learn about changes to Gmailify</h2><p>You won’t be able to get specific features in Gmail applied to your third-party account, like:</p><h2>Learn about changes to POP connections</h2><ul><li>Gmail will no longer support checking emails from third-party accounts through POP.</li><li>The option to \"Check mail from other accounts\" will no longer be available in Gmail on your computer.</li></ul><ul><li>To continue to receive messages from your other account in Gmail, you need to set up IMAP access.\n    <ul><li>Check your email provider’s documentation for instructions on how to enable IMAP for your account.</li></ul></li></ul><h2>Frequently asked questions</h2><div><p>No. All messages synced before the deprecation stay in Gmail.</p></div><div><p>Yes. For third-party accounts like Yahoo! and Outlook, you can add them to the Gmail mobile app on Android and iPhone and iPad.</p></div></div>","contentLength":1298,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45439670"},{"title":"Show HN: Autism Simulator","url":"https://autism-simulator.vercel.app/","date":1759330111,"author":"joshcsimmons","guid":107,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45438346"},{"title":"Inflammation now predicts heart disease more strongly than cholesterol","url":"https://www.empirical.health/blog/inflammation-and-heart-health/","date":1759262421,"author":"brandonb","guid":106,"unread":true,"content":"<div><h2>Inflammation now predicts heart disease better than cholesterol</h2></div><p>Chronic inflammation has long been known to double your risk of heart disease, but prior to now,\ninflammation has never been a SMuRF: tandard odifiable isk actor for heart disease.</p><p>The American College of Cardiology just released recommendations that change that. The ACC is now recommending that everyone\nmeasure inflammation (specifically, hs-CRP) via a blood test:</p><blockquote><p>Because clinicians will not treat what they do not measure, universal screening of hsCRP in both primary and secondary prevention patients, in combination with cholesterol, represents a major clinical opportunity and is therefore recommended. <a href=\"https://www.jacc.org/doi/10.1016/j.jacc.2025.08.047\">American College of Cardiology</a></p></blockquote><p>There were many interesting bits of evidence that led to this recommendation. The whole <a href=\"https://www.jacc.org/doi/10.1016/j.jacc.2025.08.047\">article, published in JACC</a>, is worth a read, but this blog post extracts a few of the most interesting parts — or at\nleast, the parts I thought were most interesting.</p><div>\nWant to skip ahead and measure your inflammation? Empirical Health's <a href=\"https://www.empirical.health/product/comprehensive-health-panel?utm_source=blog\">advanced heart health panel</a> includes hs-CRP, ApoB, Lp(a), and other critical heart health biomarkers.\n</div><p>For decades, LDL cholesterol (or <a href=\"https://www.empirical.health/blog/apob-blood-test/\">ApoB</a>) has been the main focus of cardiovascular risk assessment. But this chart shows hs-CRP is actually a  predictor of cardiovascular events than LDL.</p><p>Why? In some ways, <strong>cholesterol has become a victim of its own success.</strong> We now screen the whole population\nfor high cholesterol, give statins to those with high LDL (or ApoB), and so then the majority of people who\nend up having heart attacks have lower cholesterol than they would naturally have. This means most of\nthe majority of residual risk for heart attacks will be found in biomarkers that aren’t SMuRFs.</p><p>Inflammation (hs-CRP) is one such non-SMuRF, one perhaps one of the strongest. This is especially true\nin people already on statins or those without traditional risk factors (sometimes called “SMuRF-less” patients).\nIn these groups, cholesterol may be well controlled, but inflammation remains a key driver of events.</p><p>Of course, other traditional risk factors matter  to inflammation: blood pressure, HbA1c or\ninsulin resistance, eGFR (kidney function), and so on.</p><p>The ACC consensus reviews a range of clinical trials testing both drugs and lifestyle interventions for lowering inflammation and reducing cardiovascular risk. Here’s a summary of the clinical trials and their results:</p><table><thead><tr><th>Population/NYHA Functional Class</th></tr></thead><tbody><tr><td>Infliximab (TNF inhibitor)</td><td>Clinical status (composite score)</td><td>No improvement or worsening; deaths highest in high-dose infliximab</td></tr><tr><td>Composite all-cause mortality and CV hospitalization</td><td>No reduction in events; trend toward benefit in NYHA III and IV</td></tr><tr><td>Nonfatal MI, nonfatal stroke, or CV death (MACE); HF-related mortality</td><td>Reduced MACE and HF events; no effect on all-cause mortality; primary endpoint events: 3.86% vs 4.50%</td></tr><tr><td>No effect on CV events, inflammation, or lipids</td></tr><tr><td>Death from CV causes, recurrent MI, ischemic stroke</td><td>No significant difference in primary endpoint</td></tr><tr><td>CV events lower than placebo</td></tr><tr><td>Composite of CV death, nonfatal MI, ischemic stroke, or ischemia-driven revasc.</td><td>CV events lower than placebo</td></tr><tr><td>All-cause mortality and CV hospitalization</td><td>No effect on primary endpoints</td></tr><tr><td>No CVD / LDL &lt;130 mg/dL; hsCRP ≥2 mg/L</td><td>MI, stroke, arterial revascularization, hospitalization for unstable angina, or CV death</td><td>Reduced events (HR 0.56–0.69)</td></tr><tr><td>NYHA II-IV HF; ischemic etiology</td><td>CV death, nonfatal MI, nonfatal stroke</td><td>No effect on primary endpoint</td></tr><tr><td>Etanercept (TNF inhibitor)</td><td>Death, hospitalization, or worsening HF</td><td>No effect on primary endpoint</td></tr><tr><td>Prednisone (corticosteroid)</td><td>NYHA II-IV HF; biopsy-proven myocarditis</td><td>Improvement in LVEF, survival, or combined outcome of death or transplantation</td></tr><tr><td>Etanercept (TNF inhibitor)</td><td>Composite outcome of death or hospitalization</td><td>No effect on primary endpoint</td></tr></tbody></table><p> to lower inflammation?</p><ul><li> (especially in people with high hs-CRP): Substantial reduction in events, even when LDL is normal (JUPITER trial).</li><li>: Reduces recurrent events in people with established heart disease (COLCOT, LoDoCo2).</li><li>: Reduces events but is expensive and increases infection risk (CANTOS).</li><li>: Anti-inflammatory diets (Mediterranean, DASH), regular exercise, smoking cessation, and maintaining a healthy weight all lower hs-CRP and reduce risk.</li></ul><ul><li>Some anti-inflammatory drugs (methotrexate, TNF inhibitors, corticosteroids) have not shown benefit in major trials.</li></ul><p>If you’ve already measured your hs-CRP (great!), then it’s ideally below &lt;1 mg/L. hs-CRP above 3 mg/L is\nhigh risk:</p><p>(If you’re in moderate or high ranges, see the section above for what to do.)</p><p>The ACC standardized hs-CRP as the marker of inflammation. But the ACC evaluated other markers: IL-6, fibrinogen, neutrophil-to-lymphocyte ratio, EPA/AA ratio, and serum amyloid A. While these markers have also been shown to predict cardiovascular risk, once hs-CRP is known, the others don’t add more signal.</p><p>In other words, you’re best off simply measuring hs-CRP with a blood test, and then spending money elsewhere on heart health.</p><p>The JACC article is packed with other interesting insights. These ones were interesting:</p><ul><li> (like CT, PET, MRI, and perivascular “fat attenuation index”) can detect vascular inflammation and may help predict coronary events, but are not yet ready for routine clinical use.</li><li> is a newer cholesterol-lowering drug that also lowers hs-CRP, but its long-term outcomes are still being studied.</li><li><strong>Residual inflammatory risk</strong>: Even with well-controlled LDL on statins, many people still have elevated hs-CRP and ongoing risk—so inflammation should be addressed separately from cholesterol.</li><li><strong>Universal hs-CRP screening</strong> is now recommended by the ACC for both people with and without established heart disease.</li><li> is now FDA-approved as an adjunct for secondary prevention in stable ASCVD, but should be avoided in people with significant kidney or liver disease.</li><li> are being studied as future anti-inflammatory therapies for heart disease.</li></ul><p>Blood tests for hs-CRP are widely available and inexpensive. Empirical Health’s comprehensive health panel is one option. The ACC now recommends routine hs-CRP testing for both people at risk (primary prevention) and those with established heart disease (secondary prevention).</p>","contentLength":6205,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45430498"},{"title":"Sora 2","url":"https://openai.com/index/sora-2/","date":1759251301,"author":"skilled","guid":105,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45427982"},{"title":"Kagi News","url":"https://blog.kagi.com/kagi-news","date":1759244940,"author":"grappler","guid":104,"unread":true,"content":"<p><strong>A comprehensive daily press review with global news. Fully private, with sources openly curated by our community.</strong></p><p>News is broken. We all know it, but we’ve somehow accepted it as inevitable. The endless notifications. The clickbait headlines designed to trigger rather than inform, driven by relentless ad monetization. The exhausting cycle of checking multiple apps throughout the day, only to feel more anxious and less informed than when we started. This isn’t what news was supposed to be.\nWe can do better, and create what news should have been all along: pure, essential information that respects your intelligence and time.</p><h2>Our approach: Signal over noise</h2><p><a href=\"https://kite.kagi.com\">Kagi News</a> operates on a simple principle: understanding the world requires hearing from the world. Every day, our system reads thousands of community curated RSS feeds from publications across different viewpoints and perspectives. We then use AI to distill this massive information into one comprehensive daily briefing, while clearly citing sources.</p><p>We strive for diversity and transparency of resources and welcome your contributions to widen perspectives. This multi-source approach helps reveal the full picture beyond any single viewpoint.</p><h2>Design principles that put readers first</h2><p> We publish once per day around noon UTC, creating a natural endpoint to news consumption. This is a deliberate design choice that turns news from an endless habit into a contained ritual.</p><p><strong>Five-minute complete understanding:</strong> Our briefings cover everything important in just five minutes. No endless scrolling. No attention hijacking. You read, understand, and move on with your day.</p><p><strong>Diversity over echo chambers:</strong> Rather than personalizing feeds to match existing preferences, we expose readers to the full spectrum of global perspectives. This approach breaks down information silos instead of reinforcing them.</p><p> Your reading habits belong to you. We don’t track, profile, or monetize your attention. You remain the customer and not the product.</p><p><strong>Community-driven sources:</strong> Our news sources are open source and community-curated through our public <a href=\"https://github.com/kagisearch/kite-public\">GitHub repository</a>. Anyone can propose additions, flag problems, or suggest improvements.</p><p> In your settings, you can select and reorder categories to match your interests and priorities. You can also adjust the number of stories shown, as well as dragging to re-order various sections, so that your briefing is focused on the depth and topics that matter most to you.</p><p> You can choose your preferred interface and content language. News stories are generated in their original source language, and then translated using <a href=\"https://translate.kagi.com\">Kagi Translate.</a> The default mode shows regional stories in their original language without translation, and all other ones in your browser’s language.</p><h2>Technical implementation that respects publishers</h2><p>We don’t scrape content from websites. Instead, we use publicly available RSS feeds that publishers choose to provide. Publishers decide what content appears in their feeds; some include full articles, others only titles or summaries. We respect those choices completely. We’re working within the ecosystem publishers have created rather than circumventing their intentions.</p><h2>Ready to experience news differently?</h2><p>If you’re tired of news that makes you feel worse about the world while teaching you less about it, we invite you to try a different approach with Kagi News, so download it today:</p>","contentLength":3405,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45426490"},{"title":"Imgur pulls out of UK as data watchdog threatens fine","url":"https://www.express.co.uk/news/uk/2115228/image-site-imgur-pulls-out","date":1759237265,"author":"ANewbury","guid":103,"unread":true,"content":"<div><p>The Information Commissioner’s Office (ICO) said that it has reached provisional findings in an investigation in the parent company of image hosting site, Imgur. Its probe was launched earlier this year, as part of the regulator's Children’s Code strategy, which is intended to set the standards for how <a data-link-tracking=\"InArticle|Link\" href=\"https://www.express.co.uk/news/politics/2089661/online-safety-law-watchdog\">online services handle the personal information of young people</a>.</p><p>In a statement the ICO said: “We are aware of reports that the social media platform <a data-link-tracking=\"InArticle|Link\" href=\"https://www.express.co.uk/life-style/science-technology/2091749/expressvpn-online-safety-act-2025\">Imgur is currently not available in the UK</a>. Imgur's decision to restrict access in the UK is a commercial decision taken by the company.”</p></div><div><p>Tim Capel, the ICO’s Interim Executive Director for Regulatory Supervision, said that the regulator had now issued a notice of intent to fine.</p><p>He said: “We reached our provisional findings on this investigation, and we issued a notice of intent to impose a monetary penalty on MediaLab on 10 September 2025.</p><p>“Our findings are provisional and the ICO will carefully consider any representations from MediaLab before taking a final decision whether to issue a monetary penalty.”</p><p>The ICO also confirmed that companies could not avoid accountability by withdrawing their services in the UK.</p><p>Mr Capel said: “We have been clear that exiting the UK does not allow an organisation to avoid responsibility for any prior infringement of data protection law, and our investigation remains ongoing.</p><p>“This update has been provided to give clarity on our investigation, and we will not be providing any further detail at this time.”</p></div><div><p>He added that protecting young people’s information remains a central focus: “Safeguarding children’s personal information is a key priority for the ICO and our Children’s code strategy outlines our key interventions in this area. Keeping children safe online is the responsibility of the companies offering online services to them and we will continue to hold them to account.”</p><p>Regulators did not disclose the potential size of the penalty for specific breaches it has identified.</p><p>Under UK law, the “notice of intent” process gives the company an opportunity to make representations before any final decision is made.</p><p>Imgur, founded in 2009 and acquired by Los Angeles-based MediaLab AI Inc in 2021, is an image hosting and sharing site popular for memes, viral content and online communities. It’s services appeared to become unavailable in the UK last night.</p><p>Imgur was approached for comment.</p></div>","contentLength":2432,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45424888"},{"title":"Comprehension debt: A ticking time bomb of LLM-generated code","url":"https://codemanship.wordpress.com/2025/09/30/comprehension-debt-the-ticking-time-bomb-of-llm-generated-code/","date":1759228659,"author":"todsacerdoti","guid":102,"unread":true,"content":"<p>An effect that’s being more and more widely reported is the increase in time it’s taking developers to modify or fix code that was generated by Large Language Models. </p><p>If you’ve worked on legacy systems that were written by other people, perhaps decades ago, you’ll recognise this phenomenon. Before we can safely change code, we first need to understand it – understand what it does, and also oftentimes why it does it the way it does. In that sense, this is nothing new.</p><p>What new is the scale of the problem being created as lightning-speed code generators spew reams of unread code into millions of projects.</p><p>Teams that care about quality will take the time to review and understand (and more often than not, rework) LLM-generated code before it makes it into the repo. This slows things down, to the extent that any time saved using the LLM coding assistant is often canceled out by the downstream effort.</p><p>But teams have opted for a different approach. They’re the ones checking in code nobody’s read, and that’s only been cursorily tested – if it’s been tested at all. And, evidently, there’s a of them.</p><p>When teams produce code faster than they can understand it, it creates what I’ve been calling “comprehension debt”. If the software gets used, then the odds are high that  that generated code will need to change. The “A.I.” boosters will say “We can just get the tool to do that”. And that might work maybe 70% of the time. </p><p>But those of us who’ve experimented a lot with using LLMs for code generation and modification know that there will be times when the tool just won’t be able to do it. </p><p>“Doom loops”, when we go round and round in circles trying to get an LLM, or a bunch of different LLMs, to fix a problem that it just doesn’t seem to be able to, are an everyday experience using this technology. Anyone claiming it doesn’t happen to them has either been extremely lucky, or is fibbing.</p><p>It’s pretty much guaranteed that there will be many times when we have to edit the code ourselves. The “comprehension debt” is the extra time it’s going to take us to understand it first.</p><p>And we’re sitting on a rapidly growing mountain of it.</p>","contentLength":2196,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45423917"},{"title":"I’ve removed Disqus. It was making my blog worse","url":"https://ryansouthgate.com/goodbye-disqus/","date":1759221366,"author":"ry8806","guid":101,"unread":true,"content":"<p>This will be a short and sweet post. As I’m not big on goodbyes.</p><p>Disqus started showing ads for their “free” tier comments system a few years back. At the time, the communication they sent out via email, seemed quite laid-back and had the tone of “don’t worry about it, it’s not a big thing”. Which in part lead me to almost forget it happened.</p><p>At the time, the disqus comments system looked quite smart and sleek. I remember thinking that the ads system will possibly look smart and sleek too. Which alleviated any worries I had at the time.</p><p>WELL…….I’ve just seen the ads, and they look horrific!!!</p><p>I have a <a href=\"https://pi-hole.net/\" target=\"_blank\">Pihole</a> set up, so ads are blocked on my home network. When I’m out of the house, my phone is connected to a <a href=\"https://www.wireguard.com/\" target=\"_blank\">Wireguard VPN</a> which routes my data through my home internet, therefore - getting all the ad-blocking, Pihole goodness.</p><p>After years with Pi-hole, which now blocks over a million domains, I’ve become incredibly accustomed to a mostly ad-free web. Without realizing it, I’d forgotten what the typical internet experience feels like.</p><p>I used to get a couple of emails from Disqus, letting me know that there’s a new comment on this blog. I haven’t had many of these emails recently, so I decided to disable my adblocker for a few minutes and check out the comments.</p><p>There were none, instead I was greeted by some horribly formatted and obviously scammy ads:</p><p>For the people who read this blog, I’m sorry.</p><p>I became “blind” to what the web is really like for most users. I’ve tried to keep this blog minimalist - a clean place to find answers. Those ads not only ruin that experience; they trample privacy too:</p><p>With this post, I’ve removed Disqus. It was making my blog worse, and frankly, they were profiting off my work and my visitor’s data.\nI want this blog to be a resource for devs and technologists, free not just in money, but in freedom from unwanted tracking and invasive ads.</p><p>I’m not entirely sure comments are needed here. There are other ways to reach me, for example; <a href=\"https://github.com/ryansouthgate\" target=\"_blank\">GitHub</a> or <a href=\"https://twitter.com/ryan_southgate\" target=\"_blank\">Twitter/X</a>. But having a place for discussion under each post can be valuable.\nIf you have any recommendations for alternative commenting systems (especially those that respect privacy or are self-hosted), I’d love to hear them! Please reach out if you’ve found something that works well.</p><p>Thanks as always for reading - your trust matters to me.</p><p><em>Sorry again for the mess!</em></p>","contentLength":2406,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45423268"},{"title":"Claude Code 2.0","url":"https://www.npmjs.com/package/@anthropic-ai/claude-code","date":1759165933,"author":"polyrand","guid":100,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45416228"},{"title":"Claude Sonnet 4.5","url":"https://www.anthropic.com/news/claude-sonnet-4-5","date":1759164779,"author":"adocomplete","guid":99,"unread":true,"content":"<p>Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It’s the best model at using computers. And it shows substantial gains in reasoning and math.</p><p>Code is everywhere. It runs every application, spreadsheet, and software tool you use. Being able to use those tools and reason through hard problems is how modern work gets done.</p><p>Claude Sonnet 4.5 makes this possible. We're releasing it along with a set of major upgrades to our products. In <a href=\"https://anthropic.com/news/enabling-claude-code-to-work-more-autonomously\" target=\"_blank\" rel=\"noopener noreferrer\">Claude Code</a>, we've added checkpoints—one of our most requested features—that save your progress and allow you to roll back instantly to a previous state. We've refreshed the terminal interface and shipped a <a href=\"https://marketplace.visualstudio.com/items?itemName=anthropic.claude-code\" target=\"_blank\" rel=\"noopener noreferrer\">native VS Code extension</a>. We've added a new <a href=\"https://anthropic.com/news/context-management\" target=\"_blank\" rel=\"noopener noreferrer\">context editing feature and memory tool</a> to the Claude API that lets agents run even longer and handle even greater complexity. In the Claude <a href=\"https://claude.ai/redirect/website.v1.ceac24f9-b8c3-4706-ae66-6c921e6f059c/download\" target=\"_blank\" rel=\"noopener noreferrer\">apps</a>, we've brought code execution and <a href=\"https://www.anthropic.com/news/create-files\" target=\"_blank\" rel=\"noopener noreferrer\">file creation</a> (spreadsheets, slides, and documents) directly into the conversation. And we've made the <a href=\"https://www.anthropic.com/news/claude-for-chrome\" target=\"_blank\" rel=\"noopener noreferrer\">Claude for Chrome</a> extension available to Max users who joined the waitlist last month.</p><p>We're also giving developers the building blocks we use ourselves to make Claude Code. We're calling this the <a href=\"https://anthropic.com/engineering/building-agents-with-the-claude-agent-sdk\" target=\"_blank\" rel=\"noopener noreferrer\">Claude Agent SDK</a>. The infrastructure that powers our frontier products—and allows them to reach their full potential—is now yours to build with.</p><p>This is the <a href=\"https://www.anthropic.com/claude-sonnet-4-5-system-card\" target=\"_blank\" rel=\"noopener noreferrer\">most aligned frontier model</a> we’ve ever released, showing large improvements across several areas of alignment compared to previous Claude models.</p><p>Claude Sonnet 4.5 is available everywhere today. If you’re a developer, simply use  via <a href=\"https://docs.claude.com/en/docs/about-claude/models/overview\" target=\"_blank\" rel=\"noopener noreferrer\">the Claude API</a>. Pricing remains the same as Claude Sonnet 4, at $3/$15 per million tokens.</p><p>Claude Sonnet 4.5 is state-of-the-art on the SWE-bench Verified evaluation, which measures real-world software coding abilities. Practically speaking, we’ve observed it maintaining focus for more than 30 hours on complex, multi-step tasks.</p><p>Claude Sonnet 4.5 represents a significant leap forward on computer use. On OSWorld, a benchmark that tests AI models on real-world computer tasks, Sonnet 4.5 now leads at 61.4%. Just four months ago, Sonnet 4 held the lead at 42.2%. Our <a href=\"https://www.anthropic.com/news/claude-for-chrome\" target=\"_blank\" rel=\"noopener noreferrer\">Claude for Chrome</a> extension puts these upgraded capabilities to use. In the demo below, we show Claude working directly in a browser, navigating sites, filling spreadsheets, and completing tasks.</p><p>The model also shows improved capabilities on a broad range of evaluations including reasoning and math:</p><p>Experts in finance, law, medicine, and STEM found Sonnet 4.5 shows dramatically better domain-specific knowledge and reasoning compared to older models, including Opus 4.1.</p><p>The model’s capabilities are also reflected in the experiences of early customers:</p><h2>Our most aligned model yet</h2><p>As well as being our most capable model, Claude Sonnet 4.5 is our most aligned frontier model yet. Claude’s improved capabilities and our extensive safety training have allowed us to substantially improve the model’s behavior, reducing concerning behaviors like sycophancy, deception, power-seeking, and the tendency to encourage delusional thinking. For the model’s agentic and computer use capabilities, we’ve also made considerable progress on defending against prompt injection attacks, one of the most serious risks for users of these capabilities.</p><p>You can read a detailed set of safety and alignment evaluations, which for the first time includes tests using techniques from mechanistic interpretability, in the Claude Sonnet 4.5 <a href=\"https://www.anthropic.com/claude-sonnet-4-5-system-card\" target=\"_blank\" rel=\"noopener noreferrer\">system card</a>.</p><p>Claude Sonnet 4.5 is being released under our AI Safety Level 3 (ASL-3) protections, as per <a href=\"https://www.anthropic.com/news/announcing-our-updated-responsible-scaling-policy\" target=\"_blank\" rel=\"noopener noreferrer\">our framework</a> that matches model capabilities with appropriate safeguards. These safeguards include filters called classifiers that aim to detect potentially dangerous inputs and outputs—in particular those related to chemical, biological, radiological, and nuclear (CBRN) weapons.</p><p>These classifiers might sometimes inadvertently flag normal content. We’ve made it easy for users to continue any interrupted conversations with Sonnet 4, a model that poses a lower CBRN risk. We've already made significant progress in reducing these false positives, reducing them by a factor of ten since <a href=\"https://www.anthropic.com/news/constitutional-classifiers\" target=\"_blank\" rel=\"noopener noreferrer\">we originally described them</a>, and a factor of two since Claude Opus 4 was released in May. We’re continuing to make progress in making the classifiers more discerning.</p><p>We've spent more than six months shipping updates to Claude Code, so we know what it takes to <a href=\"https://www.youtube.com/watch?v=DAQJvGjlgVM\" target=\"_blank\" rel=\"noopener noreferrer\">build</a> and <a href=\"https://www.youtube.com/watch?v=vLIDHi-1PVU\" target=\"_blank\" rel=\"noopener noreferrer\">design</a> AI agents. We've solved hard problems: how agents should manage memory across long-running tasks, how to handle permission systems that balance autonomy with user control, and how to coordinate subagents working toward a shared goal.</p><p>Now we’re making all of this available to you. The <a href=\"https://anthropic.com/engineering/building-agents-with-the-claude-agent-sdk\" target=\"_blank\" rel=\"noopener noreferrer\">Claude Agent SDK</a> is the same infrastructure that powers Claude Code, but it shows impressive benefits for a very wide variety of tasks, not just coding. As of today, you can use it to build your own agents.</p><p>We built Claude Code because the tool we wanted didn’t exist yet. The Agent SDK gives you the same foundation to build something just as capable for whatever problem you're solving.</p><p>We’re releasing a temporary research preview alongside Claude Sonnet 4.5, called \"<a href=\"https://claude.ai/redirect/website.v1.ceac24f9-b8c3-4706-ae66-6c921e6f059c/imagine\" target=\"_blank\" rel=\"noopener noreferrer\">Imagine with Claude</a>\".</p><p>In this experiment, Claude generates software on the fly. No functionality is predetermined; no code is prewritten. What you see is Claude creating in real time, responding and adapting to your requests as you interact.</p><p>It's a fun demonstration showing what Claude Sonnet 4.5 can do—a way to see what's possible when you combine a capable model with the right infrastructure.</p><p>\"Imagine with Claude\" is available to Max subscribers for the next five days. We encourage you to try it out on <a href=\"https://claude.ai/redirect/website.v1.ceac24f9-b8c3-4706-ae66-6c921e6f059c/imagine\" target=\"_blank\" rel=\"noopener noreferrer\">claude.ai/imagine</a>.</p><p>We recommend upgrading to Claude Sonnet 4.5 for all uses. Whether you’re using Claude through our apps, our API, or Claude Code, Sonnet 4.5 is a drop-in replacement that provides much improved performance for the same price. Claude Code updates are available to all users. <a href=\"https://claude.com/platform/api\" target=\"_blank\" rel=\"noopener noreferrer\">Claude Developer Platform</a> updates, including the Claude Agent SDK, are available to all developers. Code execution and file creation are available on all paid plans in the Claude apps.</p><p>For complete technical details and evaluation results, see our <a href=\"https://www.anthropic.com/claude-sonnet-4-5-system-card\" target=\"_blank\" rel=\"noopener noreferrer\">system card</a>, <a href=\"https://www.anthropic.com/claude/sonnet\" target=\"_blank\" rel=\"noopener noreferrer\">model page</a>, and <a href=\"https://docs.claude.com/en/docs/about-claude/models/overview\" target=\"_blank\" rel=\"noopener noreferrer\">documentation</a>. For more information, explore our <a href=\"https://www.anthropic.com/engineering/building-agents-with-the-claude-agent-sdk\" target=\"_blank\" rel=\"noopener noreferrer\">engineering</a><a href=\"https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents\" target=\"_blank\" rel=\"noopener noreferrer\">posts</a> and research post on <a href=\"https://red.anthropic.com/2025/ai-for-cyber-defenders\" target=\"_blank\" rel=\"noopener noreferrer\">cybersecurity</a>.</p>","contentLength":6436,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45415962"},{"title":"What if I don't want videos of my hobby time available to the world?","url":"https://neilzone.co.uk/2025/09/what-if-i-dont-want-videos-of-my-hobby-time-available-to-the-entire-world/","date":1759145286,"author":"speckx","guid":98,"unread":true,"content":"<p>I am very much enjoying my newly-resurrected hobby of Airsoft.</p><p>Running around in the woods, firing small plastic pellets at other people, in pursuit of a contrived-to-be-fun mission, turns out to be, well, fun.</p><p>I have also had to accept that, for some other players, part of that fun comes from making videos of their game days, and uploading them to YouTube.</p><p>They often have quite impressive setups, with multiple cameras - head, rear-facing from barrel of weapon, and scope cam - and clearly put time, money, and effort into doing this.</p><p>Great! Just like someone taking photos on their holidays, or when out and about, I can see the fun in it.</p><p>It is the “non-consensually publishing it online for the world to see” aspect which bugs me a bit.</p><p>In the handful of games that I have played, no-one has ever asked about consent of other participants.</p><p>There has been no “put on this purple lanyard if you don’t want to be included in the public version of the video” rule, which I’ve seen work pretty well at conferences I have attended (even if it is opt-out rather than consent).</p><p>I could, I suppose, ask each person that I see with a camera “would you mind not including me in anything you upload, please?”. And, since everyone with whom I’ve spoken at games, so far anyway, has been perfectly pleasant and friendly, I’d be hopeful that they would at least consider my request. I have not done this.</p><p>The impression I get is that this is just seen as part and parcel of the hobby: by running around in the woods of northern Newbury on a Sunday morning, I need to accept that I may well appear on YouTube, for the world to see.</p><p>I don’t love it, but it is not a big enough deal for me to make a fuss.</p><p>I occasionally see people saying “well, if you don’t want to be in photos published online, don’t be in public spaces”.</p><p>This is nonsense, for a number of reasons. Clearly, one should be able to exist in society, including going outside one’s own home, without needing to accept this kind of thing.</p><p>In any case, here, the issue is somewhat different, since it is a private site, where people engage in private activity (a hobby).</p><p>But then I’ve seen the same at (private) conferences, with people saying “Of course I’m free to take photos of identifiable individuals without their consent and publish them online”.</p><p>Publishing someone’s photo online, without their consent, without another strong justification, just because they happen to be in view of one’s camera lens, feels wrong to me.</p><p>This isn’t about what is legal (although, in some cases, claims of legality may be poorly conceived), but around my own perceptions of a private life, and a dislike for the fact that, just because one  publish such things, that one .</p><h2>[Updated] Some more notes</h2><ul><li>I’m just blogging. Sharing my thoughts. I’m not trying to set anyone’s policy, demand that anyone takes anything down or stops doing anything, or change anyone’s view.</li><li>I am in the UK. Different places may well have different norms, laws, and expectations. But this is just about something which bugs me a bit, not what the legal rights and wrongs might be. Plus, I don’t think that anyone (that I’ve seen so far) is doing this meanly or nefariously. This is just part of the fun for them, and fun is important.</li><li>Yes, biodegradable BBs are available, although it is not a site requirement, and the site shop does not sell them. I have used them a couple of times, and I haven’t found them to shatter on impact as much as (non-biodegradable) tracer BBs. I tend to buy BBs from the site’s own shop, to support them.</li><li>Yes, I wear two-part face covering; goggles/glasses (depending on the heat and humidity), and a lower face and ear mask. I prefer this to a full face mask. But players are still pretty obviously distinguishable, given differences in loadouts, patches they wear, and people shouting names. Few people wear face coverings in the “safe zone” (where one rests, eats/drinks, chats, loads up etc.), which are sometimes included in videos.</li><li> at everyone from HN. Thanks for a pleasant, thought-provoking, discussion, with numerous different perspectives.</li></ul>","contentLength":4146,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45412419"},{"title":"F-Droid and Google’s developer registration decree","url":"https://f-droid.org/2025/09/29/google-developer-registration-decree.html","date":1759111820,"author":"gumby271","guid":97,"unread":true,"content":"<p>For the past <a href=\"https://f-droid.org/en/2025/09/04/twif.html\">15 years</a>, F-Droid\nhas provided a safe and secure haven for Android users around the world to\nfind and install free and open source apps. When contrasted with the\ncommercial app stores — of which the Google Play store is the most prominent\n— the differences are stark: they are hotbeds of spyware and scams,\nblatantly promoting apps that prey on their users through attempts to\nmonetize their attention and mine their intimate information through any\nmeans necessary, including <a href=\"https://techcrunch.com/2025/02/13/spyware-maker-caught-distributing-malicious-android-apps-for-years\">trickery and dark\npatterns</a>.</p><p>F-Droid is different. It distributes apps that have been validated to work\n the user’s interests, rather than for the interests of the app’s\ndistributors. The way F-Droid works is simple: when a developer creates an\napp and hosts the source code publicly somewhere, the F-Droid team reviews\nit, inspecting it to ensure that it is completely open source and contains\nno undocumented <a href=\"https://f-droid.org/en/docs/Anti-Features/\"></a>\nsuch as advertisements or trackers. Once it passes inspection, the F-Droid\nbuild service compiles and packages the app to make it ready for\ndistribution. The package is then signed either with F-Droid’s cryptographic\nkey, or, if the build is\n<a href=\"https://f-droid.org/en/docs/Reproducible_Builds/\">reproducible</a>, enables\ndistribution using the original developer’s private key. In this way, users\ncan trust that any app distributed through F-Droid is the one that was built\nfrom the specified source code and has not been tampered with.</p><p>Do you want a weather app that doesn’t <a href=\"https://www.howtogeek.com/884233/your-weather-app-is-spying-on-you-heres-what-to-do/#why-are-weather-apps-such-a-privacy-nightmare\">transmit your every\nmovement</a>\nto a shadowy data broker? Or a scheduling assistant that doesn’t <a href=\"https://www.eff.org/deeplinks/2025/01/online-behavioral-ads-fuel-surveillance-industry-heres-how\">siphon\nyour intimate\ndetails</a>\ninto an advertisement network? F-Droid has your back. Just as sunlight is\nthe best disinfectant against corruption, open source is the best defense\nagainst software acting against the interests of the user.</p><h3>Google’s move to break free app distribution</h3><p>The future of this elegant and proven system was put in jeopardy last month,\nwhen Google unilaterally\n<a href=\"https://android-developers.googleblog.com/2025/08/elevating-android-security.html\">decreed</a>\nthat Android developers everywhere in the world are going to be required to\nregister centrally with Google. In addition to demanding payment of a\nregistration fee and agreement to their (non-negotiable and ever-changing)\nterms and conditions, Google will also require the <a href=\"https://developer.android.com/developer-verification#verify-your-identity\">uploading of personally\nidentifying\ndocuments</a>,\nincluding government ID, by the authors of the software, as well as\n<a href=\"https://developer.android.com/developer-verification#register-your-apps\">enumerating</a>\nall the unique “application identifiers” for every app that is to be\ndistributed by the registered developer.</p><p>The F-Droid project cannot require that developers register their apps\nthrough Google, but at the same time, we cannot “take over” the application\nidentifiers for the open-source apps we distribute, as that would\neffectively seize exclusive distribution rights to those applications.</p><p>If it were to be put into effect, the developer registration decree will end\nthe F-Droid project and other free/open-source app distribution sources as\nwe know them today, and the world will be deprived of the safety and\nsecurity of the catalog of thousands of apps that can be trusted and\nverified by any and all. F-Droid’s myriad users will be left adrift, with no\nmeans to install — or even update their existing installed —\napplications. (How many F-Droid users are there, exactly? We don’t know,\nbecause we don’t track users or have any registration: <a href=\"https://f-droid.org/en/2022/02/28/no-user-accounts-by-design.html\">“No user accounts,\nby design”</a>)</p><p>While directly installing — or “sideloading” — software can be construed as\ncarrying some inherent risk, it is false to claim that centralized app\nstores are the only safe option for software distribution. Google Play\nitself has\n<a href=\"https://www.malwarebytes.com/blog/news/2025/09/224-malicious-apps-removed-from-the-google-play-store-after-ad-fraud-campaign-discovered\">repeatedly</a><a href=\"https://www.theregister.com/2025/08/26/apps_android_malware/\">hosted</a>\nmalware, proving that corporate gatekeeping doesn’t guarantee user\nprotection. By contrast, F-Droid offers a trustworthy and transparent\nalternative approach to security: every app is free and open source, the\ncode can be audited by anyone, the build process and logs are public, and\nreproducible builds ensure that what is published matches the source code\nexactly. This transparency and accountability provides a  basis\nfor trust than closed platforms, while still giving users freedom to\nchoose. Restricting direct app installation not only undermines that choice,\nit also erodes the diversity and resilience of the open-source ecosystem by\nconsolidating control in the hands of a few corporate players.</p><p>Furthermore, Google’s framing that they need to mandate developer\nregistration in order to defend against malware is disingenuous because they\n have a remediation mechanism for malware they identify on a\ndevice: the <a href=\"https://support.google.com/googleplay/answer/2812853\">Play Protect\nservice</a> that is\nenabled on all Android Certified devices already scans and disables apps\nthat have been identified as malware, regardless of their provenience. Any\nperceived risks associated with direct app installation can be mitigated\nthrough user education, open-source transparency, and existing security\nmeasures without imposing exclusionary registration requirements.</p><p>We do not believe that developer registration is motivated by security. We\nbelieve it is about consolidating power and tightening control over a\nformerly open ecosystem.</p><p>If you own a computer, you should have the right to run whatever programs\nyou want on it. This is just as true with the apps on your Android/iPhone\nmobile device as it is with the applications on your Linux/Mac/Windows\ndesktop or server. Forcing software creators into a centralized registration\nscheme in order to publish and distribute their works is as egregious as\nforcing writers and artists to register with a central authority in order to\nbe able to distribute their creative works. It is an offense to the core\nprinciples of free speech and thought that are central to the workings of\ndemocratic societies around the world.</p><p>By tying application identifiers to personal ID checks and fees, Google is\nbuilding a choke point that restricts competition and limits user\nfreedom. It must find a solution which preserves user rights, freedom of\nchoice, and a healthy, competitive ecosystem.</p><p>Regulatory and competition authorities should look carefully at Google’s\nproposed activities, and ensure that policies designed to improve security\nare not abused to consolidate monopoly control. We urge regulators to\nsafeguard the ability of alternative app stores and open-source projects to\noperate freely, and to protect developers who cannot or will not comply with\nexclusionary registration schemes and demands for personal information.</p><p>If you are a developer or user who values digital freedom, you can\nhelp. Write to your <a href=\"https://www.europarl.europa.eu/meps/en/home\">Member of\nParliament</a>,\n<a href=\"https://www.house.gov/representatives/find-your-representative\">Congressperson</a>\nor other representative, sign petitions in defense of sideloading and\nsoftware freedom, and\n<a href=\"https://digital-markets-act.ec.europa.eu/contact-dma-team_en\">contact</a> the\nEuropean Commission’s Digital Markets Act (DMA) team to express why\npreserving open distribution matters. By making your voice heard, you help\ndefend not only F-Droid, but the principle that software should remain a\ncommons, accessible and free from unnecessary corporate gatekeeping.</p>","contentLength":6902,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45409794"},{"title":"Play snake in the URL address bar","url":"https://demian.ferrei.ro/snake/","date":1759093695,"author":"macote","guid":96,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45408021"},{"title":"We bought the whole GPU, so we're damn well going to use the whole GPU","url":"https://hazyresearch.stanford.edu/blog/2025-09-28-tp-llama-main","date":1759093218,"author":"sydriax","guid":95,"unread":true,"content":"<p> We're releasing a throughput-optimized megakernel for tensor-parallel inference with Llama-70B on H100s. Our kernel can aggressively overlap compute, memory, and communication ops in order to simultaneously use the different hardware resources available on a GPU. When integrated into the Tokasaurus inference engine, our megakernel can outperform SGLang by  on end-to-end throughput (measured as time to finish 65,536 prompts from the ShareGPT benchmark). We're releasing the code <a href=\"https://github.com/HazyResearch/Megakernels/tree/throughput\">here</a>; please be warned that this  is research code; it is sensitive to compiler versions, GPU setup, and sometimes even being looked at the wrong way, and we have no intention whatsoever of supporting it. We hope you'll find the ideas and results interesting nonetheless!</p><div><img src=\"https://hazyresearch.stanford.edu/static/posts/2025-09-28-tp-llama-main/results.png\" alt=\"Performance comparison graph\"></div><p>A few months ago, we <a href=\"https://hazyresearch.stanford.edu/blog/2025-05-27-no-bubbles\">showed</a> how we could fuse an entire model forward pass into a single \"megakernel\" in order to deliver low-latency inference with Llama-1B. In that post, we teased that many of the same concepts we introduced would also be useful for optimizing for throughput. We're now excited to bring receipts and release a new megakernel optimized for high-throughput inference with Llama-70B.</p><p>The inference workloads targeted by our low-latency and high-throughput megakernels are quite different and require distinct optimizations. Our low-latency megakernel targeted inference using Llama-1B when running on a single GPU with batch size one. This workload was entirely memory bound, and our focus was therefore on eliminating stalls that delayed loading model weights from global memory.</p><p>With large-batch Llama-70B inference, our workload is much more heterogeneous. Large portions of it (e.g. matrix multiplies, attention prefill) are compute-bound. Other parts (e.g. attention decode, RMS norm) are still bottlenecked by global memory bandwidth. Additionally, by distributing our model across multiple GPUs, we now need to perform cross-GPU communication that throttles the NVLink connections between devices. By running these components sequentially, we've paid for the whole GPU, but are only using little bits and pieces of it at a time. :(</p><p>Overall, these different operations in our model each make use of different resources available on the GPU (e.g. tensor cores, non-matmul compute units, HBM bandwidth, NVLink bandwidth) in unique ways. Therefore, a key area for optimizing this high-throughput workload is to <em>overlap multiple kinds of work</em> in order to simultaneously use more of the GPU's resources. We want to do this across many levels of the GPU -- within an individual SM, across multiple SMs, and even across GPUs.</p><p>In the rest of this blog, we will:</p><ol><li>Give a brief recap on the design of our megakernels from our last, low-latency post.</li><li>Walk through the details of the tensor-parallel Llama forward pass that we map into our megakernel, including a novel approach to communicating intermediate results across GPUs right after running attention. This new operation requires a complicated multi-GPU transpose not efficiently expressable with standard communication patterns, but is trivial to implement within the megakernel!</li><li>Show how megakernels can achieve fine-grained resource overlapping at multiple levels of the GPU hierarchy: within individual SMs, across multiple SMs, and across multiple GPUs!<ul><li>Within individual SMs, the same inter-instruction pipelining we used in low-latency llama can also help keep overlap memory movement and compute across instructions, thereby keeping the tensor cores running.</li><li>Across multiple SMs, careful scheduling of instructions can overlap both compute-intensive (e.g. matrix multiply) and memory-intensive (e.g. RMS norm) kinds of work at once, on an individual GPU.</li><li>Across GPUs, we can hide communication costs within special \"storer\" threads, leaving other threads free to do work on the next instruction while communication happens in the background.</li></ul></li><li>Finally, we put it all together by benchmarking our megakernel against vLLM and SGLang.</li></ol><h2>Megakernels: A Brief Recap</h2><p><a href=\"https://hazyresearch.stanford.edu/blog/2025-05-27-no-bubbles\">In our last post</a>, we wrote our first full-model megakernel in order to optimize a low-latency scenario: running inference with Llama-3.2-1B and batch size one. We discovered that popular inference engines like vLLM and SGLang were only using about half of the available GPU bandwidth on an H100. The problem is that traditional systems break down model forward passes into dozens or hundreds of separate kernels, each with setup and teardown periods where no useful work gets done. These overhead periods create \"memory pipeline bubbles\" where an SM (i.e. a streaming multiprocessor, one of the compute subunits on a GPU) sits idle instead of loading model weights. Our solution to this was to merge the entire Llama-1B forward pass into a single fused \"megakernel\" that eliminates kernel boundaries altogether. We found that on small models, our megakernel could provide per-user throughput around 50% higher than inference frameworks like SGLang and vLLM.</p><p>The core abstraction behind our megakernel lays in an instruction-and-interpreter model.</p><ul><li> Instead of decomposing a model forward pass into a series of coarse-grained kernels, we instead decomposed it into a sequence of fine-grained . Instructions can have distinct types, loosely corresponding to the kinds of kernels one would use in conventional implementations (e.g. matrix multiply, attention prefill, RMS norm). Each instruction specifies a unit of work that would traditionally be performed by a thread block, e.g. compute an output tile for a matrix multiplication. Furthermore, each instruction is organized into dedicated sections, e.g. a load function that reads from global memory, a compute function, and a store function that writes out results.</li><li> We execute these instructions using an on-GPU interpreter. When the megakernel launches, each SM initializes an interpreter and starts executing a sequence of instructions (these sequences are  into per-SM queues ahead of time). A key feature of these interpreters is that they can aggressively pipeline across instruction boundaries, starting tasks for the next instruction (e.g. loading model weights) while the current instruction finishes. For our low-latency megakernel, this let us eliminate most of the memory bubbles between operations. For more details on the interpreter design (e.g. how we manage shared memory across instructions, how we synchronize different SMs), see the original <a href=\"https://hazyresearch.stanford.edu/blog/2025-05-27-no-bubbles\">blog post</a> and the <a href=\"https://github.com/HazyResearch/Megakernels/tree/main/demos/low-latency-llama\">codebase</a>.</li></ul><p>In this blog, we'll focus on a high-throughput workload with very different performance considerations than our previous Llama-1B target. However, as we'll describe below, this same core instruction/interpreter abstraction will be extremely helpful for achieving high throughput.</p><p>First, we'll start with a brief walkthrough of the operations needed to perform a large-batch forwards pass using tensor-parallel Llama-70B. Specifically, we implement the \"<a href=\"https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/features/parallelisms.html#sequence-parallelism\">sequence parallel</a>\" variant of TP, where some operations are performed data-parallel (i.e. each GPU holds full activation vectors for a slice of the tokens in a batch) and some operations are performed tensor-parallel (i.e. each GPU holds a slice of the activation vectors for all tokens). Concretely, with sequence parallelism each transformer block receives a data-parallel chunk of the hidden states (i.e. the full hidden states for a subset of tokens) as input and performs the following operations:</p><ul><li>Data-parallel pre-attention RMS norm.</li><li>All-gather (i.e. each GPU collects the activations from all other GPUs, so that each GPU now has the activations from all tokens).</li><li>Tensor-parallel QKV projections, attention, and O projection (each GPU is responsible for a subset of attention heads).</li><li>Data-parallel post-attention residual connection and pre-MLP RMS norm.</li><li>Post-MLP residual connection.</li></ul><p>However, we've made one important change to this formulation. One of our targets for operation overlapping is to overlap the O projection matrix multiplication with the subsequent reduce-scatter. However, the tensor-parallel sharded O matrix is too small for us to effectively hide the reduce-scatter communication cost. To solve this, we instead choose to replicate the O projection matrix across each GPU and run the O projection with data parallelism instead of with tensor parallelism. Alongside this change, we eliminate the post-attention reduce-scatter and replace it with a \"distributed transpose\" operation after attention that repartitions our data from a tensor-parallel configuration to a data-parallel configuration.</p><p>When using 8 GPUs, this reduces the network traffic by a factor of 8, which makes it much easier to hide the cost of communication cost by overlapping it with matrix multiplications. Note that the downside of this approach is it reduces the maximum batch size by about 15%, because replicating the O projection weights consumes an additional 9 GB of memory per GPU.</p><h3>Defining our Megakernel Instruction Set</h3><p>With our parallelism scheme decided on, we are able to construct the instruction set for our high-throughput megakernel. We partition our workload into the following fused instructions:</p><ol><li>A QKV matrix multiply + RoPE.</li><li>Attention + distributed transpose.</li><li>O-projection matrix multiply + residual.</li><li>Gate matrix multiply + SiLU.</li><li>Up-projection matrix multiply + elementwise multiplication with the output of the gate.</li><li>Down matrix multiply + reduce-scatter + residual.</li><li>RMS norm without the all-gather (for the LM head)</li></ol><p>Relative to our latency-focused Llama-1B megakernel, this instruction set contains several high-level changes in our approach:</p><ul><li>Most of our instructions for low-latency centered around matrix-vector multiplication, rather than the matrix-matrix multiplications we do here. The optimal work partitioning for these two operations is generally completely different. For matrix-vector products, each instruction computes several complete columns of the output vector. However, in matrix-matrix products, each instruction computes a  of the output matrix instead.</li><li>To avoid extra trips to global memory, for our low-latency megakernel we frequently recomputed results across the GPU rather than communicating them through memory. This allowed us to fuse operations more aggressively than usual. For example, our QKV matrix-vector product fused the RMS norm into its beginning, saving an instruction boundary, but performing identical RMS computations over a hundred times. When focusing on throughput, this recomputation is not worth it.</li><li>Putting many tokens in our batch requires us to expand our inter-instruction signalling scheme [link to relevant section in llama-1b post] to track data dependencies across tokens. This signalling can vary by instruction -- for some operations (e.g. attention, RMS norms), sometimes synchronization is at the granularity of 128 output rows of a matrix; other times it's at the granularity of an individual attention head for an individual token.</li></ul><h2>Overlapping Resources Within a Megakernel</h2><p>Our primary goal when optimizing forward pass for throughput is to overlap hardware resources (e.g. use as much GPU memory bandwidth, compute units, and interconnect bandwidth as possible). Below, we show that our megakernel allows us to do this at three levels of hierarchy: within an SM by overlapping the stages of different instructions, across SMs by running different instructions, and across GPUs by overlapping communication with other work.</p><h3>Overlapping within the SM</h3><p>Within individual SMs, we make use of our megakernel template's instruction pipeline (previously described <a href=\"https://hazyresearch.stanford.edu/blog/2025-05-27-no-bubbles\">here</a>) to overlap loading weights and activations for the next instruction with performing compute for the previous instruction. Even though our objective is now throughput, it's still useful to be able to start loading the next data in advance -- to keep matrix multiplies running as quickly as possible.</p><p>Within the SM, our interpreter specializes threads to different functions: each of the load, compute, and store functions within the instruction template is executed by its own, independent set of threads. This means that even while a compute or store is running, the loads for the next matrix multiplies can start as soon as possible.</p><p>To help understand this, we've written profiling tools that make it easier to see what's going on here. Figure 2, below, shows the brief transition between two different kinds of instructions on a single SM -- the Gate SiLU instruction (brown) and the Up matmul instruction (pink). If you'd like to build a better intuition for the profiler, you can <a href=\"https://benjaminfspector.com/misc/megaprofiler\">access it here</a>, alongside an example profile to download and play with.</p><div><img src=\"https://hazyresearch.stanford.edu/static/posts/2025-09-28-tp-llama-main/pipelined_micro.png\" alt=\"SM transition profile\"><p><em>Figure 2: A zoomed-in snapshot of a single SM across about 15 microseconds, as it transitions from a Gate SiLU instruction to an Up matmul instruction.</em></p></div><p>First, a quick tutorial on how to read what's going on in this zoomed-in profile snapshot:</p><ul><li>The three horizontal tracks represent different kinds of threads within the interpreter that runs instructions.<ul><li>At the top are the loader threads, which pull data from global memory into shared memory.</li><li>The thick band in the middle represents consumer threads that perform the main work -- in this case, running the tensor cores.</li><li>The bottom row tracks storer threads, which store results from shared memory back up to global memory.</li><li>Finally, in the background are controller threads that help coordinate the interpreter, although they're not programmed by the user.</li></ul></li><li>Different colored bars represent different kinds of instructions. In this case, the brown bars on the left correspond to the Gate SiLU instruction, and the pink bars on the right correspond to the Up matmul instruction.</li><li>Thin vertical lines represent different kinds of events. For example, blue and cyan lines correspond to different kinds of loads being issued. Purple lines represent the beginning of a compute phase, and yellow and orange lines correspond to different kinds of stores. Finally, red lines represent wait events, and green lines represent ready states. In general, we have instructions only report events from the first 8 and last 4 stages, since we have limited timing slots.</li><li>Tall vertical lines in the background represent events happening within the controller warp of the interpreter. The salmon line tells the SM to fetch the next instruction, the pale green line indicates that the next instruction is set up, and the white line indicates the last instruction has finished and can now be torn down.</li></ul><p>Here's a complete timeline of this little snapshot.</p><div><table><tbody><tr><td><p>Starts issuing loads for last four stages of matrix multiply pipeline (128 x 64 x 256).</p><p>Dark blue lines = A matrix loads, cyan lines = B matrix loads (300ns later)</p></td></tr><tr><td><p>Signals controller to begin setting up next instruction (after fourth-to-last load)</p></td></tr><tr><td><p>Begins running fourth-to-last matrix multiply (associated with first load)</p></td></tr><tr><td><p>Finishes setting up next Up matmul instruction, entailing fetching and decoding instruction,\nsetting up semaphores, and remapping shared memory pages.</p></td></tr><tr><td><p>Begin running dependency check before loading inputs.</p></td></tr><tr><td><p>Finish running matrix multiplies, begin storing results into two unreleased shared memory pages</p></td></tr><tr><td><p>Resolves dependency check, and issues loads for first 2.5 stages, before stalling due to lack of available shared memory pages.</p></td></tr><tr><td><p>Receives results, launches asynchronous store to global memory.</p><p>Consumer threads start Up matmul matrix multiplies while shared memory still in use</p></td></tr><tr><td><p>Begins running matrix multiplies on Up matmul instruction.</p></td></tr><tr><td><p>First store finishes reading from shared memory, releases one page.</p><p>Loader threads restart load pipeline.</p></td></tr><tr><td><p>Final store finishes reading from shared memory, releases last page to Up matmul instruction.</p><p>Up matrix multiply pipeline completely unblocked for the rest of the instruction.</p></td></tr><tr><td><p>Asynchronous stores to global memory complete.</p><p>Atomically increments flag in global memory signifying instruction completion.</p></td></tr><tr><td><p>Notified all threads completed instruction work.</p><p>Begins teardown, invalidating previous instruction semaphores, and writing timing data to global memory.</p></td></tr></tbody></table></div><p>Now let's contrast this with a snapshot with the instruction pipeline disabled.</p><div><img src=\"https://hazyresearch.stanford.edu/static/posts/2025-09-28-tp-llama-main/no_pipelined_micro.png\" alt=\"Profile without pipelining\"><p><em>Figure 3: The exact same profile, but with no inter-instruction pipelining.</em></p></div><p>In this ablated profile, the store must finish, instruction be torn down, next instruction set up, and memory loaded, before matrix multiplies can begin again. Whereas with instruction pipelining enabled, the extra gap between consecutive matrix multiply stages is just 3.4 microseconds, without the pipeline, this gap jumps to 10.2 microseconds -- meaning this optimization alone reduces runtime by over 7% on these instructions.</p><p>Nor is this effect isolated to the boundary of these two particular instructions; it shows up everywhere. In Figure 4, we take a look at some zoomed out profiles showing all 8 GPUs, and we'll use similar profiles for other ablations in the rest of this post, so it's worth understanding this profile well.</p><div><img src=\"https://hazyresearch.stanford.edu/static/posts/2025-09-28-tp-llama-main/no_pipelined.png\" alt=\"Serial execution profile\"><p><em>Figure 4: Block profiles of pipelined versus serial instruction execution. Serial execution has a lot more gaps!</em></p></div><p>What we're looking at here represents a little over two full transformer blocks of a Llama-70B forward pass with a batch size of 8,192, across all 8 GPUs. Unlike the zoomed-in view, where each SM separates into three separate bars, each horizontal bar here just represents the activity of the consumers for that SM. And, as before, each different color represents a different kind of instruction. From left to right:</p><ul><li>Blue represents the Attention RMS norm and all-gather.</li><li>Orange represents the QKV matrix multiply.</li><li>Green represents the attention and inter-GPU transpose.</li><li>Red represents the O-projection matrix multiply.</li><li>Purple represents the MLP RMS norm and all-gather.</li><li>Brown represents the Gate SiLU.</li><li>Pink represents the Up matrix multiply.</li><li>Grey represents the Down projection matrix multiply.</li></ul><p>We also lightly shade the background to make it easier to distinguish SMs on different GPUs: if you look closely you should see light color bands to help make this visible.</p><p>In the case of figure 4, the pipelined profile runs at 31,516 decoding TPS for this particular test workload (Prefill \"tell me a funny joke about cookies\", 30 decode tokens -- we have absolutely cornered the market on efficiently generating short cookie jokes), whereas the serial profile runs at just 29,607 TPS, corresponding to a difference of over 6%. This difference turns out to persist across different batch sizes, and generally provides around 2-6% end to end MFU, as shown in the table below.</p><table><thead><tr><th>Best config minus pipelining</th></tr></thead><tbody></tbody></table><p><em>Table 1: Decoding TPS Generating Short Cookie Jokes at Various Batch Sizes, with and without Pipelining</em></p><p>As a final note of this section, relating to the profiling itself, one might reasonably ask what overhead generating these plots incurs. It turns out to be very little: across 32 separate experiments we ran in the course of writing this post, we measured each runtime with and without generating timing data, each of which is a separate compilation path. We found the average difference to be just 0.39%, with a maximum of 1.07%. So, although timing may introduce a small amount of distortion, we think that this data is overall quite reliable. All TPS numbers are reported without timing recording enabled.</p><p>With our low-latency megakernel, each SM was assigned its own queue of instructions that are scheduled in advance. Instead, for our high-throughput megakernel, we create a global work queue -- a single list of instructions that defines all the work that needs to run on the GPU. When an SM needs to fetch a new instruction to run, it atomically increments a global instruction counter that keeps track of the next instruction to be assigned. This approach is automatically robust to jitter in the execution across different SMs; if one SM is slow to finish its instruction relative to others, it will simply delay its request for new work, allowing other SMs to pick up the slack. This solution wasn't possible for our low-latency megakernel, because the runtime of each instruction was so fast that the latency of this atomic increment would be prohibitive. But with a throughput-oriented megakernel -- where individual instructions frequently take 100 microseconds or more -- this cost can be entirely hidden as part of our instruction pipeline.</p><div><img src=\"https://hazyresearch.stanford.edu/static/posts/2025-09-28-tp-llama-main/no_gwq.png\" alt=\"Round robin scheduler profile\"><p><em>Figure 5: Ablating the global work queue. On top, we use the global work queue. On the bottom, we use a simple round robin scheduler to assign work. The global work queue effectively smooths out variances in runtime that are present in the round-robin scheduler.</em></p></div><p>In figure 5, we ablate the global work queue by replacing it with a simple round-robin scheduler, and find a 14.2% end-to-end reduction in performance at a batch size of 8,192. A broader report is provided in table 2.</p><table><thead><tr></tr></thead><tbody></tbody></table><p><em>Table 2: Decoding TPS Generating Short Cookie Jokes at Various Batch Sizes, with and without the Global Work Queue</em></p><p>As one can see from table 2, the global work queue becomes useful at large batch sizes, where there is enough that work that jitter across SM's becomes important, and eliminating that jitter with dynamic scheduling makes a big difference. At very small batch sizes, the overhead in the global work queue actually outweighs its benefit.</p><p>Finally, it's worth noting that the global work queue is not the only way to improve scheduling over a naive round-robin scheduler; many other schedulers might work well. However, static schedulers cannot adapt to runtime jitter in the same way that the global work queue does; the variance in runtime across GPUs in figure 5 (despite them having nearly identical schedules) suggests that this jitter is a major factor.</p><p>In order to implement our tensor-parallel Llama, we need to be able to exchange data between GPUs. In general, NVIDIA gives us two ways to do this. One common approach is to use the GPU's copy engines -- dedicated hardware for copying big, contiguous chunks of memory within or across devices. One advantage of using the copy engines is that these copies don't need to run within a kernel, freeing up the GPU's SMs to do other useful work! By using multiple cuda streams, we can launch copy engine operations that overlap with kernel computations (e.g. as is done in PyTorch's <a href=\"https://discuss.pytorch.org/t/distributed-w-torchtitan-introducing-async-tensor-parallelism-in-pytorch/209487\">AsyncTP</a>).</p><p>The other way to transfer data between GPUs is to do so  a kernel, using the GPU's SMs to write data into remote memory on other GPUs via CUDA's unified memory architecture (thanks <a href=\"https://en.wikipedia.org/wiki/Bill_Dally\">Bill</a>!) We've added a corresponding new abstraction to <a href=\"https://github.com/HazyResearch/ThunderKittens\">ThunderKittens</a> called the Parallel Global Layout (PGL). With PGLs, we perform asynchronous loads and stores directly to global memory on other devices, overlapping them with compute and local memory operations to achieve near-zero cost. We also leverage NVSwitch's central accelerator to offload collective operations to hardware outside the GPU. Read more about PGLs and our multi-GPU approach in <a href=\"https://hazyresearch.stanford.edu/blog/2025-09-22-pgl\">our earlier blog post</a>.</p><p>In our megakernel we use the second approach, because it gives us the control we need to perform all-gathers, reduce-scatters, and our post-attention distributed transpose (which allows us to do the O-projection in data-parallel form). We perform all communication from our dedicated storer threads, allowing loader and compute threads to move onto future work while inter-GPU communication is performed in the background on the same SM.</p><p>Warp specialization, instruction pipelining, and the global work queue now give us a way to overlap different hardware resources within an SM. However, we can further benefit from GPU resource overlapping if we can <em>assign different types of instructions to different SMs</em>.</p><p>For example, with a large batch size, tokens at the beginning of the batch will have their compute-bound Down projections completed earlier than tokens at the end of the batch. This makes these early tokens ready to start the next instruction, which is the network-bound pre-attention RMS norm and all-gather. If we run that RMS norm and all-gather for these early tokens on some SMs, while computing the Down projection for the later tokens on different SMs, we can reduce peak network bandwidth and better exploit the hardware resources available on the device.</p><p>Some prior work, like <a href=\"https://arxiv.org/abs/2408.12757\">NanoFlow</a>, implemented this technique on A100s by constructing a schedule ahead-of-time that assigns SMs to different groups, namely compute-focused SMs that compute matmul-heavy ops, memory-focused instructions that compute attention decoding, and comms-focused SMs that communicate results across devices. With our megakernel, we can perform this overlapping at a much finer granularity by <em>interleaving instructions from different ops</em> into our global work queue. Once we have scheduled enough Down projection instructions for some tokens to be ready for attention, we can start interleaving RMS norm instructions while we add the remaining Down projection instructions into our schedule. This interleaving lets different SMs run different kinds of work without needing to explicitly assign SMs to groups like in NanoFlow.</p><div><img src=\"https://hazyresearch.stanford.edu/static/posts/2025-09-28-tp-llama-main/no_interleave.png\" alt=\"Non-interleaved schedule profile\"><p><em>Figure 6: Ablating interleaving. On top, we use our standard interleaved schedule. On the bottom, we disable interleaving. Notice how the two RMS norms (purple and blue) are no longer quilted the same way, but instead take up 200-300 microseconds each time they come up. (Note that although QKV and Gate SiLU instructions may begin early, they rely on many RMS norms to satisfy their dependencies, and usually start after all of the RMS norm instructions are finished.)</em></p></div><p>The effect of the interleaving is easy to see in our profiles, since it creates \"quilts\" (i.e. dense multi-colored regions where different SMs are running different kinds of instructions at a given point in time).</p><table><thead><tr><th>Best config minus interleaving</th></tr></thead><tbody></tbody></table><p><em>Table 3: Decoding TPS Generating Short Cookie Jokes at Various Batch Sizes, with and without Interleaving</em></p><p>As one can see from table 3, interleaving also kicks in at large batch sizes (like the global work queue), where there is enough work on the GPU to allow several waves of instructions of each type, and therefore the effective interleaving of those waves. At these large batch sizes, it's just as important of an optimization as intra-SM pipelining.</p><p>The key advantage of our approach is that it eliminates the overhead of additional kernel launches and cross-stream synchronization. We also note that we've removed our reliance on NCCL entirely, continuing in our march towards <a href=\"https://youtu.be/WJngbyHgS8E?si=pqPWR4uk6_qDHzjj&amp;t=32\">Obadiah Stane's paragon of minimal dependencies</a>. Of course, there's been an astounding amount of work attempting to hide communication overhead alongside compute -- it was surprising to us how straightforward it was to overlap communication within the megakernel framework!</p><p>To evaluate our megakernel, we integrated it into <a href=\"https://scalingintelligence.stanford.edu/blogs/tokasaurus/\">Tokasaurus</a>, which helps schedule batches of prefill and decode, alongside KV pages. This also allows us to schedule megakernel instructions on the CPU while the previous batch is running on the GPUs; with 64 threads generating instructions in C++, we generally find &gt;90% CPU idle time.</p><p>For our benchmark, we sampled a set of 65,536 prompts + completion lengths from the ShareGPT dataset, and ran them through both SGLang and our Megakernel. We reproduced SGLang using their recommended <a href=\"https://docs.sglang.ai/developer_guide/benchmark_and_profiling.html\">benchmarking settings</a>; nonetheless, we recognize that expert tuning is sensitive and important. We report the input, output, and total throughputs in Table 4 (a more precise restatement of the results from Figure 1).</p><table><thead><tr><th>Input Throughput (Tokens/s)</th><th>Output Throughput (Tokens/s)</th><th>Total Throughput (Tokens/s)</th></tr></thead><tbody><tr></tr></tbody></table><p>Even despite these promising initial results, we suspect there's still considerable room for optimization within megakernels. Our scheduling heuristics are quite simple, our instructions frequently stall on synchronizations that could likely be hidden better, and our megakernel still has register spills and other low-level problems. All of these point towards this being an exciting direction for future work!</p><h2>Conclusion: Megakernels are Cool</h2><p>In this post, we introduced a tensor-parallel Llama-70B megakernel focused on maximizing decoding throughput. We designed a custom instruction set for this megakernel within our megakernel interpreter framework, and ablated several key scheduling decisions including pipelining across instruction boundaries, choosing processors for each instruction, and interleaving communication with compute. Finally, we integrated our megakernel into Tokasaurus, and found it outperformed SGLang by 22% on ShareGPT prompts.</p><p>A direction for future work: a key challenge of writing megakernels is that there is tremendous complexity in both designing these custom instruction sets, and coordinating (and especially debugging) synchronization patterns across GPUs. A corresponding learning from this work is that, going forward, we'd like to design a more general megakernel instruction set and abstract these decisions into the host-side scheduler, in order to simplify the process of designing high-performance megakernels. We think this might make megakernels for training viable, alongside megakernels for inference.</p><p>*Work by Jordan done while at Stanford.</p>","contentLength":28948,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45407953"},{"title":"The AI coding trap","url":"https://chrisloy.dev/post/2025/09/28/the-ai-coding-trap","date":1759074213,"author":"chrisloy","guid":94,"unread":true,"content":"<p>If you ever watch someone “coding”, you might see them spending far more time staring\ninto space than typing on their keyboard. No, they (probably) aren’t slacking off.\nSoftware development is fundamentally a practice of problem-solving, and so, as with\nsolving a tricky crossword, most of the work is done in your head.</p><p>In the software development lifecycle, coding is the letters filled into the crossword,\nonly a small amount of effort compared to all the head scratching and scribbled notes.\nThe real work\nusually happens alongside coding, as the developer learns the domain, narrows down\nrequirements, maps out relevant abstractions, considers side effects, tests features\nincrementally, and finally squashes bugs that survived this rigorous process. It looks\nsomething like this:</p><p>But with AI-driven coding, things play out very differently.</p><h2>“Code first, ask questions later”</h2><p>AI coding agents such as <a href=\"https://claude.com/product/claude-code\">Claude Code</a> are\nmaking it astonishingly fast to write code in isolation. But most software lives within\ncomplex systems, and since LLMs can't yet hold the full context\nof an application in memory at once, human review, testing, and integration needs will\nremain. And that is a lot harder when the code has been written without the human thinking\nabout it. As a result, for complex software, much of the time will be spent on post hoc\nunderstanding of what code the AI has written.</p><p>This is the root of the difference between marketing copy that boasts of the paradigm\nshifting speed of  with AI (often framed as\n“<a href=\"https://docs.coderabbit.ai/overview/introduction\">10X</a><a href=\"https://zencoder.ai/product/coding-agent\">faster</a>”), and the marginal productivity gains\nin <strong>delivering working software</strong> seen in the wild (usually\n<a href=\"https://www.microsoft.com/en/customers/story/22158-allpay-github-copilot\">closer to 10%</a>).</p><p>An even more dispiriting upshot of this is that, as developers, we spend an ever greater\nproportion of our time merely fixing up the output of these wondrous\n<a href=\"https://chrisloy.dev/post/2024/12/24/llms-are-scrappy-innovators\">babbling machines</a>. While the LLMs get to\nblast through all the fun, easy work at lightning speed, we are then left with all the\nthankless tasks: testing to ensure existing functionality isn’t broken, clearing out\nduplicated code, writing documentation, handling deployment and infrastructure, etc.\nVery little time is actually dedicated to the thing that developers actually love doing: coding.</p><p>Fortunately, help is at hand. While LLMs are shaking up how software development is\nperformed, this issue in itself is not actually new. In fact, it is merely a stark example\nof an age-old problem, which I call:</p><p>As engineers progress in their careers, they will eventually\nstep into the role of . They might be  a team,\nor they could be a , driving technical delivery without the\npeople management. In either case, they are responsible for the team’s technical\ndelivery. They are also usually the most experienced developer in the team: either in their career,\nin the specialised domain of the team, or in both.</p><p>Software delivery is a team effort, but one in which experience can have a highly imbalancing\neffect on individual contribution\n<a href=\"https://chrisloy.dev/post/2023/11/10/software-engineering-mechanics\">velocity</a>. As such, when the tech lead’s\nprimary job is to maximise delivery, they will often face an internal conflict between two\nways to deliver software:</p><ul><li> across the team, maximising learning and ownership opportunities for\njunior team members, but allowing delivery to be bottlenecked by the speed of the least productive\nteam members.</li><li> the team, by delegating only the easy or non-critical work to juniors,\nand keeping the hardest work for themselves, as the person on the team most capable of\ndelivering at speed.</li></ul><p>Unfortunately, while we shall see that mollycoddling is extremely harmful to long-term team\nhealth, it is also often a very effective way to accelerate delivery. The higher bandwidth of\nthe tech lead is often most efficiently deployed by eating up all the hardest work:</p><p>As such, I have seen this pattern repeated time and again over the course of my career. And,\nof course, it comes at a cost. Siloing of experience in the tech lead makes the team brittle,\nit makes support harder, and it places ever greater pressure on the tech lead as a single point\nof failure. What follows next is predictable: burnout, departure, and ensuing crisis as the\nteam struggles to survive without the one person who really knows how everything works.</p><p>As is usually the case, the solution lies in a third way that avoids these two extremes\nand balances delivery with team growth. We might frame it as something like:</p><blockquote><p><em>Implement team practices that allow engineers to deliver working code within a framework\nthat minimises rework, maximises effective collaboration, and promotes personal growth and\nlearning.</em></p></blockquote><p>When I was CTO of Datasine, we enshrined this attitude in a simple tech team motto:</p><blockquote><p><em><strong>Learn. Deliver. Have fun.</strong></em></p></blockquote><p>Good tech leads expose their engineers to work at the limit of their capabilities,\nusing processes and practices that minimise delivery risk while also enabling each team\nmember to grow their skills, knowledge, and domain expertise. This is, in fact, the essence\nof good technical leadership.</p><p>There are many ways to accomplish it, from strict codified frameworks such as the\n<a href=\"http://www.extremeprogramming.org/rules.html\">Extreme Programming rules</a>, through\nto looser sets of principles which we might broadly refer to as “best practices”:</p><ul></ul><p>So, for experienced engineers today, an urgent question is: how can we translate these practices\ninto a world of AI-driven coding?</p><h2>LLMs are lightning fast junior engineers</h2><p>In 2025, many engineers are finding themselves for the first time in a position familiar\nto every tech lead: overseeing a brilliant but unpredictable junior engineer. Harnessing\nand controlling such talent, in a way that benefits effective team collaboration, is one\nof the primary challenges of engineering leadership. But AI coding agents need different\nmanagement to junior engineers, because the nature of their productivity and growth is\nfundamentally different.</p><p>As software engineers gain experience, we tend to improve our productivity in multiple\nways at the same time: writing more robust code, using better abstractions, spending less\ntime writing and fixing bugs, understanding more complex architectures, covering edge\ncases more effectively, spotting repeated patterns earlier, etc. Engineering is a rich\nand complex discipline with many avenues for specialisation, but for simplicity we might\ngroup these dimensions into two broad themes:</p><ul><li>: ability to deliver more complex, more performant, more maintainable code</li><li>: ability to develop working, bug-free code in a shorter space of time</li></ul><p>Over time, good engineers will improve in both axes.</p><p>Early LLMs were fast to write code, but time spent fixing bugs and removing hallucinations\nmeant they were slow to complete bug-free code. Over time, smarter LLMs and better use of\ncontext engineering and tools have meant that modern AI coding agents are much better at\n“one shot” writing of code. The current generation of commercially available agents can be\nincredibly fast at producing working code for problems that would challenge some mid-level\nengineers, though they cannot yet match the expertise of senior engineers:</p><p>So we can think of the current generation of AI coding agents as junior engineers, albeit\nwith two fundamental differences:</p><ol><li>LLMs deliver code much, much faster than junior engineers, constrained neither by thinking\nnor writing time;</li><li>LLMs have no true capacity to learn, and instead only improve through more effective\n<a href=\"https://chrisloy.dev/post/2025/08/03/context-engineering\">context engineering</a> or\nthe arrival of new foundation models.</li></ol><p>As with junior engineering talent, there are broadly two ways that you can deploy them, depending on\nwhether your focus is long-term or short-term:</p><ul><li>: employing best practices, foregrounding human\nunderstanding of the code, moving slowly to make development sustainable.</li><li>: throwing caution to the wind and implementing at speed, sacrificing\nunderstanding for delivery velocity, hitting an eventual wall of unsalvageable, messy code.</li></ul><p>As might be expected, the long-term trajectories of choosing between these two approaches\nfollow much the same pattern as choosing between parallel delegation and mollycoddling of a\njunior team:</p><p>This is why the <a href=\"https://chrisloy.dev/post/2025/09/07/vibe-coding-art\">vibe coding</a> approach\nis great for tiny projects or throwaway prototypes: applications of sufficient simplicity\ncan be delivered without the need for any human thinking at all. By limiting the complexity\nof our projects and leaning into the capabilities of the tools, we can deliver end-to-end\nworking software in no time at all.</p><p>But you  hit a wall of complexity that AI is incapable of scaling alone.</p><p>Building prototypes is now easier than ever. But if we want to effectively use LLMs to\naccelerate delivery of real, complex, secure, working software, and to realise more than\nmarginal efficiency gains, we need to write a new playbook of engineering practices tailored\nto maximise collaboration between engineering teams that include both humans and LLMs.</p><h2>How to avoid the AI coding trap</h2><p>AI coding agents are dazzlingly productive, but lack in-depth knowledge of your business,\ncodebase, or roadmap. Left unchecked, they will happily churn\nout thousands of lines of code with no heed paid to design, consistency, or maintainability.\nThe job of the engineer, then, is to act as a tech lead to these hotshots: to provide the structure,\nstandards, and processes that convert raw speed into sustainable delivery.</p><p>We need a new playbook for how to deliver working software efficiently, and we can look to the\npast to learn how to do that. By treating LLMs as <strong>lightning-fast junior engineers</strong>, we\ncan lean on best practices from the software development lifecycle to build systems that scale.</p><p>Just as tech leads don't just write code but set practices for the team, engineers\nnow need to set practices for AI agents. That means bringing AI into every stage of\nthe lifecycle:</p><blockquote><p>: exploring, analysing, and refining feature specifications to cover edge cases and narrow focus.</p></blockquote><blockquote><p>: generating and reviewing documentation up front to provide reusable guardrails and lasting evidence.</p></blockquote><blockquote><p>: scaffolding modular architectures to control context scope and maximise comprehension.</p></blockquote><blockquote><p>: generating extensive test cases prior to implementation to guide implementation and prevent regression.</p></blockquote><blockquote><p>: applying house styles and best practice when generating code, through context engineering.</p></blockquote><blockquote><p><strong>Monitoring &amp; Introspection</strong>: analysing logs and extracting insights faster than any human ever could.</p></blockquote><p>By understanding that delivering software is so much more than just writing code, we can\navoid the AI coding trap and instead hugely amplify our ability to deliver working, scalable software.</p>","contentLength":10447,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45405177"},{"title":"When I say “alphabetical order”, I mean “alphabetical order”","url":"https://sebastiano.tronto.net/blog/2025-09-28-alphabetic-order/","date":1759064416,"author":"sebtron","guid":93,"unread":true,"content":"<p>Last month I have been on a multi-day hike with my dad. Each of us took\nmany pictures, and when we came back we put them all in a shared folder.\nWe both have Android phones, and the naming scheme used for our pictures\nwas the same:  followed maybe by some other numbers\nand then a . Here  stands for the year,  for month and\nso on, so that sorting the pictures in alphabetical order is the same as\nsorting them by date.</p><p>Or so I thought. Strangely, when I looked at the files from my dad’s\nWindows PC, they were not sorted correctly: all the pictures took\nwith my phone came first, followed by all the pictures took by him.\nI thought this was surely some weird Microsoft bug - after using\nWindows 11 at work for a while, I would not be surprised if you\ntold me their file explorer can’t figure out how to sort strings.</p><p>But then I looked at the same files in a shared Google Drive folder,\nand again they were in the wrong order:</p><p>As you can see, the picture taken at 5:54 (with my dad’s phone) comes\nbefore the one taken at 9:20 (also with my dad’s phone), but after the\none taken at 12:11 (with my phone).</p><p>Weird. Well, maybe Microsoft  Google got this wrong. But\nthat seems unlikely.</p><p>Indeed, KDE’s Dolphin file manager does the same thing:</p><p>I’ll spare you the screenshots, but Gnome and both the file managers\nthat I have on my phone also get the alphabetical order wrong.</p><p>At this point I thought that maybe one of the two phones is using some\nweird alternative unicode character instead of the underscore . Really,\nI could not see any other explanation. But nope, this is not it, because\nthe good old  sorts my files correctly:</p><pre><code>$ ls -l\n\ntotal 218572\n-rw-r--r-- 1 seba seba 1866185 Aug 28 18:51 IMG_20250820_055436307.jpg\n-rw-r--r-- 1 seba seba 4749899 Aug 28 18:50 IMG_20250820_092016029_HDR.jpg\n-rw-r--r-- 1 seba seba 6201609 Aug 28 18:52 IMG_20250820_092440966_HDR.jpg\n-rw-r--r-- 1 seba seba 7694802 Aug 28 18:51 IMG_20250820_092832138_HDR.jpg\n-rw-r--r-- 1 seba seba 1536520 Aug 20 09:57 IMG_20250820_095716_607.jpg\n-rw-r--r-- 1 seba seba 1054553 Aug 20 10:38 IMG_20250820_103857_991.jpg\n-rw-r--r-- 1 seba seba  965353 Aug 20 10:39 IMG_20250820_103903_811.jpg\n(and so on)\n</code></pre><p>This was consistent among the couple of Linux distros I use, as well\nas my OpenBSD server. On the one hand this is good: not  single\npiece of software fucks up something as basic as string sorting. On the\nother hand, this makes it harder to debug what the fuck is going on with\nall the other file managers.</p><p>It took me more than a month to figure this one out. Tell me, which\nfile do you think comes first in alphabetical order,  or\n?</p><p>Of course, the user who named those files probably wants  to\ncome before . But  is smaller than , so \nshould be first in alphabetical order. Everyone understands that, and\nsoon people learn to put enough leading zeros if they want their files\nto stay sorted the way they like.</p><p>Well, apparently all these operating systems have decided that no,\nusers are too dumb and they cannot possibly understand what alphabetical\norder means.  So when you ask them to sort your files alphabetically,\nthey don’t. Instead, they decide that if some piece of the file name is\na number, the real numerical value must be used.</p><p>I don’t know when this became the norm, to be honest I have not used a\nnormal graphical file manager in a long time.</p><p><em>I know you asked for the files to be sorted in alphabetical order,\nbut you don’t want  to come before , do\nyou? No, I know you don’t. I am not even going to ask you, your\nmushy human brain is too small to comprehend the intricacies of\nsuch a question. I’ll spare you the thinking.</em></p><p>So it turns out that my dad’s phone wrote the milliseconds in the file\nname right after the seconds, while mine added an extra underscore to\nseparate them from the seconds.  Which in my mind it should not have\nmattered, because alphabetically they should still have been sorted\ncorrectly to the second. But with this “modern” interpretation of the\nalphabetical order, the files without the extra separator in the name had\na much higher number, so they come last.</p><p>Now that I know what the issue is, I can solve it by renaming the files\nwith a consistent scheme. I have also found a setting to fix Dolphin’s\nbehavior, but it was very much buried into its many configuration\noptions. And I would rather not have to change this setting in every\napplication I use, assuming they even allow it.</p><p>I miss the time when computers did what you told them to, instead of\ntrying to read your mind.</p>","contentLength":4509,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45404022"},{"title":"Privacy Badger is a free browser extension made by EFF to stop spying","url":"https://privacybadger.org/","date":1759064394,"author":"doener","guid":92,"unread":true,"content":"<p>Privacy Badger is a browser extension that stops advertisers and other third-party trackers from secretly tracking where you go and what pages you look at on the web.  If an advertiser seems to be tracking you across multiple websites without your permission, Privacy Badger automatically blocks that advertiser from loading any more content in your browser.  To the advertiser, it’s like you suddenly disappeared.</p><p>Privacy Badger was born out of our desire to be able to recommend a single extension that would:</p><ul><li>Automatically analyze and block any tracker or ad that violated the principle of user consent</li><li>Function well without any settings, knowledge, or configuration by the user</li><li>Use algorithmic methods to decide what is and isn’t tracking</li><li>Be produced by an organization that is unambiguously working for its users rather than for profit</li></ul><p>As a result, Privacy Badger differs from traditional ad-blocking extensions in two key ways. First, while most other blocking extensions prioritize blocking ads, Privacy Badger doesn’t block ads unless they happen to be tracking you; in fact, one of our goals is to incentivize advertisers to adopt better privacy practices.</p><p>Second, most other blockers rely on a human-curated list of domains or URLs to block. Privacy Badger is an algorithmic tracker blocker – we define what “tracking” looks like, and then Privacy Badger blocks or restricts domains that it observes tracking in the wild. What is and isn’t considered a tracker is entirely based on how a specific domain acts, not on human judgment.</p><p>Privacy Badger sends the <a href=\"https://globalprivacycontrol.org/\">Global Privacy Control</a> signal to opt you out of data sharing and selling, and the <a href=\"https://www.eff.org/issues/do-not-track\">Do Not Track</a> signal to tell companies not to track you. If trackers ignore these signals, Privacy Badger will learn to block them.</p><p>Beyond this, Privacy Badger comes with other advantages like cookie blocking, <a href=\"https://privacybadger.org/#How-does-Privacy-Badger-handle-social-media-widgets\">click-to-activate placeholders</a> for potentially useful tracker widgets (video players, comments widgets, etc.), and outgoing link click tracking removal on <a href=\"https://www.eff.org/deeplinks/2018/05/privacy-badger-rolls-out-new-ways-fight-facebook-tracking\">Facebook</a> and <a href=\"https://www.eff.org/deeplinks/2018/10/privacy-badger-now-fights-more-sneaky-google-tracking\">Google</a>.</p><p>Privacy Badger was created by the <a href=\"https://www.eff.org\">Electronic Frontier Foundation</a>, a nonprofit organization that protects your privacy and free expression online. We make free tools like Privacy Badger, publish educational guides, testify before lawmakers about technology, and fight for the public interest in court—all thanks to support from EFF’s members. If you want a better internet and a strong democracy, <a href=\"https://supporters.eff.org/donate/support-privacy-badger\">join the fight</a> against creepy online surveillance.</p><p>When you view a webpage, that page will often be made up of content from many different sources. For example, a news webpage might load the actual article from the news company, ads from an ad company, and the comments section from a different company that’s been contracted out to provide that service.</p><p>Privacy Badger keeps track of all of this. If the same source seems to be tracking across different websites, then Privacy Badger springs into action, telling the browser not to load any more content from that source. And when your browser stops loading content from a source, that source can no longer track you. Voila!</p><p>At a more technical level, Privacy Badger keeps track of the “third party” domains that embed images, scripts and advertising in the pages you visit. Privacy Badger looks for tracking techniques like uniquely identifying cookies, local storage “supercookies,” and canvas fingerprinting. If it observes the same third-party host tracking on three separate sites, Privacy Badger will automatically disallow content from that third-party tracker.</p><p>By default, Privacy Badger receives <a href=\"https://www.eff.org/deeplinks/2023/10/privacy-badger-learns-block-ever-more-trackers\">periodic learning updates</a> from <a href=\"https://github.com/EFForg/badger-sett\">Badger Sett</a>, our Badger training project. This “remote learning” automatically discovers trackers present on thousands of the most popular sites on the Web.</p><p>When you visit a webpage parts of the page may come from domains and servers other than the one you asked to visit. This is an essential feature of <a href=\"https://en.wikipedia.org/wiki/Hypertext\">hypertext</a>. On the modern Web, embedded images and code often use cookies and other methods to track your browsing habits — often to display advertisements. The domains that do this are called “third party trackers”, and you can read more about how they work <a href=\"https://www.eff.org/wp/behind-the-one-way-mirror\">here</a>.</p><p>Red means that content from this third party domain has been completely disallowed.</p><p>Yellow means that the third party domain appears to be trying to track you, but it is on Privacy Badger’s cookie-blocking “yellowlist” of third party domains that, when analyzed, seemed to be necessary for Web functionality. In that case, Privacy Badger will load content from the domain but will try to screen out third party cookies and referrers from it.</p><p>Green means “no action”; Privacy Badger will leave the domain alone.</p><p>Actually, nothing in the Privacy Badger code is specifically written to block ads. Rather, it focuses on disallowing any visible or invisible “third party” scripts or images that appear to be tracking you even though you specifically denied consent by sending <a href=\"https://www.eff.org/issues/do-not-track\">Do Not Track</a> and <a href=\"https://globalprivacycontrol.org/\">Global Privacy Control</a> signals. It just so happens that most (but not all) of these third party trackers are advertisements. When you see an ad, the ad sees you, and can track you. Privacy Badger is here to stop that.</p><p>Because Privacy Badger is primarily a privacy tool, not an ad blocker. Our aim is not to block ads, but to prevent non-consensual invasions of people’s privacy because we believe they are inherently objectionable. We also want to create incentives for advertising companies to do the right thing. Of course, if you really dislike ads, you can also install a traditional ad blocker.</p><p><a href=\"https://globalprivacycontrol.org/\">Global Privacy Control (GPC)</a> is a new specification that allows users to tell companies they’d like to opt out of having their data shared or sold. By default, Privacy Badger sends the GPC signal to every company you interact with alongside the Do Not Track (DNT) signal.</p><p>What’s the difference? Do Not Track is meant to tell companies that you don’t want to be tracked in any way (learn more about what we mean by “tracking” <a href=\"https://www.eff.org/pages/understanding-effs-do-not-track-policy-universal-opt-out-tracking\">here</a>). Privacy Badger gives third-party companies a chance to comply with DNT by adopting <a href=\"https://www.eff.org/dnt-policy/\">our DNT policy</a>, and blocks those that look like they’re tracking you anyway.</p><p>When DNT was developed, many websites simply ignored users’ requests not to be tracked. That’s why Privacy Badger has to act as an enforcer: trackers that don’t want to comply with your wishes get blocked. Today, users in many jurisdictions have the legal right to opt out of some kinds of tracking. That’s where GPC comes in.</p><p>GPC is meant to be a legally-binding request to all companies in places with applicable privacy laws. For example, <a href=\"https://theccpa.org\">the California Consumer Privacy Act</a> gives California residents the right to opt out of having their data sold. By sending the GPC signal, Privacy Badger is telling companies that you would like to exercise your rights.</p><p>The CCPA and other laws are <a href=\"https://advocacy.consumerreports.org/press_release/consumer-reports-study-finds-significant-obstacles-to-exercising-california-privacy-rights/\">not perfect</a>, which is why Privacy Badger uses both approaches. It asks websites to respect your privacy, and it blocks known trackers from loading at all.</p><p>At present, Privacy Badger primarily protects you against tracking by third party sites. As far as privacy protections for “first party” sites (sites that you visit directly), Privacy Badger removes outgoing link click tracking on <a href=\"https://www.eff.org/deeplinks/2018/05/privacy-badger-rolls-out-new-ways-fight-facebook-tracking\">Facebook</a> and <a href=\"https://www.eff.org/deeplinks/2018/10/privacy-badger-now-fights-more-sneaky-google-tracking\">Google</a>. We plan on adding more first party privacy protections in the future.</p><p>We are doing things in this order because the most scandalous, intrusive and objectionable form of online tracking is that conducted by companies you’ve <a href=\"https://lumapartners.com/content/lumascapes/display-ad-tech-lumascape/\">often never heard of</a> and have no relationship with. First and foremost, Privacy Badger is there to enforce Do Not Track against these domains by providing the technical means to restrict access to their tracking scripts and images. The right policy for whether nytimes.com, facebook.com or google.com can track you  – and the technical task of preventing it – is more complicated because often tracking is interwoven with the features the site offers.</p><p>Unlike other blocking tools, we have not made decisions about which sites to block, but rather about which behavior is objectionable. Domains will only be blocked if Privacy Badger observes the domain collecting unique identifiers after it was sent Do Not Track and Global Privacy Control signals.</p><p>Privacy Badger  contain a “<a href=\"https://github.com/EFForg/privacybadger/blob/master/src/data/pbconfig.json\">yellowlist</a>” of some sites that are known to provide essential third party resources; those sites show up as yellow and have their cookies blocked rather than being blocked entirely. This is a compromise with practicality, and in the long term we hope to phase out the yellowlist as these third parties begin to <a href=\"https://www.eff.org/dnt-policy\">explicitly commit to respecting Do Not Track</a>. The criteria for including a domain on the yellowlist can be <a href=\"https://github.com/EFForg/privacybadger/blob/master/doc/yellowlist-criteria.md\">found here</a>.</p><p>The initial list of domains that should be cookie blocked rather than blocked entirely was derived from a <a href=\"https://jonathanmayer.org/papers_data/bau13.pdf\">research project</a> on classifying third party domains as trackers and non-trackers. We will make occasional adjustments to it as necessary. If you find domains that are under- or over-blocked, please <a href=\"https://github.com/EFForg/privacybadger/issues\">file a bug</a> on GitHub.</p><p>Browser fingerprinting is an extremely subtle and problematic method of tracking, which we documented with the <a href=\"https://coveryourtracks.eff.org/\">Cover Your Tracks project</a>. Privacy Badger can detect <a href=\"https://www.propublica.org/article/meet-the-online-tracking-device-that-is-virtually-impossible-to-block\">canvas-based fingerprinting</a>, and will block third party domains that use it. Detection of other forms of fingerprinting and protections against first-party fingerprinting are ongoing projects. Of course, once a domain is blocked by Privacy Badger, it will no longer be able to fingerprint you.</p><p>No. Privacy Badger analyzes the cookies from each site; unique cookies that contain tracking IDs are disallowed, while “low entropy” cookies that perform other functions are allowed. For instance a cookie like LANG=fr that encodes the user’s language preference, or a cookie that preserves a very small amount of information about ads the user has been shown, would be allowed provided that individual or small groups of users’ reading habits could not be collected with them.</p><p>We are working towards <a href=\"https://github.com/EFForg/privacybadger/issues/549#issuecomment-1209648999\">Safari on macOS</a> support. <a href=\"https://github.com/EFForg/privacybadger/issues/549#issuecomment-744583479\">Safari on iOS</a> seems to lack certain extension capabilities required by Privacy Badger to function properly.</p><p>Chrome on Android does not support extensions. To use Privacy Badger on Android, install <a href=\"https://play.google.com/store/apps/details?id=org.mozilla.firefox\">Firefox for Android</a>.</p><p>If you use Google Chrome, you have to install extensions from Chrome Web Store. To install Privacy Badger in Chrome, visit <a href=\"https://chromewebstore.google.com/detail/privacy-badger/pkehgijcmpdhfbdbbnkijodmdjhbjlgp\">Privacy Badger’s Chrome Web Store listing</a> and click the “Add to Chrome” button there.</p><p>Otherwise, you can use the following links to get the latest version of Privacy Badger directly from eff.org:</p><p>One way is to stop tracking users who have turned on Global Privacy Control or Do Not Track signals (i.e., stop collecting cookies, supercookies or fingerprints from them). Privacy Badger will stop learning to block that domain. The next version of Privacy Badger to ship with an updated pre-trained list will no longer include that domain in the list. Most Privacy Badger users will then update to that list.</p><p>You can also unblock yourself by promising to meaningfully respect the Do Not Track signal. To do so, post a  copy of <a href=\"https://www.eff.org/dnt-policy\">EFF’s Do Not Track policy</a> to the URL <a href=\"https://example.com/.well-known/dnt-policy.txt\">https://example.com/.well-known/dnt-policy.txt</a>, where “example.com” is replaced by your domain. Posting EFF’s DNT policy on a domain is a promise of compliance with EFF’s DNT Policy by that domain.</p><p>If your domain is compliant with EFF’s DNT policy and declares this compliance, most Privacy Badgers will see this declaration the next time they encounter your domain. Also, the next version of Privacy Badger to ship with an updated pre-trained list will probably include your declaration of compliance in the list.</p><p>Note that the domain must support HTTPS, to protect against tampering by network attackers. The path contains “.well-known” per <a href=\"https://tools.ietf.org/html/rfc5785\">RFC 5785</a>. Also note that you must post a copy of the policy at each compliant subdomain you control. For example, if you wish to declare compliance by both sub1.example.com and sub2.example.com, you must post EFF’s DNT policy on each domain.</p><p>Thanks for asking! Individual donations make up about half of EFF’s support, which gives us the freedom to work on user-focused projects. If you want to support the development of Privacy Badger and other projects like it, you can <a href=\"https://supporters.eff.org/donate/support-privacy-badger\">throw us a few dollars here</a>. Thank you.</p><p>Social media widgets (such as the Facebook Like button) often track your reading habits. Even if you don’t click them, the social media companies often see exactly which pages you’re seeing the widget on. When blocking social buttons and other potentially useful (video, audio, comments) widgets, <a href=\"https://www.eff.org/deeplinks/2024/01/privacy-badger-puts-you-control-widgets\">Privacy Badger can replace them</a> with click-to-activate placeholders. You will not be tracked by these replacements unless you explicitly choose to click them.</p><p>Privacy Badger should be compatible with other extensions.</p><p>While there is likely to be overlap between the various manually-edited advertising/tracker lists and Privacy Badger, unlike adblockers, Privacy Badger automatically learns to block trackers based on their behavior. This means that Privacy Badger may learn to block trackers your adblocker doesn’t know about.</p><p>It’s fine to use Firefox’s built-in content blocking (<a href=\"https://blog.mozilla.org/en/products/firefox/firefox-now-available-with-enhanced-tracking-protection-by-default/\">Enhanced Tracking Protection</a> or ETP) and Privacy Badger together. While there is overlap between Firefox’s tracker lists and Privacy Badger, Privacy Badger automatically learns to block trackers based on their behavior. This means that Privacy Badger’s automatically-generated and regularly updated blocklist contains trackers not found in Firefox’s human-generated lists. Additionally, <a href=\"https://support.mozilla.org/en-US/kb/enhanced-tracking-protection-firefox-desktop#w_what-enhanced-tracking-protection-blocks\">Firefox does not fully block “tracking content”</a> in regular (non-“private”) windows by default.</p><p>EFF uses Fastly to host EFF’s Web resources: Fastly is EFF’s CDN. Privacy Badger pings the CDN for the following resources to ensure that the information in them is fresh even if there hasn’t been a new Privacy Badger release in a while:</p><p>EFF does not set cookies or retain IP addresses for these queries.</p><p>When you install Privacy Badger, your browser warns that Privacy Badger can “access your data for all websites” (in Firefox), or “read and change all your data on the websites you visit” (in Chrome). You are right to be alarmed. You should only install extensions made by organizations you trust.</p><p>Privacy Badger requires these permissions to do its job of automatically detecting and blocking trackers on all websites you visit. We are not ironically (or unironically) spying on you. For more information, see our <a href=\"https://github.com/EFForg/privacybadger/blob/master/doc/permissions.md\">Privacy Badger extension permissions explainer</a>.</p><p>Note that the extension permissions warnings only cover what the extension has access to, not what the extension actually does with what it has access to (such as whether the extension secretly uploads your browsing data to its servers). Privacy Badger will never share data about your browsing unless you choose to share it (by filing a broken site report). For more information, see EFF’s <a href=\"https://www.eff.org/code/privacy/policy\">Privacy Policy for Software</a>.</p><p>Is YouTube not working? Try <a href=\"https://privacybadger.org/#I-found-a-bug%21-What-do-I-do-now\">disabling Privacy Badger</a> on YouTube. If that resolves the issue, see if re-enabling Privacy Badger breaks YouTube again. If YouTube goes back to not working, please <a href=\"https://privacybadger.org/#I-found-a-bug%21-What-do-I-do-now\">tell us</a> so we can look into what’s going on.</p><p>Are you surprised that ads aren’t being blocked on YouTube? Privacy Badger is primarily a privacy tool, <a href=\"https://privacybadger.org/#Why-doesn%27t-Privacy-Badger-block-all-ads\">not an ad blocker</a>. When you <a href=\"https://privacybadger.org/#What-about-tracking-by-the-sites-I-actively-visit%2c-like-NYTimes.com-or-Facebook.com\">visit YouTube directly</a>, Privacy Badger does not block ads on YouTube because YouTube does not use <a href=\"https://privacybadger.org/#What-is-a-third-party-tracker\">“third party” trackers</a>. If you really dislike ads, you can also install a traditional ad blocker.</p><p>If a website isn’t working like it should, you can disable Privacy Badger just for that site, leaving Privacy Badger enabled and protecting you everywhere else. To do so, navigate to the site with the problem, click on Privacy Badger’s icon in your browser toolbar, and click the “” button in Privacy Badger’s popup. You can also let us know about broken sites using the “” button.</p>","contentLength":15986,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45404021"},{"title":"Greenland is a beautiful nightmare","url":"https://matduggan.com/greenland-is-a-beautiful-nightmare/","date":1758988006,"author":"zdw","guid":91,"unread":true,"content":"<p>Greenland is a complicated topic here in Denmark. The former colony that is still treated a bit like a colony is something that inspires a lot of emotions. Greenland has been subjected to a lot of unethical experiments by Denmark, from taking their kids to wild experiments in criminal justice. But there is also a genuine pride a lot of people have here for the place and you run into Danes who grew up there more often than I would have guessed. </p><p>When the idea of going to Greenland was introduced to me, I was curious. Having lived in Denmark for awhile, you hear a lot about the former colony and its 55,000 residents. We were invited by a family that my wife was close with growing up and is Danish. They wanted to take their father back to see the place he had spend some time in during his 20s and had left quite an impression. A few drinks in, I said \"absolutely let's do it\", not realizing we had already committed to going and I had missed the text message chain. </p><p>A few weeks before I went, I realized \"I don't know anything about Greenland\" and started to watch some YouTube videos. It was about this time when I started to get a pit in my stomach, the \"oh god I think I've made a huge mistake\" feeling I'm painfully familiar with after a career in tech.  Greenland appeared to have roughly 9 people living there and maybe 5 things to look at. Even professional travel personalities seemed to be scraping the bottom of the barrel. \"There's the grocery store again!\" they would point out as they slipped down the snowy roads. I couldn't  tell any difference between different towns in the country.</p><p>It reminded me a lot of driving through Indiana. For those not in the US, Indiana is a state in the US famous for being a state one must drive through in order to get somewhere better. If you live in Michigan, a good state and want to go to Illinois, another good state, one must pass through Indiana, a blank state. Because of this little strip here, you often found yourself passing through this place. </p><p>Driving through Indiana isn't bad, it's just an empty void. It's like a time machine back to the 90s when people still smoke in restaurants but also there's nothing that sticks out about it. There is nothing distinct about Indiana, it's just a place full of people who got too tired on their way to somewhere better and decided \"this is good enough\". The difference is that Greenland is very hard to get to, as I was about to learn. </p><p>Finally the day arrived. Me, my wife, daughter, 4 other children and 6 other adults all came to the Copenhagen Airport and held up a gate agent for what felt like an hour to slowly process all of our documents. Meanwhile, I nursed a creeping paranoia that I'd be treated as some sort of American spy, given my government's recent hobby of threatening to purchase entire countries like they're vintage motorcycles on Craigslist.</p><p>The 5 hour flight is uneventful, the children are beautifully behaved and I begin to think \"well this seems ok!\" like the idiot I am. As I can look down and see the airport, the pilot comes on and informs us that there is too much fog to land safely. <em>Surely fog cannot stop a modern aircraft full of all these dials and screens</em> I think, foolishly. We are informed there is enough fuel to circle the airport for 5 hours to wait for the fog to lift. </p><p>What followed was three hours of flying in lazy circles, like a very expensive, very slow merry-go-round. After the allotted time, we are informed that we must fly to Iceland to refuel and then <em>we will be returning to Denmark</em>. After a total of 15 hours in the air we will be going back to exactly where we started, to do the entire thing again. We were obviously upset at this turn of events, but I noticed the native Greenlandic folks seemed not surprised at this turn of events. As I later learned, this happens . </p><p>The native Greenlanders on board seemed utterly unsurprised by this development, displaying the kind of resigned familiarity that suggested this was Tuesday for them. I began wondering if I could just pretend Iceland was Greenland—surely my family wouldn't notice the difference? But the pilot, apparently reading my mind, announced that no one would be disembarking in Iceland. It felt oddly authoritarian, like being grounded by an airline, as if they knew we'd all just wander off into Reykjavik and call it close enough.</p><p>We crash out in a airport hotel 20 minutes from our apartment after 15 hours in the air and tons of CO2 emissions only to wake up the next day to start again. This time, I notice that all of the people are asking for (and receiving) free beer from the crew that they are stashing in their bags. It turns out soda and beer, really anything that needs to be imported, is pretty expensive in Greenland. The complimentary drinks are there to be kept for later. </p><p>Finally we land. The first thing you notice when you land in Greenland is there are no trees or grass. There is snow and then there is exposed rock. The exterior of the airport is metal but the inside is wood, which is strange because again there are no trees. This would end up being a theme, where buildings representing Denmark were made out of lots of wood, almost to ensure that you understood they weren't from here. We ended up piling all of our stuff into a bus and heading for the hotel in Nuuk. </p><p>Nuuk is the capital of Greenland and your introduction to the incredible calm of the Greenlandic people. I have never met a less stressed out group of humans in my life. Nobody is really rushing anywhere, it's all pretty quiet and calm. The air is cold and crisp with lots of kids playing outside and just generally enjoying life. </p><p>The city itself sits in a landscape so dramatically inhospitable it makes the surface of Mars look cozy. Walking through the local mall, half the shops sell gear designed to help you survive what appears to be the apocalypse. Yet somehow, there's traffic. Actual traffic jams in a place where you can walk from one end to the other in twenty minutes. It's like being stuck behind a school bus in your own driveway.</p><p>To put this map into some perspective, it is only six kilometers from the sorta furthest tip to the airport. </p><p>But riding the bus around Nuuk was a peaceful experience that lets you see pretty much the entire city without needing to book a tour or spend a lot of money. We went to Katuaq, a cultural center with a cafe and a movie theater that was absolutely delicious food. </p><p>But again even riding the bus around it is impossible to escape the feeling that this is a fundamentally hostile to human life place. The sun is bright and during the summer its pretty hot, with my skin feeling like it was starting the burn pretty much the second it was exposed to the light. It's hard to even dress for, with layers of sunscreen, bug spray and then something warm on top if you suddenly got cold. </p><p>The sun, meanwhile, has apparently forgotten how to set, turning our hotel rooms into solar ovens. You wake up in a pool of your own sweat, crack a window for relief, and immediately get hit with air so cold it feels personal. It's like being trapped in a meteorological mood swing.</p><p>So after a night here, we went back to the airport again and flew to our final destination, Ilulissat.</p><p>The flight to our final destination revealed Greenland's true nature: endless, empty hills stretching toward infinity, punctuated by ice formations that look like nature's sculpture garden.</p><p>Landing in Ilulissat felt like victory—we'd made it to the actual destination, not just another waypoint in our Arctic odyssey. Walking through the tiny airport, past Danish military recruitment posters (apparently someone, somewhere, thought this place needed defending), I felt genuinely optimistic for the first time in days.</p><p>Well you can sleep easy Danish military, because Ilulissat is completely protected from invasion. The second I stepped outside I was set upon by a flood of mosquitos like I have never experienced before. I have been to the jungles of Vietnam, the swamps of Florida and the Canadian countryside. This was beyond anything I've ever experienced. </p><p>There are bugs in my mouth, ears, eyes and nose almost immediately. The photo below is not me being dramatic, it is actually what is required to keep them off of me. </p><p>In fact what you need to purchase in order to walk around this area at all are basically bug nets for your face. They're effectively plastic mesh bags that you put on. </p><p>Our hotel, charming in that \"remote Arctic outpost\" way, sat adjacent to what I can only describe as a canine correctional facility. Dozens of sled dogs were chained to rocks like some sort of prehistoric parking lot, each with a tiny house they could retreat to when the existential weight of their circumstances became too much.</p><p>Now, I'd always imagined sled dogs living their best life—running through snow, tongues lolling, living the Disney version of Arctic life. I'd never really considered their downtime, assuming they frolicked in meadows or something equally wholesome. The reality was more \"minimum security prison with a view.\"</p><p>The dogs are visited roughly twice a day by the person who owns and feeds them, which was quite the party for the dogs that lost their minds whenever the car pulled up. Soon the kids really looked forward to dog feeding time. The fish scrapes the dogs lived on came out of a chest freezer that was left exposed up on the rock face without electricity and you could smell it from 50 yards away when it opened. </p><p>During one such performance, a fellow parent leaned over and whispered with the casual tone of someone commenting on the weather, \"I think that one is dead.\" Before I could process this information, the frozen canine was unceremoniously launched over a small cliff like a furry discus. A second doggy popsicle followed shortly after, right in front of our assembled children, who watched with the kind of wide-eyed fascination usually reserved for magic shows.</p><p>We stopped making dog feeding time a group activity after that and had to distract the kids from ravens flying away with tufts of dog fur. </p><h3>Whales taste like seaweed </h3><p>Obviously a big part of Greenland is the nature, specifically the icebergs. Icebergs are incredible and during the week we spend up there, I enjoyed watching them every morning. It's like watching a mountain slowly moving while you sit still. The visual contrast of the ice and the exposed stone is beautiful and peaceful. </p><p>Finding our tour operator proved to be an exercise in small-town efficiency. The man who gave me directions was the same person who picked us up from the airport, who was also our tour guide, who probably doubled as the mayor and local meteorologist. It was like a one-man civic operation disguised as multiple businesses—the ultimate small-town gig economy.</p><p>The sea around Greenland is calmer than anything I've ever been on before, perfectly calm and serene. All around us whales emerged, thrilling my daughter. However the biggest hit of the entire tour, maybe the entire trip, was a member of the crew who handed each of the kids a giant rock of glacier ice to eat. I had to pull my daughter away to observe the natural beauty as she ate glacier ice like it was ice cream. \"LOOK AT MY ICE\" she was yelling as they slipped and slid around the deck of this boat. </p><p>So if you've ever wonder \"what is a glacier\", let me tell you. Greenland has a lot of ice and it pushes out from the land that is covers into the sea. When that happens, a lot of it breaks off. This sounds more exciting than it is. On TV in 4K it looks incredible, giant mountains of ice falling into the ocean. Honestly you can go read the same thing I did <a href=\"https://science.howstuffworks.com/environmental/earth/geophysics/glacier.htm\" rel=\"noreferrer\">here</a>.</p><p>However that doesn't happen very often. So in order for us tourists to be able to see anything, we had to go to a very productive glacier. This means there are constantly small chunks breaking off and falling into the sea. Practically though, it kinda looks like you are a boat in a slushee. It's beautiful and something to see, but also depressing to see along the rock face how much more ice there used to be. </p><p>Back in town, we hopped on the \"bus\". Now the bus here is clearly a retrofitted party van, complete with blue LED lights. The payment system is zip tied to a desk chair that is, itself, wedged in the front. However the bus works well and does get you around. The confusing part is that you will, once again, sometimes encounter a lot of traffic. People are driving pretty quickly and really seem to have somewhere to go. You also see a lot of fancy cars parked outside of houses here. </p><p>Which begs a pretty basic question. If there was almost nowhere to drive to in Nuuk, where in the <em>hell are these people driving</em>. The distance between the end of the road and the beginning of the road is less than 6 km. Also the process to make a road here is beyond anything you've ever seen. Everything requires a giant pile of explosives. </p><p>Where did these vehicles even come from? Why does one ship a BMW to a place accessible only by plane and boat? More importantly, where was everyone going with such determination? It was like watching a very expensive version of bumper cars, except everyone was committed to the illusion that they had somewhere important to be. Everyone had dings and scrapes like crashes were common. </p><h3>Grocery Store from the Sea</h3><p>Anyway, as I dodged speeding cars filled with people heading nowhere, I decided to hop off the bus and head to the grocery store. Inside was less a store and more the idea of a store. There was a lot of alcohol, chips, candy and shelf-stable foods, which all makes sense to me. What was strange was there wasn't a lot else, including meat. Locals couldn't be eating at the local restaurants, where the prices were as high as Berlin or Copenhagen for food. So what were they eating?</p><p>When I asked one of my bus drivers, he told me that it was pretty unusual to buy meat. They purchased a lot of whale and seal meat. I had sorta heard this before, but when we stopped the bus he pointed out a group of men hauling guns out into a small boat to go shoot seals. The guns were held together with a surprising amount of duct tape, which is not something I associate with the wild. </p><p>I had assumed, based on my casual reading of the news, that we were  done killing whales. As it turns out, I was wrong. They eat a lot of whale and it is, in fact, not hard to find. If you are curious, whale does not taste fishy. It tastes a little bit like if you cooked reindeer in a pot of seaweed. I wouldn't go out of your way for it, but it's not terrible. </p><p>The argument I've always heard for why people still kill whales is because it's part of their culture and also because it's an important source of protein. When you hear the phrase \"part of their culture\" I always imagined like traditional boats going out with spears. What I didn't imagine was industrial fishing boats and an industrial crane that lifts the dead whale out of the water for \"processing\". Some of the illusion is broken when your boat tour guide points out the metal warehouse with the word \"whale\" on the side. \"Yeah the water here was red with blood for a week\" the guide said, counting the cigarettes left in a pack he had. </p><h3>Should you go to Greenland?</h3><p>It's a wild place unlike anywhere I've ever been. It is the closest I have ever felt to living a sci-fi type experience. The people of Greenland are amazing, tough, calm and kind. I have nothing but positive experiences to recount from the many people I met there, Danish and Greenlandic, who patiently sat through my millions of questions. </p><p>However it is, by far, the least hospitable to human life place I've ever been to. The folks who live there have adapted to the situation in, frankly, genius ways. If that's your idea of a good time, Greenland is perfect for you. Maybe don't get emotionally attached to the sled dogs though. Or the whales. </p>","contentLength":15866,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45396754"},{"title":"A WebGL game where you deliver messages on a tiny planet","url":"https://messenger.abeto.co/","date":1758986250,"author":"thecupisblue","guid":90,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45396441"},{"title":"SSH3: Faster and rich secure shell using HTTP/3","url":"https://github.com/francoismichel/ssh3","date":1758983230,"author":"tempaccount420","guid":89,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45395991"},{"title":"Typst: A Possible LaTeX Replacement","url":"https://lwn.net/Articles/1037577/","date":1758958299,"author":"pykello","guid":88,"unread":true,"content":"<blockquote><b>This article brought to you by LWN subscribers</b><p>\nSubscribers to LWN.net made this article — and everything that\n       surrounds it — possible.  If you appreciate our content, please\n       <a href=\"https://lwn.net/Promo/nst-nag3/subscribe\">buy a subscription</a> and make the next\n       set of articles possible.\n</p></blockquote><div><p>This article was contributed by Lee Phillips</p></div><p><a href=\"https://typst.app\">Typst</a> is a program for document\ntypesetting. It is especially well-suited to technical material\nincorporating elements such as mathematics, tables, and floating\nfigures. It produces high-quality results, comparable to the gold standard,\n<a href=\"https://www.latex-project.org/\">LaTeX</a>, with a simpler markup\nsystem and easier customization, all while compiling documents\nmore quickly. Typst is free software, Apache-2.0 licensed, and is written in Rust.\n</p><h4>Desire for a LaTeX replacement</h4><p>\nLaTeX is a document typesetting system built on the foundation of Donald\nKnuth's <a href=\"https://tug.org/whatis.html\">TeX</a>. LaTeX has become the\nstandard tool for the preparation of scholarly papers and books in several\nfields, such as mathematics and computer science, and widely adopted in\nothers, such as physics. TeX and LaTeX, which <a href=\"https://www.latex-project.org/publications/2001-CAR-LaTeX-legacy.pdf\">predate\nLinux</a>, are early free software <a href=\"https://www.tug.org/TUGboat/tb24-1/gaudeul.pdf\">success\nstories</a>. The quality of TeX's (and therefore LaTeX's) output rivals the\nwork of skilled hand typesetters for both text and mathematics.\n</p><p>\nDespite the acclaim earned by LaTeX, its community of users has been\ngriping about it for years, and wondering aloud whether one day a\nreplacement might arrive. There are several reasons for this\ndissatisfaction: the LaTeX installation is huge, compilation of large\ndocuments is not fast, and its error messages are riddles delivered by an\ninfuriating oracle. In addition, any nontrivial customization or alteration to the\nprogram's behavior requires expertise in an arcane macro-expansion\nlanguage.\n</p><p>\nAlong with the griping came resignation: after decades of talk about a\nLaTeX replacement with nothing plausible on the horizon, and with the\nrecognition that LaTeX's collection of specialized packages would take\nyears to replace, it seemed impossible to dislodge the behemoth from its\nexalted position.\n</p><p>\nI had been aware of this project for over a year but had not paid much\nattention, assuming it to be yet another attempt to supplant LaTeX that was\ndoomed to fail. A rising chorus of enthusiasm among early\nadopters, and the beginnings of <a href=\"https://juti.if.its.ac.id/index.php/juti/index\">acceptance</a> of\nTypst manuscripts by scholarly journals, made me curious enough\nto take the young project for a spin.\n</p><p>\nTypst is available as Rust source and as a compiled binary. To install,\nvisit the <a href=\"https://github.com/typst/typst/releases\">releases\npage</a> and download the appropriate archive. There are options for Linux,\nmacOS and Windows; I used the\nprecompiled Linux version for my testing.\n</p><p>\nThe \"\" command accepts several subcommands. Entering\n\"\" lists all of the usable fonts to be found in\nstandard locations on the machine; nonstandard font directories can be\nadded manually. In my case, Typst found all of my 476&nbsp;fonts instantly;\nthe only ones omitted were some ancient PostScript Type&nbsp;1 fonts used\nby LaTeX. Users who have LaTeX installed will have a large collection of\nOpenType and TrueType math and text fonts on their machines; Typst can\nuse all of these. But Typst will work fine without them, as the program has\na small collection of fonts built in (try \"\" to see them).\n</p><p>\nTwo other subcommands to explore are\n\"\", which generates the output (PDF by default, with PDF/A,\nSVG, and PNG available, along with HTML under development) from a source file, and\n\"\" for interactive editing. The  subcommand keeps a Typst process running that\nincrementally and automatically compiles the document to PDF in response to\nchanges in the source. To use \"\" effectively, the\nscreen should be divided into three windows: a small terminal window to monitor\nthe  output for error (or success) messages, the editing\nwindow, and an area for any PDF reader that automatically reloads the\ndisplayed document when it changes (many, such as my favorite, <a href=\"https://github.com/ahrm/sioyek?tab=readme-ov-file#sioyek\">Sioyek</a>, do this). The result is a\nresponsive live preview, even of large documents, due to Typst's speed and\nincremental compilation. For example, Frans Skarman <a href=\"https://fransskarman.com/phd_thesis_in_typst.html\">described</a>\nhis experience writing his doctoral thesis in Typst, and noted\nthat he was able to enjoy nearly instant previews of content changes to\nthe book-length document. \n</p><h4>How Typst improves on LaTeX</h4><p>\nTypst output is quite close to that of LaTeX. It uses the same <a href=\"https://gwern.net/doc/design/typography/tex/1981-knuth.pdf\">line-breaking\nalgorithm</a> developed by Donald Knuth and Michael Plass for TeX, so it\ncreates nicely balanced paragraphs of regular text. Its mathematical\ntypesetting algorithms are <a href=\"https://typst.app/blog/2023/january-update\">based closely</a> on the\nTeX algorithms, and indeed mathematical rendering is nearly\nindistinguishable between the two systems.\n</p><p>\nGetting started with LaTeX can be confusing for newcomers, because it comes\nwith several alternative \"engines\" reflecting the long and complex history\nof the project. These are the various binaries such as \"\",\n\"\", \"\", \"\", \"\",\nand more, each with somewhat different capabilities. For Typst there is\nonly \"\".\n</p><p>\nMarkup in Typst is less verbose and easier to read than in LaTeX. It\ndispenses with the plethora of curly brackets and backslashes littering\nLaTeX documents by adopting, for prose, syntax in the style of <a href=\"https://www.markdownguide.org/basic-syntax\">Markdown</a>, and, for\nequations, a set of conventions designed for easy input. The\nfact that curly brackets and backslashes are awkward to type on German\nkeyboards may have provided a little extra impetus for the developers to\ncreate an alternative markup system that doesn't require a forest of these\nsymbols.\n</p><p>\nWhen users make syntax errors in markup or programming, inevitable\neven in Typst, the system presents them with another dramatic improvement\nover LaTeX (and TeX): error messages using colored glyphs that\nclearly point out exactly where the problem is. I've even discovered that\nTypst will save me from trying to run a syntactically correct infinite\nloop.\n</p><p>\nHere is a bit of Typst markup for a shopping list, with\nthe resulting rendering to the right:\n</p><img src=\"https://static.lwn.net/images/2025/typst-list.png\" border=\"0\" hspace=\"5\" align=\"right\" alt=\"[Rendered list]\" title=\"Rendered list\" width=\"235\" height=\"291\"><pre>   = Shopping List\n   \n   == Vegetables\n   \n   + Broccoli\n   + Asparagus (*fresh only*)\n   + Plantains (_ripe and green_)\n   \n   == Booze\n   \n   + Rum\n     - White\n     - Dark\n   + #underline[Good] gin  \n</pre><p>\nThe example gives a flavor of Typst's terse markup syntax. Headings are\nindicated with leading  signs. Automatically numbered lists are\ncreated by prepending  signs to items, and bulleted lists with\n signs; lists can be nested. Delimiters are shown for bold text\nand italics. These are shortcuts, or markup syntax sugar, for Typst\nfunctions for transforming text. Not every function has a corresponding\nshortcut; in those cases one needs to call the function explicitly, as in\nthe final item.\n</p><p>\nTypst input is always within one of three modes. Markup (text) mode is the\ndefault. The  sign preceding the function call in the last line\nof the example\nplaces Typst in \"code mode\". The \"<a href=\"https://typst.app/docs/reference/text/underline/\"></a>\" function accepts a number of keyword arguments that affect its behavior, and one trailing argument, in square brackets, containing the text that it modifies. In the example, we've stuck with the default behavior, but if we wanted, for example, a red underline, we could use \"\". Following the square-bracketed text argument, Typst returns to interpreting input in text mode.\n</p><p>Other functions produce output directly, rather than modifying a text argument. This bit of Typst input:</p><pre>    #let word = \"Manhattan\"\n    There are #str.len(word) letters in #word.\n</pre><p>produces the output (in typeset form) \"There are 9 letters in Manhattan.\". The \"<a href=\"https://typst.app/docs/reference/foundations/str/#definitions-len\"></a>\" function is part of the\n\"<a href=\"https://typst.app/docs/reference/foundations/str/\"></a>\" module, so it needs the namespace.</p><p>\nLet's take a look at the LaTeX equivalent for the first half of the\nshopping list for comparison:\n</p><pre>   \\documentclass[12pt]{article}\n   \\begin{document}\n   \\section*{Shopping List}\n   \n   \\subsection*{Vegetables}\n   \n   \\begin{enumerate}\n   \\item Broccoli\n   \\item Asparagus ({\\bfseries fresh only})\n   \\item Plantains (\\emph{ripe and green})\n   \\end{enumerate}\n   \\end{document}   \n</pre><p>\nThe first two and the last line are boilerplate that is not required in\nTypst. The difference in verbosity level and ease of reading the source is\nclear.\n</p><p>\nThe third Typst mode, in addition to markup and code, is math mode,\ndelimited by dollar signs. This is also best illustrated by an example:\n</p><pre>   $ integral_0^1 (arcsin x)^2 (dif x)/(x^2 sqrt(1-x^2)) = π ln 2 $  \n</pre><p>\nWhen this is compiled by Typst, it produces the result shown below:\n</p><p>\nThose who've used LaTeX will begin to see from this example how math in\nTypst source is less verbose and easier to read than in LaTeX. Greek\nletters and other Unicode symbols can be used directly, as in modern LaTeX\nengines such as , which we <a href=\"https://lwn.net/Articles/731581/\">looked at back in 2017</a>, but with no\nimports required.\n</p><p>\nThe advent of the LuaTeX and LuaLaTeX projects provided users who wanted to\nincorporate programming into their documents a more pleasant alternative to\nthe TeX macro language. As powerful as the embedded Lua system is, however,\nit betrays its bolted-on status, requiring users to negotiate the interface\nbetween Lua data structures and LaTeX or TeX internals. In Typst,\nprogramming is thoroughly integrated into the system, with no seams between\nthe language used for calculation and the constructs that place characters\nin the final PDF. Typst programs are invariably <a href=\"https://lee-phillips.org/TLexamples/\">simpler</a> than their LuaLaTeX\nequivalents. All authors using Typst will make at least some simple use of\nits programming language, as such basic necessities as changing fonts, or\ncustomizations such as changing the style of section headings, are\naccomplished by calling Typst functions.\n</p><p>\nThe Typst language is somewhat similar to Rust, perhaps\nunsurprisingly. Most Typst functions are : they have no side effects and\nalways produce the same result given the same arguments (aside from certain\nfunctions that mutate their arguments, such as <a href=\"https://typst.app/docs/reference/foundations/array/#definitions-push\"></a>). This\naspect reduces the probability of difficult-to-debug conflicts among\npackages that plague LaTeX, and makes it easier to debug Typst documents.\n</p><p>\nAlthough Typst uses the same line-breaking algorithm as LaTeX, its internal\napproach to overall page layout is <a href=\"https://laurmaedje.github.io/posts/layout-models\">distinct</a>. Some\nconsequences are that Typst does a better job at handling movable elements\nsuch as floating figures, and can, for example, easily split large tables\nacross page breaks, something that LaTeX struggles with even with\nspecialized packages.\n</p><p>\nTypst's page layout algorithm doesn't always permit the refinements that \nLaTeX is capable of.\nFor example, Typst is not as good\nas LaTeX at avoiding <a href=\"https://practicaltypography.com/widow-and-orphan-control.html\">widows\nand orphans</a>.\nAnother salient deficiency is Typst's relative lack of specialized packages,\ncompared with the vast ecosystem produced by LaTeX's decades of community\ninvolvement. However, the relative ease of programming in Typst (and the\nwell-organized and extensively commented underlying Rust code) suggests\nthat this drawback may be remedied before a comparable number of decades\nhave elapsed. Indeed, there are already <a href=\"https://typst.app/universe\">over 800&nbsp;packages available</a>. Typst still cannot do\neverything that LaTeX can, but the breadth of its package collection is\nencouraging.\n</p><p>\nAlmost no journals that provide LaTeX templates for submissions offer a\nTypst option, so physicists and mathematicians adopting Typst will need to\nfind a way to convert their manuscripts. This is made easier for those who\nuse <a href=\"https://pandoc.org/\">Pandoc</a>, as that conversion program\nhandles Typst.\n</p><p>\nAnother drawback is the difficulty of learning Typst. The <a href=\"https://typst.app/docs\">official documentation</a> is confusingly\norganized, with information scattered unpredictably among \"Tutorial\",\n\"Reference\", and \"Guides\" sections. Concepts are not always clearly\nexplained, and sometimes not presented in a logical order. The manual is\nnot keeping up with the rapid development of the program, and contains some\nout-of-date information and errors. None of this is surprising considering\nhow quickly the project is moving, its early stage, and its\nsmall core team. A work-in-progress called the <a href=\"https://sitandr.github.io/typst-examples-book/book\"></a> has appeared, which may be a better starting point\nthan the official documentation.\n</p><p>\nThere are other minor deficiencies compared with LaTeX, such as the\ninability to include PDF documents. Typst provides no analogue to LaTeX's\n command, which lets authors mold paragraphs to, for\nexample, wrap around complex illustrations. The situation is likely to\nchange, however, as something like  is <a href=\"https://forum.typst.app/t/is-there-an-equivalent-to-latex-s-parshape/1006\">being\nconsidered</a> for the future.\n</p><p>\nMore serious is the possibility of breaking changes as the system evolves,\nalways a risk of early adoption. I suspect, however, that these will require\nonly minor edits to documents in most cases. Progress seems to be steady,\nrational, and <a href=\"https://typst.app/docs/roadmap\">responsive to\nuser requests</a>.\n</p><p>\nI'm using Typst in real work right now to write a physics paper. I will\nneed to submit my manuscript using the journal's LaTeX template, but I'm\ntaking advantage of Typst to make the entry of the paper's many equations\nsimpler, and I'll <a href=\"https://lee-phillips.org/typstfilters/\">transform</a> the result to\nLaTeX with Pandoc without needing any manual adjustment. The tooling is excellent, as my preferred editor, <a href=\"http://neovim.io/\">Neovim</a>, has <a href=\"https://github.com/nvim-treesitter/nvim-treesitter?tab=readme-ov-file#nvim-treesitter\">support</a>\nfor the <a href=\"https://tree-sitter.github.io/tree-sitter/\">Tree-sitter\nincremental parser</a>\nfor Markdown and Typst, which provides syntax-aware highlighting\nand navigation of the source files. I use Typst's fast incremental\ncompilation to get live feedback as I fiddle with my math markup.\n</p><p>\nI was skeptical when I downloaded Typst to try it out, but became\nenthusiastic within minutes, as I saw the first (of many) of its lovely\nerror messages, and remained sanguine as I saw the quality of its output. I\npredict that Typst will eventually take the place of LaTeX. But even if\nthat never comes to pass, it is a useful tool right now.\n</p>","contentLength":13424,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45393842"},{"title":"Open Social","url":"https://overreacted.io/open-social/","date":1758902515,"author":"knowtheory","guid":87,"unread":true,"content":"<p>Open source has clearly won. Yes, there are plenty of closed source products and businesses. But the shared infrastructure—the commons—runs on open source.</p><p>We might take this for granted, but it wasn’t a foregone conclusion thirty five years ago. There were powerful forces that wanted open source to lose. Some believed in the open source model but didn’t think it could ever compete with closed source. Many categories of tools only existed as closed source. A Microsoft CEO called open source cancer—a decade before Microsoft has rebuilt its empire around it. The open source movement may not have lived up to the ideals of the “free software”, but it won in industry adoption. Nobody gets fired for choosing open source these days. For much crucial software, open source is now .</p><p>I believe we are at a similar juncture with social apps as we have been with open source thirty five years ago. <strong>There’s a new movement on the block.</strong> I like to call it . There are competing visions for what “open social” should be like. I think the <a target=\"_blank\" href=\"https://atproto.com/\">AT Protocol</a> created by Bluesky is the most convincing take on it so far. It’s not perfect, and it’s a work in progress, but there’s nothing I know quite like it.</p><p>(Disclosure: I used to work at Bluesky on the Bluesky client app. I wasn’t involved in the protocol design. I am a fan, and this post is my attempt to explain why.)</p><p>In this post, I’ll explain the ideas of the AT Protocol, lovingly called , and how it changes the relationship between the user, the developer, and the product.</p><p>I don’t expect atproto and its ecosystem (known as ) to win hearts overnight. Like open source, it might take a few decades to become ubiquitous. By explaining these ideas here, I’m hoping to slightly nudge this timeline. Despite the grip of today’s social media companies, I believe open social will eventually seem inevitable in retrospect—just like open source does now. Good things  happen; all it takes is years of sustained effort by a community of stubborn enthusiasts.</p><p>What open source did for code, open social does for data.</p><p>The web is a beautiful invention.</p><p>You type  and you end up on Alice’s website.</p><p>Or you type  and you end up on Bob’s website.</p><p>In a sense, your browser is a portal to millions of different worlds, each with its own little jurisdiction. Only Alice decides what appears on Alice’s website. Only Bob decides what appears on Bob’s website. They meaningfully “own their data”.</p><p>This doesn’t mean that they’re isolated. On the contrary, Alice can embed Bob’s picture with an , and Bob can link to Alice’s page with :</p><p>Alice and Bob can link to each other, but they remain in charge of their sites.</p><p>What do I  by saying Alice and Bob are in charge of their own sites? Even if they’re not physically hosting their content on their own computers, they could always change hosting. For example, if Alice’s hosting provider starts deleting her pages or injecting ads into them, Alice can take her content to another host, and point  at another computer. <em>The visitors won’t need to know.</em></p><p> Hosting providers have no real leverage over Alice and Bob. If the hosting provider “turns evil” and starts messing with your site, you can just walk away and host it elsewhere (as long as you have a backup). You’re not going to lose your traffic. All existing links will seamlessly resolve to the new destination.</p><p>If Alice changes her hosting, Bob won’t need to update any links to Alice’s website. Alice’s site will keep working as if nothing had happened. At worst, a DNS change might make it inaccessible for a few hours, but then the web will be repaired:</p><p>Imagine how different the incentives would be if links  tied to physical hosts!</p><p>If changing a hosting provider caused Alice to lose her traffic, she would think many times before changing providers. Perhaps she’d stick with her existing provider even if it was messing with her site, as losing her connections is even worse. Luckily, web’s decentralized design avoids this. Because it’s easy to walk away, hosting providers are forced to compete, and hosting is now a commodity.</p><p>I think the web is a beautiful idea. It links decentralized islands controlled by different people and companies into one interconnected surface that anyone can index and navigate. Links describe a <em>relationship between logical documents</em> rather than between physical servers. As a result, you’re not a hostage to your hosting.</p><p>As a wise person said, in theory, there is no difference between theory and practice, but in practice there is. So what’s been happening with the web?</p><p>In the early 90’s, the main way to publish something on the web was to have your own website. Today, most people publish content by using a social media app.</p><p>Alice and Bob are still publishing things. But instead of publishing at domains like  and , they publish at usernames like  and  allocated by a social media company. The things they publish are not HTML pages, but app-specific entities such as profiles, posts, comments, likes, and so on.</p><p>These entities are usually stored in a database on the social company’s servers. The most common way to visualize a database is as a sequence of rows, but you could also visualize it as a graph. This makes it look very similar to web itself:</p><p>What does this social graph enable that a web of personal sites doesn’t?</p><p>The advantage of storing structured app-specific entities, such as posts and likes, instead of HTML documents is obvious. App-specific entities such as posts and likes have a richer structure: you can always turn them  HTML documents later, but you can  aggregate them, filter them, query, sort, and recombine them in different ways before that. This allows you to create <strong>many projections of the same data</strong>—a profile page, a list of posts, an individual post with comments.</p><p>Where this really shines, though, is when many people use  social app. Since everyone’s public content is now in a single database, it is easy to aggregate  content published by many people. This enables social features like global search, notifications, feeds, personalized algorithms, shared moderation, etc.</p><p>It’s specifically this  that blows the “personal sites” paradigm out of the water. People are social creatures, and we want to congregate in shared spaces. We don’t just want to visit each other’s sites—we want to hang out , and social apps provide the shared infrastructure. Social aggregation features like notifications, feeds, and search are non-negotiable in modern social products.</p><p>Today, the most common way to implement these features is shaped like this:</p><p>There still  a web-like logical model of our data—our profiles, our posts, our follows, our likes, all the things that we’ve created—but it lives  some social app’s database. What’s exposed to the web are only  of that model—the Home screen, the Notifications screen, the HTML pages for individual posts.</p><p>This architecture makes sense. It is the easiest way to evolve the “personal sites” paradigm to support aggregation so it’s not surprising today’s apps have largely converged on it. People create accounts on social apps, which lets those apps build aggregated features, which entices more people to sign up for those apps.</p><p>However, something got lost in the process. <em>The web we’re actually creating</em>—our posts, our follows, our likes—is no longer meaningfully ours. Even though much of what we’re creating is public, it is not a part of the open web. We can’t change our “hosting provider” because we’re now  from how the internet works. We, and the web we create, have become rows in somebody else’s database:</p><p>This creates an imbalance.</p><p>When Alice used to publish her stuff on , she was not tied to any particular hosting provider. If she were unhappy with a hosting provider, she knew that she could swap it out without losing any traffic or breaking any links:</p><p>That kept the hosting providers in check.</p><p>But now that Alice publishes her stuff on a social media platform, she can no longer “walk away” without losing something. If she signs up to another social platform, she would be  to start from scratch, even if she  to retain her connections. There is no way for Alice to sever the relationship with a particular app without ripping herself, and anything she created there, out of its social graph:</p><p>The web Alice created—who she follows, what she likes, what she has posted—is trapped in a box that’s owned by somebody else. To leave is to leave it .</p><p>On an individual level, it might not be a huge deal.</p><p>Alice can rebuild her social presence connection by connection somewhere else. Eventually she might even have the same reach as on the previous platform.</p><p>However, , the net effect is that social platforms—at first, gradually, and then suddenly—turn their backs on their users. If you can’t leave without losing something important, the platform has no incentives to respect you as a user.</p><p>Maybe the app gets squeezed by investors, and every third post is an ad. Maybe it gets bought by a congolomerate that wanted to get rid of competition, and is now on life support. Maybe it runs out of funding, and your content goes down in two days. Maybe the founders get acquihired—an exciting new chapter. Maybe the app was bought by some guy, and now you’re slowly getting cooked by the algorithm.</p><p>If your next platform doesn’t respect you as a user, you might try to leave it, too.</p><p>But what are you going to do? Will you “export your data”? What will you do with that lonely shard of a social graph? You can upload it somewhere as an archive but it’s ripped out of its social context—a pitiful memento of your self-imposed exile.</p><p>Those megabytes of JSON you got on your way out are . It’s like a branch torn apart from its tree. It doesn’t  anywhere. To give a new life to our data, we’d have to  export it and then  import it into some next agreed-upon social app—a near-impossible feat of coordination. Even then, the network effects are so strong that most people would soon find their way back.</p><p>You can’t  a social app without  the web you’ve created.</p><p>What if you could keep it?</p><p>Alice and Bob are still using social apps. Those apps don’t look much different from today’s social apps. <strong>You could hardly tell that something has changed.</strong></p><p>Something has changed, though. (Can you spot it?)</p><p>Notice that Alice’s handle is now . It is not allocated by a social media company. Rather, her handle is  universal “internet handle”, i.e. a domain. Alice  the  domain, so she can  on any open social app. (On most open social apps, she goes by , but for others she wants a distinct disconnected identity, so she owns another handle she’d rather not share.)</p><p>Bob owns a domain too, even though he isn’t technical. He might not even know what a “domain” is. Bob just thinks of  as his “internet handle”. Some open social apps will offer you a free subdomain on registration, just like Gmail gives you a free Gmail address, or may offer an extra flow for buying a domain. You’re not locked into your first choice, and can swap to a different domain later.</p><p>Your internet handle being something you  is the most user-visible aspect of open social apps. But the much bigger difference is invisible to the user.</p><p>When you previously saw the social graph above, it was trapped  a social app’s database. There was a box around that graph—it wasn’t a part of the web. With open social, Alice’s data—her posts, likes, follows, etc— hosted on the web itself. Alongside her personal site, Alice now has a  of her data:</p><p>This “repository” is a regular web server that implements the <a target=\"_blank\" href=\"https://atproto.com/\">AT Protocol</a> spec. The only job of Alice’s personal repository is to store and serve data created by Alice in the form of signed JSON. Alice is technical, so she likes to sometimes inspect her repo using open source tools like <a target=\"_blank\" href=\"https://pdsls.dev/\">pdsls</a>, <a target=\"_blank\" href=\"https://atproto.at/\">Taproot</a>, or <a target=\"_blank\" href=\"https://atproto-browser.vercel.app/\">atproto-browser</a>.</p><p>Bob, however, isn’t technical. He doesn’t even know that there is a “repository” with his “data”. He got a repository behind the scenes when he signed up for his first open social app. His repository stores  data (from all open social apps).</p><p>Have another look at this picture:</p><p><strong>These aren’t rows in somebody’s database. This is a web of hyperlinked JSON.</strong> Just like every HTML page has an  URI so other pages can link to it, every JSON record has an <a href=\"https://overreacted.io/where-its-at/\"> URI</a>, so any other JSON record can link to it. (On this and other illustrations,  is a shorthand for .) The  protocol is <a target=\"_blank\" href=\"https://www.ietf.org/archive/id/draft-newbold-at-architecture-00.html\">a bunch of conventions</a> on top of DNS, HTTP, and JSON.</p><p>Now have a look at the arrows between their records. Alice follows Bob, so she has a  record linking to Bob’s  record. Bob commented on Alice’s post, so he has a  record that links to Alice’s  record. Alice liked his comment, so she has a  record with a link to his  record. Everything Alice creates stays in her repo under her control, everything Bob creates stays in his repo under his control, and links express the connections—just like in HTML.</p><p><strong>All of this happens behind the scenes and is invisibile to a non-technical user.</strong> The user doesn’t need to think about where their data is stored until it matters, just like the user doesn’t think about how servers work when navigating the web.</p><p>Alice’s and Bob’s repositories could be hosted on the same machine. Or they could be hosted by different companies or communities. Maybe Alice is self-hosting her repository, while Bob uses a free hosting service that came by default with his first open social app. They may even be running completely <a target=\"_blank\" href=\"https://github.com/bluesky-social/pds\">different</a><a target=\"_blank\" href=\"https://github.com/blacksky-algorithms/rsky/tree/main/rsky-pds\">implementations</a>. If both servers follow the AT protocol, they can participate in this web of JSON.</p><p>Note that  and  do not need to resolve to the same server. This is intentional so that having a nice handle like  doesn’t  Alice to host her own data, to mess with her website, or even to  a site at all. If she owns , she can point  at any server.</p><p>If Alice is unhappy with her hosting, she can pack up and leave:</p><p><em>(This requires a modicum of technical skill today but it’s getting <a target=\"_blank\" href=\"https://pdsmoover.com/info.html\">more accessible</a>.)</em></p><p>Just like with moving a personal site, changing where her repo is being served from doesn’t require cooperation from the previous host. It also doesn’t disrupt her ability to log into apps and doesn’t break any links. The web repairs itself:</p><p>It is worth pausing for a moment to appreciate what we have here.</p><p><strong>Every bit of public data that Alice and Bob created—their posts, their likes, their comments, their recipes, their scrobbles—is meaningfully owned by them.</strong> It’s not in a database subject to some CEO’s whims, but hosted  on the open web, with ability to “walk away” without losing traffic or breaking any links.</p><p>Like the web of personal sites, this model is centered around the user.</p><p>What does it mean for apps?</p><p>Each open social app is like a CMS (content management system) for a subset of data that lives in its users’ repositories. In that sense, your personal repository serves a role akin to a Google account, a Dropbox folder, or a Git repository, with data from your different open social apps grouped under different “subfolders”.</p><p>When you make a post on <a target=\"_blank\" href=\"https://bsky.app/\">Bluesky</a>, Bluesky puts that post into  repo:</p><p>When you star a project on <a target=\"_blank\" href=\"https://tangled.org/\">Tangled</a>, Tangled puts that star into  repo:</p><p>When you create a publication on <a target=\"_blank\" href=\"https://leaflet.pub\">Leaflet</a>, Leaflet puts it into  repo:</p><p>Over time, your repo grows to be a collection of data from different open social apps. This data is open by default—if you wanted to look at my Bluesky posts, or Tangled stars, or Leaflet publications, you wouldn’t need to hit these applications’ APIs. You could just <a target=\"_blank\" href=\"https://atproto-browser.vercel.app/at/danabra.mov\">hit my personal repository and enumerate all of its records</a>.</p><p>To avoid naming collisions, the data in the repository is grouped by the format:</p><p>In any user’s repo, Bluesky posts go with other Bluesky posts, Leaflet publications go with Leaflet publications, Tangled stars go with Tangled stars, and so on. Each data format is <a target=\"_blank\" href=\"https://www.pfrazee.com/blog/why-not-rdf#lexicon\">controlled and evolved</a> by developers of the relevant application.</p><p>I’ve drawn a dotted line to separate them but perhaps this is misleading.</p><p>Since the data from different apps “lives together”, there’s a much lower barrier for open social apps to piggyback on each other’s data. In a way, it starts to feel like a connected multiverse of apps, with data from one app “bleeding into” other apps.</p><p>When I signed up for Tangled, I chose to use my existing  handle. That makes sense since identity can be shared between open social apps. What’s more interesting is that Tangled  my avatar based on my Bluesky profile. It didn’t need to hit the Bluesky API to do that; it just read the Bluesky profile record in my repository. <strong>Every app can choose to piggyback on data from other apps.</strong></p><p>That might remind you of <a target=\"_blank\" href=\"https://gravatar.com/\">Gravatar</a>, but it works for . Every open social app can take advantage of data created by every other open social app:</p><p>There is no API to hit, no integrations to build, nothing to get locked out of. All the data is in the user’s repository, so you can parse it (as typed JSON), and use it.</p><p><strong>This has deep implications for the lifecycle of products.</strong> If a product gets shut down, the data doesn’t disappear. It’s still in its users’ repos. Someone can build a replacement that makes this data comes back to life. Someone can build a new product that incorporates  of that data, or lets users choose what to import. Someone can build an alternative projection of existing data—</p><p>This also reduces the “cold start” problem for new apps. If some of the data you care about already exists on the network, you can bootstrap your product off of that. For example, if you’re launching a short video app, you can piggyback on the Bluesky  records so that people don’t have to find each other again. But if that doesn’t make sense for your app, you can have your own  records instead, or offer a one-time import. All existing data is up for reuse and remixing.</p><p>Some open social apps are explicitly based  this sort of remixing. <a target=\"_blank\" href=\"https://anisota.net/\">Anisota</a> is primarily a Bluesky client, but it <a target=\"_blank\" href=\"https://anisota.net/profile/dame.is/document/3lxankooyf22l\">natively supports</a> showing Leaflet documents. <a target=\"_blank\" href=\"https://popfeed.social/\">Popfeed</a> can <a target=\"_blank\" href=\"https://bsky.app/profile/leaflet.pub/post/3lzjsw7c6os23\">cross-post reviews</a> to both Bluesky and Leaflet. If Leaflet does get very popular, there’s nothing stopping Bluesky itself from supporting a Leaflet document as another type of post attachment. In fact, some third-party Bluesky client could decide to do that first, and the official one could eventually follow.</p><p>This is why I like “open social” as a term.</p><p><strong>Open social frees up our data like open source freed up our code.</strong> Open social ensures that old data can get a new life, that people can’t be locked out of the web they’ve created, and that <em>products can be forked and remixed</em>. You don’t need an “everything app” when data from different apps circulates in the open web.</p><p>If you’re technical, by now you might have a burning question.</p><p>How the hell does aggregation work?!</p><p>Since every user’s records live in  repository, there could be millions (potentially billions?) of repositories. How can an app efficiently query, sort, filter, and aggregate information from them? Surely it can’t search them on demand.</p><p>I’ve previously used a CMS as an analogy—for example, a blogging app could directly write posts to your repository and then read posts from it when someone visits your blog. This “singleplayer” use case would not require aggregation at all.</p><p>To avoid hitting the user’s repository every time you want to display their blog post, you can connect to the user’s repository by a websocket. Every time a record relevant to your app is created, updated, or deleted, you can update your database:</p><p>This database isn’t the  for user’s data—it’s more like an app-specific cache that lets you avoid going to the user repo whenever you need some data.</p><p><strong>Coincidentally, that’s the exact mechanism you would use for aggregation.</strong> You listen to events from all of your app users’ repositories, write them to a local database, and query that database as much as you like with zero extra latency.</p><p>This might remind you of how Google Reader crawls RSS (rip).</p><p>To avoid opening a million event socket connections, it makes sense to listen to a stream that retransmits events from all known repositories on the network:</p><p>You can then filter down such a stream to just the events you’re interested in, and then update your local database in response to the events your app cares about.</p><p>For example, Leaflet is only interested in events concerning  records. However, Leaflet can also  to listen to other events. If Leaflet wanted to add a feature that shows backlinks to Bluesky discussions of a Leaflet document, it would simply start tracking  records too. <em>(Edit: I’ve been informed that Leaflet <a target=\"_blank\" href=\"https://bsky.app/profile/o.simardcasanova.net/post/3luujudlr5c2j\">already does this</a> to display quotes from Bluesky.)</em></p><p>You can see the combined event stream from every known repository <a target=\"_blank\" href=\"https://pdsls.dev/jetstream?instance=wss%3A%2F%2Fjetstream1.us-east.bsky.network%2Fsubscribe\">here</a>:</p><p>This is a realtime stream of every single event on the network. It’s dominated by  records because Bluesky is the most-used app, but you can filter it down to other record types. This retransmitter (called a “relay”) is operated by Bluesky, but you don’t have to depend on it. The <a target=\"_blank\" href=\"https://github.com/blacksky-algorithms\">Blacksky community</a> runs <a target=\"_blank\" href=\"https://github.com/blacksky-algorithms/rsky/tree/main/rsky-relay\">their own relay implementation</a> at , which you can try <a target=\"_blank\" href=\"https://pdsls.dev/firehose?instance=wss%3A%2F%2Fatproto.africa\">here</a>. It doesn’t matter which relay is used by which app—everyone “sees” the same web.</p><p>An important detail is that commits are cryptographically signed, which means that you don’t need to trust a relay or a cache of network data. You can verify that the records haven’t been tampered with, and each commit is legitimate. This is why “AT” in “AT Protocol” stands for “authenticated transfer”. You’re supposed to pronounce it like “@” (“at”) though. Don’t say “ay-tee” or you’ll embarrass me!</p><p>As time goes by, we’ll see more infrastructure built around and for open social apps. <a target=\"_blank\" href=\"https://www.graze.social/\">Graze</a> is letting users build their own algorithmic feeds, and <a target=\"_blank\" href=\"https://slices.network/\">Slices</a> is an upcoming developer platform that does large-scale repository indexing for you. <a target=\"_blank\" href=\"https://constellation.microcosm.blue/\">Constellation</a> and <a target=\"_blank\" href=\"https://app.ifthisthen.at/\">If This Then AT://</a> offer easy network querying and automation.</p><p>These are all technical details, though.</p><p>What matters is the big picture.</p><p>The pre-social web of “personalized sites” got data ownership, hosting independence, and linking right. Alice and Bob fully participate in the web:</p><p>The closed social web innovated in scaling and in social aggregation features. Notifications, search, and feeds are non-negotiable in modern social products:</p><p>However, the closed social web has also excluded  from the web.  is no longer meaningfully ours. We’re just rows in somebody else’s database.</p><p>Open social frees the web we’re creating from somebody else’s boxes. Our profiles, likes, follows, recipes, scrobbles, and other content meaningfully belong to us:</p><p>The data no longer lives  the products; the products  our data:</p><p>This blurs the boundaries between apps. Every open social app can use, remix, link to, and riff on data from every other open social app.</p><p>The web we’ve created remains after the products we used to create it are gone. Developers can build new products to recontextualize it. No one can take it away.</p><p>People might not ever start using technical concepts like “decentralization” but they do understand when data from one app can seamlessly flow into other apps.</p><p>People might not care about “federation” but they do notice when they log into a competing product, and their data is , and their reach is intact.</p><p>And people  understand when they’re being fucked with.</p><p>For a long time, open social will rely on a community of <a target=\"_blank\" href=\"https://bsky.app/profile/atprotocol.dev\">stubborn enthusiasts</a> who see the promise of the approach and are willing to bear the pains of building (and failing) in a new ecosystem. But I don’t think that dooms the effort. That’s the history of every big community-driven change. Somebody has to work through the kinks. Like with open source, open social is a compounding effort. Every mildly successful open social app lifts all open social apps. Every piece of shared infrastructure can benefit somebody else. At some point, open is bound to win.</p><p>I just hope it doesn’t take thirty five years.</p>","contentLength":24126,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45388021"},{"title":"Britain to introduce compulsory digital ID for workers","url":"https://www.reuters.com/world/uk/britain-introduce-mandatory-digital-id-cards-2025-09-26/","date":1758852587,"author":"alex77456","guid":86,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45381810"},{"title":"Improved Gemini 2.5 Flash and Flash-Lite","url":"https://developers.googleblog.com/en/continuing-to-bring-you-our-latest-models-with-an-improved-gemini-2-5-flash-and-flash-lite-release/","date":1758820856,"author":"meetpateltech","guid":85,"unread":true,"content":"<h2 data-block-key=\"dlq6f\"><b>Updated Gemini 2.5 Flash-Lite</b></h2><p data-block-key=\"9smg5\">The latest version of Gemini 2.5 Flash-Lite was trained and built based on three key themes:</p><ul><li data-block-key=\"3j771\"><b>Better instruction following</b>: The model is significantly better at following complex instructions and system prompts.</li></ul><ul><li data-block-key=\"dqar4\"> It now produces more concise answers, a key factor in reducing token costs and latency for high-throughput applications (see charts above).</li></ul><ul><li data-block-key=\"enl6e\"><b>Stronger multimodal &amp; translation capabilities:</b> This update features more accurate audio transcription, better image understanding, and improved translation quality.</li></ul><p data-block-key=\"446ee\">You can start testing this version today using the following model string: <code>gemini-2.5-flash-lite-preview-09-2025</code>.</p><p data-block-key=\"1dihg\">This latest 2.5 Flash model comes with improvements in two key areas we heard consistent feedback on:</p><ul><li data-block-key=\"202mt\"> We've improved how the model uses tools, leading to better performance in more complex, agentic and multi-step applications. This model shows noticeable improvements on key agentic benchmarks, including a 5% gain on SWE-Bench Verified, compared to our last release (48.9% → 54%).</li></ul><ul><li data-block-key=\"33iqo\"> With thinking on, the model is now significantly more cost-efficient—achieving higher quality outputs while using fewer tokens, reducing latency and cost (see charts above).</li></ul><p data-block-key=\"24i20\">We’re already seeing positive feedback from early testers. As Yichao ‘Peak’ Ji, Co-Founder &amp; Chief Scientist at <a href=\"https://manus.im/\">Manus</a>, an autonomous AI agent, noted: <i>“The new Gemini 2.5 Flash model offers a remarkable blend of speed and intelligence. Our evaluation on internal benchmarks revealed a 15% leap in performance for long-horizon agentic tasks. Its outstanding cost-efficiency enables Manus to scale to unprecedented levels—advancing our mission to Extend Human Reach.”</i></p><p data-block-key=\"322ui\">You can start testing this preview version today by using the following model string: <code>gemini-2.5-flash-preview-09-2025</code>.</p><h2 data-block-key=\"rw3s3\"><b>Start building with Gemini</b></h2><p data-block-key=\"au69p\">Over the last year, we’ve learned that shipping preview versions of our models allows you to test our latest improvements and innovations, provide feedback, and build production-ready experiences with the best of Gemini. Today’s releases are not intended to graduate to a new, <a href=\"https://ai.google.dev/gemini-api/docs/models#stable\">stable version</a> but will help us shape our future stable releases, and allow us to continue iterating and bring you the best of Gemini.</p><p data-block-key=\"3cljv\">To make it even easier to access our latest models while also reducing the need to keep track of long model string names, we are also introducing a <a href=\"https://ai.google.dev/gemini-api/docs/models#latest\">alias</a> for each model family. This alias always points to our most recent model versions, allowing you to experiment with new features without needing to update your code for each release. You can access the new previews using:</p><p data-block-key=\"5hnmp\">To ensure you have time to test new models, we will always provide a 2-week notice (via email) before we make updates or deprecate a specific version behind . These are just model aliases so the rate limits, cost, and features available may fluctuate between releases.</p><p data-block-key=\"6nene\">For applications that require more stability, continue to use  and .</p><p data-block-key=\"62m93\">We continue to push the frontier of what is possible with Gemini and this release is just another step in that direction. We will have more to share soon, but in the meantime, happy building!</p>","contentLength":3152,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45375845"},{"title":"ChatGPT Pulse","url":"https://openai.com/index/introducing-chatgpt-pulse/","date":1758819595,"author":"meetpateltech","guid":84,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45375477"}],"tags":["dev"]}